{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334a04dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c49f5edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import finalprojectneuralnetworks as fpnn\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "087f60f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "\n",
    "# with open(\"Dummy_Validate.csv\", \"w\", newline=\"\") as csvfile:\n",
    "#     spamwriter = csv.writer(\n",
    "#         csvfile, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL\n",
    "#     )\n",
    "#     col_labels = [\n",
    "#         \"AF3\",\n",
    "#         \"F7\",\n",
    "#         \"F3\",\n",
    "#         \"FC5\",\n",
    "#         \"T7\",\n",
    "#         \"P7\",\n",
    "#         \"O1\",\n",
    "#         \"O2\",\n",
    "#         \"P8\",\n",
    "#         \"T8\",\n",
    "#         \"FC6\",\n",
    "#         \"F4\",\n",
    "#         \"F8\",\n",
    "#         \"AF4\",\n",
    "#         \"eyeDetection\",\n",
    "#     ]\n",
    "#     spamwriter.writerow(col_labels)\n",
    "#     list_a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 0]\n",
    "#     list_b = [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 1]\n",
    "#     for i in range(500):\n",
    "#         random_number = random.randint(1, 10)\n",
    "#         if random_number > 5:\n",
    "#             spamwriter.writerow(list_a)\n",
    "#         else:\n",
    "#             spamwriter.writerow(list_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e029db25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# root_directory = 'D:\\College\\CS_545\\Final_Project'\n",
    "# fp_csv_file = 'EEG_Eye_State.csv'\n",
    "# train_fraction = 0.8\n",
    "# validate_fraction = 0.1\n",
    "# X = pd.read_csv(os.path.join(root_directory,fp_csv_file), delimiter=',', usecols=range(15)).dropna(axis=0).to_numpy()\n",
    "# n_samples = X.shape[0]\n",
    "# rows = np.arange(n_samples)\n",
    "# np.random.shuffle(rows)\n",
    "\n",
    "# n_train = round(n_samples * train_fraction)\n",
    "# n_validate = round(n_samples * validate_fraction)\n",
    "\n",
    "# col_labels = [\"AF3\",\"F7\",\"F3\",\"FC5\",\"T7\",\"P7\",\"O1\",\"O2\",\"P8\",\"T8\",\"FC6\",\"F4\",\"F8\",\"AF4\",\"eyeDetection\"]\n",
    "# Xtrain = X[rows[:n_train], :]\n",
    "# Xvalidate = X[rows[n_train:n_train + n_validate], :]\n",
    "# Xtest = X[rows[n_train + n_validate:], :]\n",
    "# Xtrain = pd.DataFrame(Xtrain, columns = col_labels)\n",
    "# Xvalidate = pd.DataFrame(Xvalidate, columns = col_labels)\n",
    "# Xtest = pd.DataFrame(Xtest, columns = col_labels)\n",
    "# Xtrain = Xtrain.astype({\"eyeDetection\": int})\n",
    "# Xvalidate = Xvalidate.astype({\"eyeDetection\": int})\n",
    "# Xtest = Xtest.astype({\"eyeDetection\": int})\n",
    "# print(Xtrain.iloc[0])\n",
    "# print( type(X))\n",
    "# Xtrain.to_csv(\"EEG_Eye_State_Train.csv\", encoding='utf-8', index_label=False, index=False, quoting=csv.QUOTE_NONNUMERIC )\n",
    "# Xvalidate.to_csv(\"EEG_Eye_State_Validate.csv\", encoding='utf-8', index_label=False, index=False, quoting=csv.QUOTE_NONNUMERIC )\n",
    "# Xtest.to_csv(\"EEG_Eye_State_Test.csv\", encoding='utf-8', index_label=False, index=False, quoting=csv.QUOTE_NONNUMERIC )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb987a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AF3             4309.74\n",
      "F7              4052.82\n",
      "F3              4285.13\n",
      "FC5             4155.38\n",
      "T7              4369.23\n",
      "P7              4646.67\n",
      "O1              4100.00\n",
      "O2              4649.74\n",
      "P8              4221.54\n",
      "T8              4230.26\n",
      "FC6             4206.15\n",
      "F4              4292.82\n",
      "F8              4598.97\n",
      "AF4             4362.56\n",
      "eyeDetection       0.00\n",
      "Name: 0, dtype: float64\n",
      "0\n",
      "[[4309.74 4052.82 4285.13 4155.38 4369.23 4646.67 4100.   4649.74 4221.54\n",
      "  4230.26 4206.15 4292.82 4598.97 4362.56]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11984, 15)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "fp_csv_file = \"EEG_Eye_State_Train.csv\"\n",
    "# fp_csv_file = \"Dummy_Train.csv\"\n",
    "eeg_data = pd.read_csv(\n",
    "    os.path.join(root_directory, fp_csv_file), delimiter=\",\", usecols=range(15)\n",
    ").dropna(axis=0)\n",
    "\n",
    "print(eeg_data.iloc[0, :])\n",
    "open_or_closed = eeg_data.iloc[0, 14]\n",
    "channel_data = eeg_data.iloc[0, 0:14]\n",
    "channel_data = np.array([channel_data])\n",
    "channel_data = channel_data.astype(\"float\").reshape(-1, 14)\n",
    "print(open_or_closed)\n",
    "print(channel_data)\n",
    "eeg_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d7acdc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AF3             float64\n",
      "F7              float64\n",
      "F3              float64\n",
      "FC5             float64\n",
      "T7              float64\n",
      "P7              float64\n",
      "O1              float64\n",
      "O2              float64\n",
      "P8              float64\n",
      "T8              float64\n",
      "FC6             float64\n",
      "F4              float64\n",
      "F8              float64\n",
      "AF4             float64\n",
      "eyeDetection      int64\n",
      "dtype: object\n",
      "AF3             float64\n",
      "F7              float64\n",
      "F3              float64\n",
      "FC5             float64\n",
      "T7              float64\n",
      "P7              float64\n",
      "O1              float64\n",
      "O2              float64\n",
      "P8              float64\n",
      "T8              float64\n",
      "FC6             float64\n",
      "F4              float64\n",
      "F8              float64\n",
      "AF4             float64\n",
      "eyeDetection      int64\n",
      "dtype: object\n",
      "AF3             float64\n",
      "F7              float64\n",
      "F3              float64\n",
      "FC5             float64\n",
      "T7              float64\n",
      "P7              float64\n",
      "O1              float64\n",
      "O2              float64\n",
      "P8              float64\n",
      "T8              float64\n",
      "FC6             float64\n",
      "F4              float64\n",
      "F8              float64\n",
      "AF4             float64\n",
      "eyeDetection      int64\n",
      "dtype: object\n",
      "11984\n"
     ]
    }
   ],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "\n",
    "train_file = \"EEG_Eye_State_Train.csv\"\n",
    "validate_file = \"EEG_Eye_State_Validate.csv\"\n",
    "test_file = \"EEG_Eye_State_Test.csv\"\n",
    "\n",
    "# train_file = \"Dummy_Train.csv\"\n",
    "# validate_file = \"Dummy_Validate.csv\"\n",
    "# test_file = \"Dummy_Test.csv\"\n",
    "\n",
    "train_dataset = fpnn.FinalProjectEEGDataset(train_file, root_directory)\n",
    "validate_dataset = fpnn.FinalProjectEEGDataset(validate_file, root_directory)\n",
    "test_dataset = fpnn.FinalProjectEEGDataset(test_file, root_directory)\n",
    "print(train_dataset.__len__())\n",
    "train_dataset.__getitem__(0)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a360238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c653ec23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (Linear2): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = fpnn.NeuralNetwork(28 * 28, [512, 512], 10).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aceb2dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0034,  0.0779,  0.0361,  0.0688,  0.0196,  0.0945,  0.0134,  0.0490,\n",
      "         -0.0801,  0.0908]], grad_fn=<AddmmBackward>)\n",
      "Predicted class: tensor([5])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "print(logits)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55e5e4ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    # first = True\n",
    "    for batch, (X, T) in enumerate(dataloader):\n",
    "        X, T = X.to(device), T.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        Y = model(X.float())\n",
    "        # if first:\n",
    "        #     first = False\n",
    "        #     print('Y.shape', Y.shape)\n",
    "        #     print('T.shape', T.shape)\n",
    "        #     print('T: ', T[0])\n",
    "        loss = loss_fn(Y, T.long())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "#         if batch % 50 == 0:\n",
    "#             loss, current = loss.item(), batch * len(X)\n",
    "# print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17432e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, T in dataloader:\n",
    "            X, T = X.to(device), T.to(device)\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, T.long()).item()\n",
    "            correct += (pred.argmax(1) == T).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(\n",
    "        f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32ade0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_epochs(\n",
    "    epochs, train_dataloader, vaidate_dataloader, model, loss_fn, optimizer\n",
    "):\n",
    "    start = time.time()\n",
    "    for t in range(epochs):\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        if t % 50 == 0:\n",
    "            print(f\"Epoch {t}\\n-------------------------------\")\n",
    "            test(validate_dataloader, model, loss_fn)\n",
    "    end = time.time()\n",
    "    print(\"Done: \", end - start, \" seconds\")\n",
    "    test(validate_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41ba2342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fashion_training_data = datasets.FashionMNIST(\n",
    "#     root=\"data\",\n",
    "#     train=True,\n",
    "#     download=True,\n",
    "#     transform=ToTensor(),\n",
    "# )\n",
    "# fashion_train_dataloader = DataLoader(fashion_training_data, batch_size=64)\n",
    "# n_samples, n_inputs = train_dataset.__len__(), 28*28\n",
    "# n_outputs = 1\n",
    "# n_hiddens = [10, 10]\n",
    "# model = fpnn.NeuralNetwork(28*28, n_hiddens, 10).to(device)\n",
    "# print(model)\n",
    "\n",
    "# learning_rate = 0.5 / (n_samples * n_outputs)  ## Larger learning rate\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# train(fashion_train_dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3190dd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([64, 1, 14])\n",
      "Shape of y:  torch.Size([64]) torch.float64\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1082909f-a936-4f85-9713-3da18371ea33",
   "metadata": {},
   "source": [
    "I started out with a dummy run using dummy data that had a trivial pattern which the neural network should easily be able to learn. I did this to simply test that the network was correctly setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "319f87a6-a129-4ecf-b550-4e3a60b4cfb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AF3             int64\n",
      "F7              int64\n",
      "F3              int64\n",
      "FC5             int64\n",
      "T7              int64\n",
      "P7              int64\n",
      "O1              int64\n",
      "O2              int64\n",
      "P8              int64\n",
      "T8              int64\n",
      "FC6             int64\n",
      "F4              int64\n",
      "F8              int64\n",
      "AF4             int64\n",
      "eyeDetection    int64\n",
      "dtype: object\n",
      "AF3             int64\n",
      "F7              int64\n",
      "F3              int64\n",
      "FC5             int64\n",
      "T7              int64\n",
      "P7              int64\n",
      "O1              int64\n",
      "O2              int64\n",
      "P8              int64\n",
      "T8              int64\n",
      "FC6             int64\n",
      "F4              int64\n",
      "F8              int64\n",
      "AF4             int64\n",
      "eyeDetection    int64\n",
      "dtype: object\n",
      "AF3             int64\n",
      "F7              int64\n",
      "F3              int64\n",
      "FC5             int64\n",
      "T7              int64\n",
      "P7              int64\n",
      "O1              int64\n",
      "O2              int64\n",
      "P8              int64\n",
      "T8              int64\n",
      "FC6             int64\n",
      "F4              int64\n",
      "F8              int64\n",
      "AF4             int64\n",
      "eyeDetection    int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "\n",
    "# train_file = \"EEG_Eye_State_Train.csv\"\n",
    "# validate_file = \"EEG_Eye_State_Validate.csv\"\n",
    "# test_file = \"EEG_Eye_State_Test.csv\"\n",
    "\n",
    "train_file = \"Dummy_Train.csv\"\n",
    "validate_file = \"Dummy_Validate.csv\"\n",
    "test_file = \"Dummy_Test.csv\"\n",
    "\n",
    "train_dataset = fpnn.FinalProjectEEGDataset(train_file, root_directory)\n",
    "validate_dataset = fpnn.FinalProjectEEGDataset(validate_file, root_directory)\n",
    "test_dataset = fpnn.FinalProjectEEGDataset(test_file, root_directory)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9189f641-75c4-4724-a1ff-02c0a6df3a0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=14, out_features=5, bias=True)\n",
      "    (Sigmoid0): Sigmoid()\n",
      "    (Linear1): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (Sigmoid1): Sigmoid()\n",
      "    (Linear2): Linear(in_features=5, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 0.684092 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.058379 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.014168 \n",
      "\n",
      "Done:  17.34960651397705  seconds\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.014168 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 14\n",
    "n_outputs = 2\n",
    "n_hiddens = [5, 5]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"sigmoid\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 101\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "224f1662-7926-4429-a065-3f4fa8ac3509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.030920 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(validate_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28130031-3767-4640-8111-bb30cf31dd73",
   "metadata": {},
   "source": [
    "Unsurprisingly in this dummy run the model learned to predict with 100% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8788dca1-9457-40d3-9e4b-8b2ff7667588",
   "metadata": {},
   "source": [
    "I then moved on to the actual data to see what kind of accuracy I could get from a classical classification neural network model. I tried several structures and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "186cd8ca-6ec0-4ab1-8758-9aae7df8b876",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AF3             float64\n",
      "F7              float64\n",
      "F3              float64\n",
      "FC5             float64\n",
      "T7              float64\n",
      "P7              float64\n",
      "O1              float64\n",
      "O2              float64\n",
      "P8              float64\n",
      "T8              float64\n",
      "FC6             float64\n",
      "F4              float64\n",
      "F8              float64\n",
      "AF4             float64\n",
      "eyeDetection      int64\n",
      "dtype: object\n",
      "AF3             float64\n",
      "F7              float64\n",
      "F3              float64\n",
      "FC5             float64\n",
      "T7              float64\n",
      "P7              float64\n",
      "O1              float64\n",
      "O2              float64\n",
      "P8              float64\n",
      "T8              float64\n",
      "FC6             float64\n",
      "F4              float64\n",
      "F8              float64\n",
      "AF4             float64\n",
      "eyeDetection      int64\n",
      "dtype: object\n",
      "AF3             float64\n",
      "F7              float64\n",
      "F3              float64\n",
      "FC5             float64\n",
      "T7              float64\n",
      "P7              float64\n",
      "O1              float64\n",
      "O2              float64\n",
      "P8              float64\n",
      "T8              float64\n",
      "FC6             float64\n",
      "F4              float64\n",
      "F8              float64\n",
      "AF4             float64\n",
      "eyeDetection      int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "\n",
    "train_file = \"EEG_Eye_State_Train.csv\"\n",
    "validate_file = \"EEG_Eye_State_Validate.csv\"\n",
    "test_file = \"EEG_Eye_State_Test.csv\"\n",
    "\n",
    "# train_file = \"Dummy_Train.csv\"\n",
    "# validate_file = \"Dummy_Validate.csv\"\n",
    "# test_file = \"Dummy_Test.csv\"\n",
    "\n",
    "train_dataset = fpnn.FinalProjectEEGDataset(train_file, root_directory)\n",
    "validate_dataset = fpnn.FinalProjectEEGDataset(validate_file, root_directory)\n",
    "test_dataset = fpnn.FinalProjectEEGDataset(test_file, root_directory)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "796d2033-3bac-4f43-be51-a1e49e188983",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=14, out_features=5, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (Linear2): Linear(in_features=5, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.687582 \n",
      "\n",
      "Done:  24.560352325439453  seconds\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.685850 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 14\n",
    "n_outputs = 2\n",
    "n_hiddens = [5, 5]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs).to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "211af310-64c5-4c9f-964d-9ec13b2c1bbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i == 0\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=14, out_features=10, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (Linear2): Linear(in_features=10, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 46.4%, Avg loss: 10.459651 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 3.271880 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 1.862043 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.685989 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.685970 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.685949 \n",
      "\n",
      "Done:  1008.1690013408661  seconds\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 14\n",
    "n_outputs = 2\n",
    "n_hiddens = [10, 10]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs).to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 501\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80d29813-9c38-4d11-9fff-51591a825449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i == 0\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=14, out_features=30, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (Linear2): Linear(in_features=20, out_features=15, bias=True)\n",
      "    (ReLU2): ReLU()\n",
      "    (Linear3): Linear(in_features=15, out_features=10, bias=True)\n",
      "    (ReLU3): ReLU()\n",
      "    (Linear4): Linear(in_features=10, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 1.173773 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.685931 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.685854 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.685854 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.685854 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.685854 \n",
      "\n",
      "Done:  1033.4101610183716  seconds\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 14\n",
    "n_outputs = 2\n",
    "n_hiddens = [30, 20, 15, 10]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs).to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 501\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0647adb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i == 0\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=14, out_features=500, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (Linear2): Linear(in_features=500, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 6.962534 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 1.713657 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 1.713657 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 1.713657 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 1.713657 \n",
      "\n",
      "Done:  1214.9345533847809  seconds\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 14\n",
    "n_outputs = 2\n",
    "n_hiddens = [500, 500]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs).to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 500\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59d3a9c2-790c-4189-afa0-71999820f1b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i == 0\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=14, out_features=500, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (Linear2): Linear(in_features=500, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 288.282979 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 54.4%, Avg loss: 0.785713 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 0.756815 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.1%, Avg loss: 0.722233 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.7%, Avg loss: 0.705566 \n",
      "\n",
      "Done:  1155.8505518436432  seconds\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 14\n",
    "n_outputs = 2\n",
    "n_hiddens = [500, 500]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs).to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.01 / (n_samples * n_outputs)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 500\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9571821-5d3d-4dd7-8c10-0ac977e515d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i == 0\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=14, out_features=500, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (Linear2): Linear(in_features=500, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss:      nan \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss:      nan \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss:      nan \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss:      nan \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss:      nan \n",
      "\n",
      "Done:  1099.8602554798126  seconds\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 14\n",
    "n_outputs = 2\n",
    "n_hiddens = [500, 500]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs).to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.01 / (n_samples * n_outputs)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 500\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bd522c6-9a7e-4118-a2fd-5dee071dce3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i == 0\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=14, out_features=500, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (Linear2): Linear(in_features=500, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 77656832.685102 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 77656832.683104 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 77656832.681246 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 77656832.679520 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 77656832.677915 \n",
      "\n",
      "Done:  1064.0559866428375  seconds\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 14\n",
    "n_outputs = 2\n",
    "n_hiddens = [500, 500]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs).to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.01 / (n_samples * n_outputs)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 500\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14307458-ea6d-45d2-8d60-d871ad8aefbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=14, out_features=500, bias=True)\n",
      "    (Tanh0): Tanh()\n",
      "    (Linear1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (Tanh1): Tanh()\n",
      "    (Linear2): Linear(in_features=500, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 0.755901 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.686656 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 0.686432 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 0.686410 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 0.686408 \n",
      "\n",
      "Done:  1249.532361984253  seconds\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 14\n",
    "n_outputs = 2\n",
    "n_hiddens = [500, 500]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"tanh\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.01 / (n_samples * n_outputs)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 500\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec6a867d-a40b-4f0e-bf62-3768ef8fd61a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=14, out_features=500, bias=True)\n",
      "    (Sigmoid0): Sigmoid()\n",
      "    (Linear1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (Sigmoid1): Sigmoid()\n",
      "    (Linear2): Linear(in_features=500, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 0.670922 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.006674 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.002450 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.001424 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000983 \n",
      "\n",
      "Done:  94.19774389266968  seconds\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 14\n",
    "n_outputs = 2\n",
    "n_hiddens = [500, 500]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"sigmoid\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.01 / (n_samples * n_outputs)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 500\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b9ab2-f244-4dfd-8b51-c45a8c7e334f",
   "metadata": {},
   "source": [
    "Unfortunately this approach didn't seem to produce very good results. The models were barely better than chance, with he best of them achieving a final accuracy of 56%. They also didn't seem to learn much after the initial few epochs. I decided to try a different approach going forward. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020bb429-3120-42a6-827a-04690303d340",
   "metadata": {},
   "source": [
    "## Including Mean and Standard Deviation Approach Experimentation ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd50c9e6-b442-4021-9db7-15c993e2ba91",
   "metadata": {},
   "source": [
    "I started by removing any glaring outliers from the data. I thought this might make it a little easier for the neural network to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45c6c874-4cc1-459b-ace9-28575ee0e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# fp_csv_file = \"EEG_Eye_State.csv\"\n",
    "# data = pd.read_csv(\n",
    "#     os.path.join(root_directory, fp_csv_file), delimiter=\",\", usecols=range(15)\n",
    "# ).dropna(axis=0)\n",
    "\n",
    "# # data = pd.read_csv(os.path.join(root_directory, fp_csv_file), header=None)\n",
    "# # retrieve data as numpy array\n",
    "# values = data.values\n",
    "\n",
    "\n",
    "# print(values.shape)\n",
    "# print(values[0:5])\n",
    "# # step over each EEG column\n",
    "# for i in range(values.shape[1] - 1):\n",
    "#     # calculate column mean and standard deviation\n",
    "#     data_mean, data_std = np.mean(values[:, i]), np.std(values[:, i])\n",
    "#     # define outlier bounds\n",
    "#     cut_off = data_std * 4\n",
    "#     lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "#     # remove too small\n",
    "#     too_small = [j for j in range(values.shape[0]) if values[j, i] < lower]\n",
    "#     values = np.delete(values, too_small, 0)\n",
    "#     print(\">deleted %d rows\" % len(too_small))\n",
    "#     # remove too large\n",
    "#     too_large = [j for j in range(values.shape[0]) if values[j, i] > upper]\n",
    "#     values = np.delete(values, too_large, 0)\n",
    "#     print(\">deleted %d rows\" % len(too_large))\n",
    "# # save the results to a new file\n",
    "# col_labels = [\n",
    "#     \"AF3\",\n",
    "#     \"F7\",\n",
    "#     \"F3\",\n",
    "#     \"FC5\",\n",
    "#     \"T7\",\n",
    "#     \"P7\",\n",
    "#     \"O1\",\n",
    "#     \"O2\",\n",
    "#     \"P8\",\n",
    "#     \"T8\",\n",
    "#     \"FC6\",\n",
    "#     \"F4\",\n",
    "#     \"F8\",\n",
    "#     \"AF4\",\n",
    "#     \"eyeDetection\",\n",
    "# ]\n",
    "# values = pd.DataFrame(values, columns=data.columns)\n",
    "# values.to_csv(\n",
    "#     \"EEG_Eye_State_No_Outliers.csv\",\n",
    "#     encoding=\"utf-8\",\n",
    "#     index_label=False,\n",
    "#     index=False,\n",
    "#     quoting=csv.QUOTE_NONNUMERIC,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e61655-837e-4b78-9593-28d690219a08",
   "metadata": {},
   "source": [
    "After that I decided to try and investigate the data at a more meta level. I calculated the mean, standard deviation, maximum value, and minimum value for the eye-open state data and eye-closed state data. I also calculated the mean and standard deviation for the data as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73c5bad3-8878-4e7d-b1b2-196a32da76ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(file_directory):\n",
    "    data = pd.read_csv(file_directory, delimiter=\",\", usecols=range(15)).dropna(axis=0)\n",
    "\n",
    "    metadata_labels = [\n",
    "        \"Channel\",\n",
    "        \"Mean\",\n",
    "        \"Std\",\n",
    "        \"Max\",\n",
    "        \"Min\",\n",
    "        \"Eye-Open Mean\",\n",
    "        \"Eye-Open Std\",\n",
    "        \"Eye-Open Max\",\n",
    "        \"Eye-Open Min\",\n",
    "        \"Eye-Closed Mean\",\n",
    "        \"Eye-Closed Std\",\n",
    "        \"Eye-Closed Max\",\n",
    "        \"Eye-Closed Min\",\n",
    "    ]\n",
    "\n",
    "    metadata = [\n",
    "        [\"AF3\"],\n",
    "        [\"F7\"],\n",
    "        [\"F3\"],\n",
    "        [\"FC5\"],\n",
    "        [\"T7\"],\n",
    "        [\"P7\"],\n",
    "        [\"O1\"],\n",
    "        [\"O2\"],\n",
    "        [\"P8\"],\n",
    "        [\"T8\"],\n",
    "        [\"FC6\"],\n",
    "        [\"F4\"],\n",
    "        [\"F8\"],\n",
    "        [\"AF4\"],\n",
    "    ]\n",
    "    values = data.values\n",
    "    print(values.shape)\n",
    "    # step over each EEG column\n",
    "    for i in range(values.shape[1] - 1):\n",
    "        # calculate column mean and standard deviation\n",
    "        data_mean, data_std, data_max, data_min = (\n",
    "            np.mean(values[:, i]),\n",
    "            np.std(values[:, i]),\n",
    "            np.max(values[:, i]),\n",
    "            np.min(values[:, i]),\n",
    "        )\n",
    "        metadata[i].append(data_mean)\n",
    "        metadata[i].append(data_std)\n",
    "        metadata[i].append(data_max)\n",
    "        metadata[i].append(data_min)\n",
    "\n",
    "    zeros = []\n",
    "    ones = []\n",
    "    for sample in values:\n",
    "        if sample[14] == 0:\n",
    "            zeros.append(sample)\n",
    "        else:\n",
    "            ones.append(sample)\n",
    "    zeros = pd.DataFrame(zeros)\n",
    "    ones = pd.DataFrame(ones)\n",
    "\n",
    "    zeros.columns = data.columns\n",
    "    ones.columns = data.columns\n",
    "    print(zeros.shape)\n",
    "    zeros_values = zeros.values\n",
    "    for i in range(zeros_values.shape[1] - 1):\n",
    "        # calculate column mean and standard deviation\n",
    "        data_mean, data_std, data_max, data_min = (\n",
    "            np.mean(zeros_values[:, i]),\n",
    "            np.std(zeros_values[:, i]),\n",
    "            np.max(zeros_values[:, i]),\n",
    "            np.min(zeros_values[:, i]),\n",
    "        )\n",
    "        metadata[i].append(data_mean)\n",
    "        metadata[i].append(data_std)\n",
    "        metadata[i].append(data_max)\n",
    "        metadata[i].append(data_min)\n",
    "\n",
    "    print(ones.shape)\n",
    "    ones_values = ones.values\n",
    "    for i in range(ones_values.shape[1] - 1):\n",
    "        # calculate column mean and standard deviation\n",
    "        data_mean, data_std, data_max, data_min = (\n",
    "            np.mean(ones_values[:, i]),\n",
    "            np.std(ones_values[:, i]),\n",
    "            np.max(ones_values[:, i]),\n",
    "            np.min(ones_values[:, i]),\n",
    "        )\n",
    "        metadata[i].append(data_mean)\n",
    "        metadata[i].append(data_std)\n",
    "        metadata[i].append(data_max)\n",
    "        metadata[i].append(data_min)\n",
    "\n",
    "    metadata = pd.DataFrame(metadata)\n",
    "    metadata.columns = metadata_labels\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e012b50f-cc3b-4100-927b-dd55e0c004ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14304, 15)\n",
      "(7855, 15)\n",
      "(6449, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "      <th>Max</th>\n",
       "      <th>Min</th>\n",
       "      <th>Eye-Open Mean</th>\n",
       "      <th>Eye-Open Std</th>\n",
       "      <th>Eye-Open Max</th>\n",
       "      <th>Eye-Open Min</th>\n",
       "      <th>Eye-Closed Mean</th>\n",
       "      <th>Eye-Closed Std</th>\n",
       "      <th>Eye-Closed Max</th>\n",
       "      <th>Eye-Closed Min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AF3</td>\n",
       "      <td>4298.144530</td>\n",
       "      <td>32.383173</td>\n",
       "      <td>4466.15</td>\n",
       "      <td>4198.97</td>\n",
       "      <td>4294.614386</td>\n",
       "      <td>33.537700</td>\n",
       "      <td>4466.15</td>\n",
       "      <td>4206.67</td>\n",
       "      <td>4302.444309</td>\n",
       "      <td>30.369437</td>\n",
       "      <td>4441.03</td>\n",
       "      <td>4198.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F7</td>\n",
       "      <td>4007.199968</td>\n",
       "      <td>27.222342</td>\n",
       "      <td>4154.36</td>\n",
       "      <td>3913.33</td>\n",
       "      <td>4009.736631</td>\n",
       "      <td>28.256648</td>\n",
       "      <td>4154.36</td>\n",
       "      <td>3924.10</td>\n",
       "      <td>4004.110265</td>\n",
       "      <td>25.569113</td>\n",
       "      <td>4138.97</td>\n",
       "      <td>3913.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F3</td>\n",
       "      <td>4261.656384</td>\n",
       "      <td>16.219297</td>\n",
       "      <td>4349.23</td>\n",
       "      <td>4197.44</td>\n",
       "      <td>4260.402014</td>\n",
       "      <td>16.607684</td>\n",
       "      <td>4349.23</td>\n",
       "      <td>4197.44</td>\n",
       "      <td>4263.184229</td>\n",
       "      <td>15.597616</td>\n",
       "      <td>4333.85</td>\n",
       "      <td>4212.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FC5</td>\n",
       "      <td>4120.176654</td>\n",
       "      <td>16.904783</td>\n",
       "      <td>4191.79</td>\n",
       "      <td>4067.18</td>\n",
       "      <td>4120.948880</td>\n",
       "      <td>16.536333</td>\n",
       "      <td>4187.18</td>\n",
       "      <td>4073.33</td>\n",
       "      <td>4119.236069</td>\n",
       "      <td>17.296483</td>\n",
       "      <td>4191.79</td>\n",
       "      <td>4067.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T7</td>\n",
       "      <td>4339.673482</td>\n",
       "      <td>11.994233</td>\n",
       "      <td>4397.95</td>\n",
       "      <td>4308.72</td>\n",
       "      <td>4339.912341</td>\n",
       "      <td>11.199247</td>\n",
       "      <td>4395.90</td>\n",
       "      <td>4308.72</td>\n",
       "      <td>4339.382548</td>\n",
       "      <td>12.890536</td>\n",
       "      <td>4397.95</td>\n",
       "      <td>4309.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P7</td>\n",
       "      <td>4618.165981</td>\n",
       "      <td>12.923532</td>\n",
       "      <td>4672.31</td>\n",
       "      <td>4566.15</td>\n",
       "      <td>4619.437780</td>\n",
       "      <td>13.217759</td>\n",
       "      <td>4671.28</td>\n",
       "      <td>4566.15</td>\n",
       "      <td>4616.616906</td>\n",
       "      <td>12.380614</td>\n",
       "      <td>4672.31</td>\n",
       "      <td>4581.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>O1</td>\n",
       "      <td>4070.977384</td>\n",
       "      <td>17.707910</td>\n",
       "      <td>4138.97</td>\n",
       "      <td>4026.15</td>\n",
       "      <td>4070.531945</td>\n",
       "      <td>14.417633</td>\n",
       "      <td>4124.10</td>\n",
       "      <td>4027.18</td>\n",
       "      <td>4071.519936</td>\n",
       "      <td>21.018584</td>\n",
       "      <td>4138.97</td>\n",
       "      <td>4026.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>O2</td>\n",
       "      <td>4614.060973</td>\n",
       "      <td>14.398261</td>\n",
       "      <td>4672.82</td>\n",
       "      <td>4567.18</td>\n",
       "      <td>4613.460053</td>\n",
       "      <td>13.655753</td>\n",
       "      <td>4672.82</td>\n",
       "      <td>4567.18</td>\n",
       "      <td>4614.792904</td>\n",
       "      <td>15.221895</td>\n",
       "      <td>4672.82</td>\n",
       "      <td>4567.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P8</td>\n",
       "      <td>4199.641958</td>\n",
       "      <td>13.807744</td>\n",
       "      <td>4255.90</td>\n",
       "      <td>4147.69</td>\n",
       "      <td>4198.939306</td>\n",
       "      <td>12.424151</td>\n",
       "      <td>4255.90</td>\n",
       "      <td>4152.31</td>\n",
       "      <td>4200.497801</td>\n",
       "      <td>15.281592</td>\n",
       "      <td>4255.38</td>\n",
       "      <td>4147.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>T8</td>\n",
       "      <td>4229.291521</td>\n",
       "      <td>15.005626</td>\n",
       "      <td>4288.21</td>\n",
       "      <td>4170.26</td>\n",
       "      <td>4227.845602</td>\n",
       "      <td>14.217735</td>\n",
       "      <td>4287.69</td>\n",
       "      <td>4170.26</td>\n",
       "      <td>4231.052678</td>\n",
       "      <td>15.734208</td>\n",
       "      <td>4288.21</td>\n",
       "      <td>4174.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FC6</td>\n",
       "      <td>4199.729761</td>\n",
       "      <td>18.338192</td>\n",
       "      <td>4271.28</td>\n",
       "      <td>4125.13</td>\n",
       "      <td>4197.989321</td>\n",
       "      <td>17.985427</td>\n",
       "      <td>4264.10</td>\n",
       "      <td>4125.13</td>\n",
       "      <td>4201.849648</td>\n",
       "      <td>18.539501</td>\n",
       "      <td>4271.28</td>\n",
       "      <td>4130.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F4</td>\n",
       "      <td>4276.866449</td>\n",
       "      <td>15.089147</td>\n",
       "      <td>4332.31</td>\n",
       "      <td>4216.41</td>\n",
       "      <td>4274.783659</td>\n",
       "      <td>15.042807</td>\n",
       "      <td>4329.74</td>\n",
       "      <td>4216.41</td>\n",
       "      <td>4279.403325</td>\n",
       "      <td>14.753427</td>\n",
       "      <td>4332.31</td>\n",
       "      <td>4225.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>F8</td>\n",
       "      <td>4603.123432</td>\n",
       "      <td>25.535726</td>\n",
       "      <td>4716.41</td>\n",
       "      <td>4490.77</td>\n",
       "      <td>4599.781686</td>\n",
       "      <td>23.835106</td>\n",
       "      <td>4711.79</td>\n",
       "      <td>4490.77</td>\n",
       "      <td>4607.193740</td>\n",
       "      <td>26.910407</td>\n",
       "      <td>4716.41</td>\n",
       "      <td>4510.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AF4</td>\n",
       "      <td>4358.072562</td>\n",
       "      <td>31.876578</td>\n",
       "      <td>4491.28</td>\n",
       "      <td>4236.41</td>\n",
       "      <td>4353.527338</td>\n",
       "      <td>32.223892</td>\n",
       "      <td>4491.28</td>\n",
       "      <td>4236.41</td>\n",
       "      <td>4363.608728</td>\n",
       "      <td>30.548114</td>\n",
       "      <td>4487.69</td>\n",
       "      <td>4246.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Channel         Mean        Std      Max      Min  Eye-Open Mean  \\\n",
       "0      AF3  4298.144530  32.383173  4466.15  4198.97    4294.614386   \n",
       "1       F7  4007.199968  27.222342  4154.36  3913.33    4009.736631   \n",
       "2       F3  4261.656384  16.219297  4349.23  4197.44    4260.402014   \n",
       "3      FC5  4120.176654  16.904783  4191.79  4067.18    4120.948880   \n",
       "4       T7  4339.673482  11.994233  4397.95  4308.72    4339.912341   \n",
       "5       P7  4618.165981  12.923532  4672.31  4566.15    4619.437780   \n",
       "6       O1  4070.977384  17.707910  4138.97  4026.15    4070.531945   \n",
       "7       O2  4614.060973  14.398261  4672.82  4567.18    4613.460053   \n",
       "8       P8  4199.641958  13.807744  4255.90  4147.69    4198.939306   \n",
       "9       T8  4229.291521  15.005626  4288.21  4170.26    4227.845602   \n",
       "10     FC6  4199.729761  18.338192  4271.28  4125.13    4197.989321   \n",
       "11      F4  4276.866449  15.089147  4332.31  4216.41    4274.783659   \n",
       "12      F8  4603.123432  25.535726  4716.41  4490.77    4599.781686   \n",
       "13     AF4  4358.072562  31.876578  4491.28  4236.41    4353.527338   \n",
       "\n",
       "    Eye-Open Std  Eye-Open Max  Eye-Open Min  Eye-Closed Mean  Eye-Closed Std  \\\n",
       "0      33.537700       4466.15       4206.67      4302.444309       30.369437   \n",
       "1      28.256648       4154.36       3924.10      4004.110265       25.569113   \n",
       "2      16.607684       4349.23       4197.44      4263.184229       15.597616   \n",
       "3      16.536333       4187.18       4073.33      4119.236069       17.296483   \n",
       "4      11.199247       4395.90       4308.72      4339.382548       12.890536   \n",
       "5      13.217759       4671.28       4566.15      4616.616906       12.380614   \n",
       "6      14.417633       4124.10       4027.18      4071.519936       21.018584   \n",
       "7      13.655753       4672.82       4567.18      4614.792904       15.221895   \n",
       "8      12.424151       4255.90       4152.31      4200.497801       15.281592   \n",
       "9      14.217735       4287.69       4170.26      4231.052678       15.734208   \n",
       "10     17.985427       4264.10       4125.13      4201.849648       18.539501   \n",
       "11     15.042807       4329.74       4216.41      4279.403325       14.753427   \n",
       "12     23.835106       4711.79       4490.77      4607.193740       26.910407   \n",
       "13     32.223892       4491.28       4236.41      4363.608728       30.548114   \n",
       "\n",
       "    Eye-Closed Max  Eye-Closed Min  \n",
       "0          4441.03         4198.97  \n",
       "1          4138.97         3913.33  \n",
       "2          4333.85         4212.31  \n",
       "3          4191.79         4067.18  \n",
       "4          4397.95         4309.74  \n",
       "5          4672.31         4581.03  \n",
       "6          4138.97         4026.15  \n",
       "7          4672.82         4567.69  \n",
       "8          4255.38         4147.69  \n",
       "9          4288.21         4174.36  \n",
       "10         4271.28         4130.77  \n",
       "11         4332.31         4225.64  \n",
       "12         4716.41         4510.26  \n",
       "13         4487.69         4246.15  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "fp_csv_file = \"EEG_Eye_State_No_Outliers.csv\"\n",
    "metadata = get_metadata(os.path.join(root_directory, fp_csv_file))\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034266ed-f22d-41ab-91cf-731e332f7488",
   "metadata": {},
   "source": [
    "The differences between various channel metadata values in the eye-open versus eye-closed date tended to be very small. They did seem to be present though. Given this I thought it would be interesting to add these values as inputs to the neural network. I thought adding these as a feature might give the neural network some information that would make it easier to identify trends between the states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0585ad-872d-46f9-8e65-36ee2f5bbc73",
   "metadata": {},
   "source": [
    "I started out by just adding the mean and standard deviation for the data as a whole as one of the data points for each sample. To avoid passing information from validate or test into train I first split up the data into train, validate, and test sets. I decided to preserve the temporal ordering this time when splitting up the data. For each of these new datasets I then calculated the overall mean and standard deviation for each channel and added those as column data in each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a93f8a1-1515-4981-bcde-f5e7c93c496b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AF3', 4298.144529502238, 32.383172751002164, 4466.15, 4198.97,\n",
       "       4294.6143857415655, 33.53769989310272, 4466.15, 4206.67,\n",
       "       4302.444309195224, 30.369436627795682, 4441.03, 4198.97],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.iloc[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57ca8af8-705b-4cf4-b710-933c32ca12aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14304, 15)\n",
      "['AF3', 'AF3 Mean', 'AF3 Std', 'AF3 Max', 'AF3 Min', 'F7', 'F7 Mean', 'F7 Std', 'F7 Max', 'F7 Min', 'F3', 'F3 Mean', 'F3 Std', 'F3 Max', 'F3 Min', 'FC5', 'FC5 Mean', 'FC5 Std', 'FC5 Max', 'FC5 Min', 'T7', 'T7 Mean', 'T7 Std', 'T7 Max', 'T7 Min', 'P7', 'P7 Mean', 'P7 Std', 'P7 Max', 'P7 Min', 'O1', 'O1 Mean', 'O1 Std', 'O1 Max', 'O1 Min', 'O2', 'O2 Mean', 'O2 Std', 'O2 Max', 'O2 Min', 'P8', 'P8 Mean', 'P8 Std', 'P8 Max', 'P8 Min', 'T8', 'T8 Mean', 'T8 Std', 'T8 Max', 'T8 Min', 'FC6', 'FC6 Mean', 'FC6 Std', 'FC6 Max', 'FC6 Min', 'F4', 'F4 Mean', 'F4 Std', 'F4 Max', 'F4 Min', 'F8', 'F8 Mean', 'F8 Std', 'F8 Max', 'F8 Min', 'AF4', 'AF4 Mean', 'AF4 Std', 'AF4 Max', 'AF4 Min', 'eyeDetection']\n",
      "14304\n"
     ]
    }
   ],
   "source": [
    "# root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# fp_csv_file = \"EEG_Eye_State_No_Outliers.csv\"\n",
    "# data = pd.read_csv(\n",
    "#     os.path.join(root_directory, fp_csv_file), delimiter=\",\", usecols=range(15)\n",
    "# ).dropna(axis=0)\n",
    "\n",
    "# col_labels = []\n",
    "# print(data.shape)\n",
    "# for i in range(metadata.shape[0]):\n",
    "#     col_labels.append(metadata.iloc[i].values[0])\n",
    "#     col_labels.append(metadata.iloc[i].values[0] + \" \" + metadata.columns[1])\n",
    "#     col_labels.append(metadata.iloc[i].values[0] + \" \" + metadata.columns[2])\n",
    "#     col_labels.append(metadata.iloc[i].values[0] + \" \" + metadata.columns[3])\n",
    "#     col_labels.append(metadata.iloc[i].values[0] + \" \" + metadata.columns[4])\n",
    "\n",
    "# col_labels.append(\"eyeDetection\")\n",
    "# print(col_labels)\n",
    "\n",
    "# new_data = []\n",
    "# # num = 0\n",
    "# for index, row in data.iterrows():\n",
    "#     # if num == 100:\n",
    "#     #     break\n",
    "#     # num += num\n",
    "#     temp_list = []\n",
    "#     for i in range(len(row)):\n",
    "#         temp_list.append(row[i])\n",
    "#         if i < len(row) - 1:\n",
    "#             temp_list.append(metadata.iloc[i].values[1])\n",
    "#             temp_list.append(metadata.iloc[i].values[2])\n",
    "#             temp_list.append(metadata.iloc[i].values[3])\n",
    "#             temp_list.append(metadata.iloc[i].values[4])\n",
    "#     new_data.append(temp_list)\n",
    "# print(len(new_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87864d6b-1e5c-42e4-a1c3-123e7e600bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AF3             4329.23\n",
      "F7              4009.23\n",
      "F3              4289.23\n",
      "FC5             4148.21\n",
      "T7              4350.26\n",
      "P7              4586.15\n",
      "O1              4096.92\n",
      "O2              4641.03\n",
      "P8              4222.05\n",
      "T8              4238.46\n",
      "FC6             4211.28\n",
      "F4              4280.51\n",
      "F8              4635.90\n",
      "AF4             4393.85\n",
      "eyeDetection       0.00\n",
      "Name: 0, dtype: float64\n",
      "[4329.23, 4298.144529502238, 32.383172751002164, 4466.15, 4198.97, 4009.23, 4007.1999678411635, 27.222342363737564, 4154.36, 3913.33, 4289.23, 4261.656383529083, 16.2192974906191, 4349.23, 4197.44, 4148.21, 4120.176654082774, 16.90478308219174, 4191.79, 4067.18, 4350.26, 4339.673482242729, 11.99423301793753, 4397.95, 4308.72, 4586.15, 4618.165980844519, 12.923532182692702, 4672.31, 4566.15, 4096.92, 4070.977383948546, 17.707909625094178, 4138.97, 4026.15, 4641.03, 4614.060973154362, 14.39826115365047, 4672.82, 4567.18, 4222.05, 4199.641958193512, 13.807743808585931, 4255.9, 4147.69, 4238.46, 4229.291521252797, 15.005626018237718, 4288.21, 4170.26, 4211.28, 4199.729760906041, 18.338191607879672, 4271.28, 4125.13, 4280.51, 4276.866448545861, 15.089147439628482, 4332.31, 4216.41, 4635.9, 4603.123431907159, 25.53572578844711, 4716.41, 4490.77, 4393.85, 4358.072562220358, 31.876578172532213, 4491.28, 4236.41, 0.0]\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "# print(data.iloc[0])\n",
    "# print(new_data[0])\n",
    "# print(len(new_data[0]))\n",
    "# new_data_df = pd.DataFrame(new_data, columns=col_labels)\n",
    "# new_data_df = new_data_df.round(2)\n",
    "# new_data_df = new_data_df.astype({\"eyeDetection\": int})\n",
    "# new_data_df.to_csv(\n",
    "#     \"EEG_Eye_State_Mean_Std.csv\",\n",
    "#     encoding=\"utf-8\",\n",
    "#     index_label=False,\n",
    "#     index=False,\n",
    "#     quoting=csv.QUOTE_NONNUMERIC,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23268110-f43e-4378-93a6-c985bfdf9fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AF3             4329.23\n",
      "AF3 Mean        4298.14\n",
      "AF3 Std           32.38\n",
      "AF3 Max         4466.15\n",
      "AF3 Min         4198.97\n",
      "                 ...   \n",
      "AF4 Mean        4358.07\n",
      "AF4 Std           31.88\n",
      "AF4 Max         4491.28\n",
      "AF4 Min         4236.41\n",
      "eyeDetection       0.00\n",
      "Name: 0, Length: 71, dtype: float64\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# fp_csv_file = \"EEG_Eye_State_Mean_Std.csv\"\n",
    "# data = pd.read_csv(\n",
    "#     os.path.join(root_directory, fp_csv_file), delimiter=\",\", usecols=range(71)\n",
    "# ).dropna(axis=0)\n",
    "\n",
    "# train_fraction = 0.8\n",
    "# validate_fraction = 0.1\n",
    "# X = (\n",
    "#     pd.read_csv(\n",
    "#         os.path.join(root_directory, fp_csv_file), delimiter=\",\", usecols=range(71)\n",
    "#     )\n",
    "#     .dropna(axis=0)\n",
    "#     .to_numpy()\n",
    "# )\n",
    "# n_samples = X.shape[0]\n",
    "\n",
    "# n_train = round(n_samples * train_fraction)\n",
    "# n_validate = round(n_samples * validate_fraction)\n",
    "\n",
    "# Xtrain = X[:n_train]\n",
    "# Xvalidate = X[n_train : n_train + n_validate]\n",
    "# Xtest = X[n_train + n_validate :]\n",
    "# Xtrain = pd.DataFrame(Xtrain, columns=col_labels)\n",
    "# Xvalidate = pd.DataFrame(Xvalidate, columns=col_labels)\n",
    "# Xtest = pd.DataFrame(Xtest, columns=col_labels)\n",
    "# Xtrain = Xtrain.astype({\"eyeDetection\": int})\n",
    "# Xvalidate = Xvalidate.astype({\"eyeDetection\": int})\n",
    "# Xtest = Xtest.astype({\"eyeDetection\": int})\n",
    "# print(Xtrain.iloc[0])\n",
    "# print(type(X))\n",
    "# Xtrain.to_csv(\n",
    "#     \"EEG_Eye_State_Mean_Std_Train.csv\",\n",
    "#     encoding=\"utf-8\",\n",
    "#     index_label=False,\n",
    "#     index=False,\n",
    "#     quoting=csv.QUOTE_NONNUMERIC,\n",
    "# )\n",
    "# Xvalidate.to_csv(\n",
    "#     \"EEG_Eye_State_Mean_Std_Validate.csv\",\n",
    "#     encoding=\"utf-8\",\n",
    "#     index_label=False,\n",
    "#     index=False,\n",
    "#     quoting=csv.QUOTE_NONNUMERIC,\n",
    "# )\n",
    "# Xtest.to_csv(\n",
    "#     \"EEG_Eye_State_Mean_Std_Test.csv\",\n",
    "#     encoding=\"utf-8\",\n",
    "#     index_label=False,\n",
    "#     index=False,\n",
    "#     quoting=csv.QUOTE_NONNUMERIC,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9e2d50a0-a7e0-493f-9c3d-475ff56d23c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AF3             float64\n",
      "AF3 Mean        float64\n",
      "AF3 Std         float64\n",
      "AF3 Max         float64\n",
      "AF3 Min         float64\n",
      "                 ...   \n",
      "AF4 Mean        float64\n",
      "AF4 Std         float64\n",
      "AF4 Max         float64\n",
      "AF4 Min         float64\n",
      "eyeDetection      int64\n",
      "Length: 71, dtype: object\n",
      "AF3             float64\n",
      "AF3 Mean        float64\n",
      "AF3 Std         float64\n",
      "AF3 Max         float64\n",
      "AF3 Min         float64\n",
      "                 ...   \n",
      "AF4 Mean        float64\n",
      "AF4 Std         float64\n",
      "AF4 Max         float64\n",
      "AF4 Min         float64\n",
      "eyeDetection      int64\n",
      "Length: 71, dtype: object\n",
      "AF3             float64\n",
      "AF3 Mean        float64\n",
      "AF3 Std         float64\n",
      "AF3 Max         float64\n",
      "AF3 Min         float64\n",
      "                 ...   \n",
      "AF4 Mean        float64\n",
      "AF4 Std         float64\n",
      "AF4 Max         float64\n",
      "AF4 Min         float64\n",
      "eyeDetection      int64\n",
      "Length: 71, dtype: object\n"
     ]
    }
   ],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "\n",
    "train_file = \"EEG_Eye_State_Mean_Std_Train.csv\"\n",
    "validate_file = \"EEG_Eye_State_Mean_Std_Validate.csv\"\n",
    "test_file = \"EEG_Eye_State_Mean_Std_Test.csv\"\n",
    "\n",
    "# train_file = \"Dummy_Train.csv\"\n",
    "# validate_file = \"Dummy_Validate.csv\"\n",
    "# test_file = \"Dummy_Test.csv\"\n",
    "\n",
    "train_dataset = fpnn.FinalProjectEEGDataset(train_file, root_directory)\n",
    "validate_dataset = fpnn.FinalProjectEEGDataset(validate_file, root_directory)\n",
    "test_dataset = fpnn.FinalProjectEEGDataset(test_file, root_directory)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "50c60fd8-8d75-4d5f-8b2d-bce352c41cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=70, out_features=5, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (Linear2): Linear(in_features=5, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 93.4%, Avg loss: 0.298307 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 6.6%, Avg loss: 0.778059 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 6.6%, Avg loss: 0.778044 \n",
      "\n",
      "Done:  254.56435704231262  seconds\n",
      "Test Error: \n",
      " Accuracy: 6.6%, Avg loss: 0.778044 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 70\n",
    "n_outputs = 2\n",
    "n_hiddens = [5, 5]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs).to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 101\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "00da12db-6e44-4267-945e-0653fdcde92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 6.5%, Avg loss: 0.773588 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd86fc2-0056-4a8b-b327-870ec76ea810",
   "metadata": {},
   "source": [
    "This attempt went poorly. I quickly found that my model did not produce very good results. Imagine this was likely because the data in each particular sample was so similar. I decided to abandon this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18c9999-11b5-47fb-8528-d05abe6fd9cd",
   "metadata": {},
   "source": [
    "## Lumping the Data Into Chunks of Time ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0f3ca7-523a-4fc9-83df-23abb52e587e",
   "metadata": {},
   "source": [
    "\n",
    "I decided I would try lumping the data into chunks of time. I thought it might be useful to lump the data in this manner so that the machine learning model could get input that was collected over a greater period of time and would hopefully be able to use this to make better predictions. In order to make this work I first needed to appropriately lump the data. I would need to ensure that each chunk only contained data for eyes-open or eyes-closed states. I would throw out ambiguous chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3b0c1228-67ce-4eda-a586-7d9fb6b7c870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14304, 15)\n",
      "['AF3_0', 'F7_0', 'F3_0', 'FC5_0', 'T7_0', 'P7_0', 'O1_0', 'O2_0', 'P8_0', 'T8_0', 'FC6_0', 'F4_0', 'F8_0', 'AF4_0', 'AF3_1', 'F7_1', 'F3_1', 'FC5_1', 'T7_1', 'P7_1', 'O1_1', 'O2_1', 'P8_1', 'T8_1', 'FC6_1', 'F4_1', 'F8_1', 'AF4_1', 'AF3_2', 'F7_2', 'F3_2', 'FC5_2', 'T7_2', 'P7_2', 'O1_2', 'O2_2', 'P8_2', 'T8_2', 'FC6_2', 'F4_2', 'F8_2', 'AF4_2', 'AF3_3', 'F7_3', 'F3_3', 'FC5_3', 'T7_3', 'P7_3', 'O1_3', 'O2_3', 'P8_3', 'T8_3', 'FC6_3', 'F4_3', 'F8_3', 'AF4_3', 'AF3_4', 'F7_4', 'F3_4', 'FC5_4', 'T7_4', 'P7_4', 'O1_4', 'O2_4', 'P8_4', 'T8_4', 'FC6_4', 'F4_4', 'F8_4', 'AF4_4', 'AF3_5', 'F7_5', 'F3_5', 'FC5_5', 'T7_5', 'P7_5', 'O1_5', 'O2_5', 'P8_5', 'T8_5', 'FC6_5', 'F4_5', 'F8_5', 'AF4_5', 'AF3_6', 'F7_6', 'F3_6', 'FC5_6', 'T7_6', 'P7_6', 'O1_6', 'O2_6', 'P8_6', 'T8_6', 'FC6_6', 'F4_6', 'F8_6', 'AF4_6', 'AF3_7', 'F7_7', 'F3_7', 'FC5_7', 'T7_7', 'P7_7', 'O1_7', 'O2_7', 'P8_7', 'T8_7', 'FC6_7', 'F4_7', 'F8_7', 'AF4_7', 'AF3_8', 'F7_8', 'F3_8', 'FC5_8', 'T7_8', 'P7_8', 'O1_8', 'O2_8', 'P8_8', 'T8_8', 'FC6_8', 'F4_8', 'F8_8', 'AF4_8', 'AF3_9', 'F7_9', 'F3_9', 'FC5_9', 'T7_9', 'P7_9', 'O1_9', 'O2_9', 'P8_9', 'T8_9', 'FC6_9', 'F4_9', 'F8_9', 'AF4_9', 'eyeDetection']\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "1411\n",
      "776\n",
      "635\n",
      "Deleted: 19\n"
     ]
    }
   ],
   "source": [
    "# root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# fp_csv_file = \"EEG_Eye_State_No_Outliers.csv\"\n",
    "# data = pd.read_csv(\n",
    "#     os.path.join(root_directory, fp_csv_file), delimiter=\",\", usecols=range(15)\n",
    "# ).dropna(axis=0)\n",
    "\n",
    "# num_samples_in_chunk = 10\n",
    "# col_labels = []\n",
    "# print(data.shape)\n",
    "# for i in range(num_samples_in_chunk):\n",
    "\n",
    "#     for label in data.columns:\n",
    "#         if label == \"eyeDetection\":\n",
    "#             continue\n",
    "#         col_labels.append(label + \"_\" + str(i))\n",
    "\n",
    "# col_labels.append(\"eyeDetection\")\n",
    "# print(col_labels)\n",
    "\n",
    "# new_data = []\n",
    "\n",
    "# temp_list = []\n",
    "# count = 0\n",
    "# has_zero = False\n",
    "# has_one = False\n",
    "# zeros = 0\n",
    "# ones = 0\n",
    "# deleted = 0\n",
    "# for index, row in data.iterrows():\n",
    "\n",
    "#     if count == num_samples_in_chunk:\n",
    "#         if has_zero and has_one:\n",
    "#             count = 0\n",
    "#             temp_list = []\n",
    "#             print(\"Deleted Ambiguous Chunk\")\n",
    "#             deleted += 1\n",
    "#         elif has_zero:\n",
    "#             temp_list.append(0)\n",
    "#             new_data.append(temp_list)\n",
    "#             count = 0\n",
    "#             temp_list = []\n",
    "#             zeros += 1\n",
    "#         else:\n",
    "#             temp_list.append(1)\n",
    "#             new_data.append(temp_list)\n",
    "#             count = 0\n",
    "#             temp_list = []\n",
    "#             ones += 1\n",
    "#         has_zero = False\n",
    "#         has_one = False\n",
    "\n",
    "#     for i in range(len(row)):\n",
    "\n",
    "#         if i < len(row) - 1:\n",
    "#             temp_list.append(row[i])\n",
    "#         else:\n",
    "#             if row[i] == 0:\n",
    "#                 has_zero = True\n",
    "#             else:\n",
    "#                 has_one = True\n",
    "#     count += 1\n",
    "# print(len(new_data))\n",
    "# print(zeros)\n",
    "# print(ones)\n",
    "# print(\"Deleted: \" + str(deleted))\n",
    "\n",
    "# new_data_df = pd.DataFrame(new_data, columns=col_labels)\n",
    "# new_data_df = new_data_df.round(2)\n",
    "# new_data_df = new_data_df.astype({\"eyeDetection\": int})\n",
    "# new_data_df.to_csv(\n",
    "#     \"EEG_Eye_State_Lumped_\" + str(num_samples_in_chunk) + \".csv\",\n",
    "#     encoding=\"utf-8\",\n",
    "#     index_label=False,\n",
    "#     index=False,\n",
    "#     quoting=csv.QUOTE_NONNUMERIC,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "637b5882-6deb-4725-9582-1ef8fd53e9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AF3_0           4329.23\n",
      "F7_0            4009.23\n",
      "F3_0            4289.23\n",
      "FC5_0           4148.21\n",
      "T7_0            4350.26\n",
      "                 ...   \n",
      "FC6_9           4212.82\n",
      "F4_9            4277.95\n",
      "F8_9            4637.44\n",
      "AF4_9           4393.33\n",
      "eyeDetection       0.00\n",
      "Name: 0, Length: 141, dtype: float64\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# fp_csv_file = \"EEG_Eye_State_Lumped_\" + str(num_samples_in_chunk) + \".csv\"\n",
    "# data = pd.read_csv(\n",
    "#     os.path.join(root_directory, fp_csv_file),\n",
    "#     delimiter=\",\",\n",
    "#     usecols=range(new_data_df.shape[1]),\n",
    "# ).dropna(axis=0)\n",
    "\n",
    "# train_fraction = 0.8\n",
    "# validate_fraction = 0.1\n",
    "# X = (\n",
    "#     pd.read_csv(\n",
    "#         os.path.join(root_directory, fp_csv_file),\n",
    "#         delimiter=\",\",\n",
    "#         usecols=range(new_data_df.shape[1]),\n",
    "#     )\n",
    "#     .dropna(axis=0)\n",
    "#     .to_numpy()\n",
    "# )\n",
    "# n_samples = X.shape[0]\n",
    "\n",
    "# n_train = round(n_samples * train_fraction)\n",
    "# n_validate = round(n_samples * validate_fraction)\n",
    "\n",
    "# Xtrain = X[:n_train]\n",
    "# Xvalidate = X[n_train : n_train + n_validate]\n",
    "# Xtest = X[n_train + n_validate :]\n",
    "# Xtrain = pd.DataFrame(Xtrain, columns=col_labels)\n",
    "# Xvalidate = pd.DataFrame(Xvalidate, columns=col_labels)\n",
    "# Xtest = pd.DataFrame(Xtest, columns=col_labels)\n",
    "# Xtrain = Xtrain.astype({\"eyeDetection\": int})\n",
    "# Xvalidate = Xvalidate.astype({\"eyeDetection\": int})\n",
    "# Xtest = Xtest.astype({\"eyeDetection\": int})\n",
    "# print(Xtrain.iloc[0])\n",
    "# print(type(X))\n",
    "# Xtrain.to_csv(\n",
    "#     \"EEG_Eye_State_Lumped_\" + str(num_samples_in_chunk) + \"_Train.csv\",\n",
    "#     encoding=\"utf-8\",\n",
    "#     index_label=False,\n",
    "#     index=False,\n",
    "#     quoting=csv.QUOTE_NONNUMERIC,\n",
    "# )\n",
    "# Xvalidate.to_csv(\n",
    "#     \"EEG_Eye_State_Lumped_\" + str(num_samples_in_chunk) + \"_Validate.csv\",\n",
    "#     encoding=\"utf-8\",\n",
    "#     index_label=False,\n",
    "#     index=False,\n",
    "#     quoting=csv.QUOTE_NONNUMERIC,\n",
    "# )\n",
    "# Xtest.to_csv(\n",
    "#     \"EEG_Eye_State_Lumped_\" + str(num_samples_in_chunk) + \"_Test.csv\",\n",
    "#     encoding=\"utf-8\",\n",
    "#     index_label=False,\n",
    "#     index=False,\n",
    "#     quoting=csv.QUOTE_NONNUMERIC,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d32807f-16e9-46d5-ab07-711bb7580253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AF3_0           float64\n",
      "F7_0            float64\n",
      "F3_0            float64\n",
      "FC5_0           float64\n",
      "T7_0            float64\n",
      "                 ...   \n",
      "FC6_9           float64\n",
      "F4_9            float64\n",
      "F8_9            float64\n",
      "AF4_9           float64\n",
      "eyeDetection      int64\n",
      "Length: 141, dtype: object\n",
      "AF3_0           float64\n",
      "F7_0            float64\n",
      "F3_0            float64\n",
      "FC5_0           float64\n",
      "T7_0            float64\n",
      "                 ...   \n",
      "FC6_9           float64\n",
      "F4_9            float64\n",
      "F8_9            float64\n",
      "AF4_9           float64\n",
      "eyeDetection      int64\n",
      "Length: 141, dtype: object\n",
      "AF3_0           float64\n",
      "F7_0            float64\n",
      "F3_0            float64\n",
      "FC5_0           float64\n",
      "T7_0            float64\n",
      "                 ...   \n",
      "FC6_9           float64\n",
      "F4_9            float64\n",
      "F8_9            float64\n",
      "AF4_9           float64\n",
      "eyeDetection      int64\n",
      "Length: 141, dtype: object\n"
     ]
    }
   ],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "\n",
    "train_file = \"EEG_Eye_State_Lumped_10_Train.csv\"\n",
    "validate_file = \"EEG_Eye_State_Lumped_10_Validate.csv\"\n",
    "test_file = \"EEG_Eye_State_Lumped_10_Test.csv\"\n",
    "\n",
    "# train_file = \"Dummy_Train.csv\"\n",
    "# validate_file = \"Dummy_Validate.csv\"\n",
    "# test_file = \"Dummy_Test.csv\"\n",
    "\n",
    "train_dataset = fpnn.FinalProjectEEGDataset(train_file, root_directory)\n",
    "validate_dataset = fpnn.FinalProjectEEGDataset(validate_file, root_directory)\n",
    "test_dataset = fpnn.FinalProjectEEGDataset(test_file, root_directory)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "464f658c-8357-419b-988d-cb206588d5b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=140, out_features=5, bias=True)\n",
      "    (Sigmoid0): Sigmoid()\n",
      "    (Linear1): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (Sigmoid1): Sigmoid()\n",
      "    (Linear2): Linear(in_features=5, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.630115 \n",
      "\n",
      "Done:  2.7117764949798584  seconds\n",
      "Test Error: \n",
      " Accuracy: 7.1%, Avg loss: 0.730498 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 140\n",
    "n_outputs = 2\n",
    "n_hiddens = [5, 5]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"sigmoid\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b1bec51-05d0-4b5e-ad4c-d44b7b4204f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=140, out_features=20, bias=True)\n",
      "    (Sigmoid0): Sigmoid()\n",
      "    (Linear1): Linear(in_features=20, out_features=15, bias=True)\n",
      "    (Sigmoid1): Sigmoid()\n",
      "    (Linear2): Linear(in_features=15, out_features=10, bias=True)\n",
      "    (Sigmoid2): Sigmoid()\n",
      "    (Linear3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 7.1%, Avg loss: 0.888833 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 7.1%, Avg loss: 0.805603 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 7.1%, Avg loss: 0.805783 \n",
      "\n",
      "Done:  27.96122980117798  seconds\n",
      "Test Error: \n",
      " Accuracy: 7.1%, Avg loss: 0.805783 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 140\n",
    "n_outputs = 2\n",
    "n_hiddens = [20, 15, 10]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"sigmoid\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.05\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 101\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511c92b8-1c64-4720-baf6-41b462946f42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp",
   "language": "python",
   "name": "fp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
