{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334a04dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c49f5edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import finalprojectneuralnetworks as fpnn\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "087f60f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "\n",
    "# with open(\"Dummy_Validate.csv\", \"w\", newline=\"\") as csvfile:\n",
    "#     spamwriter = csv.writer(\n",
    "#         csvfile, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL\n",
    "#     )\n",
    "#     col_labels = [\n",
    "#         \"AF3\",\n",
    "#         \"F7\",\n",
    "#         \"F3\",\n",
    "#         \"FC5\",\n",
    "#         \"T7\",\n",
    "#         \"P7\",\n",
    "#         \"O1\",\n",
    "#         \"O2\",\n",
    "#         \"P8\",\n",
    "#         \"T8\",\n",
    "#         \"FC6\",\n",
    "#         \"F4\",\n",
    "#         \"F8\",\n",
    "#         \"AF4\",\n",
    "#         \"eyeDetection\",\n",
    "#     ]\n",
    "#     spamwriter.writerow(col_labels)\n",
    "#     list_a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 0]\n",
    "#     list_b = [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 1]\n",
    "#     for i in range(500):\n",
    "#         random_number = random.randint(1, 10)\n",
    "#         if random_number > 5:\n",
    "#             spamwriter.writerow(list_a)\n",
    "#         else:\n",
    "#             spamwriter.writerow(list_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e029db25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# root_directory = 'D:\\College\\CS_545\\Final_Project'\n",
    "# fp_csv_file = 'EEG_Eye_State.csv'\n",
    "# train_fraction = 0.8\n",
    "# validate_fraction = 0.1\n",
    "# X = pd.read_csv(os.path.join(root_directory,fp_csv_file), delimiter=',', usecols=range(15)).dropna(axis=0).to_numpy()\n",
    "# n_samples = X.shape[0]\n",
    "# rows = np.arange(n_samples)\n",
    "# np.random.shuffle(rows)\n",
    "\n",
    "# n_train = round(n_samples * train_fraction)\n",
    "# n_validate = round(n_samples * validate_fraction)\n",
    "\n",
    "# col_labels = [\"AF3\",\"F7\",\"F3\",\"FC5\",\"T7\",\"P7\",\"O1\",\"O2\",\"P8\",\"T8\",\"FC6\",\"F4\",\"F8\",\"AF4\",\"eyeDetection\"]\n",
    "# Xtrain = X[rows[:n_train], :]\n",
    "# Xvalidate = X[rows[n_train:n_train + n_validate], :]\n",
    "# Xtest = X[rows[n_train + n_validate:], :]\n",
    "# Xtrain = pd.DataFrame(Xtrain, columns = col_labels)\n",
    "# Xvalidate = pd.DataFrame(Xvalidate, columns = col_labels)\n",
    "# Xtest = pd.DataFrame(Xtest, columns = col_labels)\n",
    "# Xtrain = Xtrain.astype({\"eyeDetection\": int})\n",
    "# Xvalidate = Xvalidate.astype({\"eyeDetection\": int})\n",
    "# Xtest = Xtest.astype({\"eyeDetection\": int})\n",
    "# print(Xtrain.iloc[0])\n",
    "# print( type(X))\n",
    "# Xtrain.to_csv(\"EEG_Eye_State_Train.csv\", encoding='utf-8', index_label=False, index=False, quoting=csv.QUOTE_NONNUMERIC )\n",
    "# Xvalidate.to_csv(\"EEG_Eye_State_Validate.csv\", encoding='utf-8', index_label=False, index=False, quoting=csv.QUOTE_NONNUMERIC )\n",
    "# Xtest.to_csv(\"EEG_Eye_State_Test.csv\", encoding='utf-8', index_label=False, index=False, quoting=csv.QUOTE_NONNUMERIC )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb987a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AF3             4309.74\n",
      "F7              4052.82\n",
      "F3              4285.13\n",
      "FC5             4155.38\n",
      "T7              4369.23\n",
      "P7              4646.67\n",
      "O1              4100.00\n",
      "O2              4649.74\n",
      "P8              4221.54\n",
      "T8              4230.26\n",
      "FC6             4206.15\n",
      "F4              4292.82\n",
      "F8              4598.97\n",
      "AF4             4362.56\n",
      "eyeDetection       0.00\n",
      "Name: 0, dtype: float64\n",
      "0\n",
      "[[4309.74 4052.82 4285.13 4155.38 4369.23 4646.67 4100.   4649.74 4221.54\n",
      "  4230.26 4206.15 4292.82 4598.97 4362.56]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11984, 15)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "fp_csv_file = \"EEG_Eye_State_Train.csv\"\n",
    "# fp_csv_file = \"Dummy_Train.csv\"\n",
    "eeg_data = pd.read_csv(\n",
    "    os.path.join(root_directory, fp_csv_file), delimiter=\",\", usecols=range(15)\n",
    ").dropna(axis=0)\n",
    "\n",
    "print(eeg_data.iloc[0, :])\n",
    "open_or_closed = eeg_data.iloc[0, 14]\n",
    "channel_data = eeg_data.iloc[0, 0:14]\n",
    "channel_data = np.array([channel_data])\n",
    "channel_data = channel_data.astype(\"float\").reshape(-1, 14)\n",
    "print(open_or_closed)\n",
    "print(channel_data)\n",
    "eeg_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d7acdc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AF3             float64\n",
      "F7              float64\n",
      "F3              float64\n",
      "FC5             float64\n",
      "T7              float64\n",
      "P7              float64\n",
      "O1              float64\n",
      "O2              float64\n",
      "P8              float64\n",
      "T8              float64\n",
      "FC6             float64\n",
      "F4              float64\n",
      "F8              float64\n",
      "AF4             float64\n",
      "eyeDetection      int64\n",
      "dtype: object\n",
      "AF3             float64\n",
      "F7              float64\n",
      "F3              float64\n",
      "FC5             float64\n",
      "T7              float64\n",
      "P7              float64\n",
      "O1              float64\n",
      "O2              float64\n",
      "P8              float64\n",
      "T8              float64\n",
      "FC6             float64\n",
      "F4              float64\n",
      "F8              float64\n",
      "AF4             float64\n",
      "eyeDetection      int64\n",
      "dtype: object\n",
      "AF3             float64\n",
      "F7              float64\n",
      "F3              float64\n",
      "FC5             float64\n",
      "T7              float64\n",
      "P7              float64\n",
      "O1              float64\n",
      "O2              float64\n",
      "P8              float64\n",
      "T8              float64\n",
      "FC6             float64\n",
      "F4              float64\n",
      "F8              float64\n",
      "AF4             float64\n",
      "eyeDetection      int64\n",
      "dtype: object\n",
      "11984\n"
     ]
    }
   ],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "\n",
    "train_file = \"EEG_Eye_State_Train.csv\"\n",
    "validate_file = \"EEG_Eye_State_Validate.csv\"\n",
    "test_file = \"EEG_Eye_State_Test.csv\"\n",
    "\n",
    "# train_file = \"Dummy_Train.csv\"\n",
    "# validate_file = \"Dummy_Validate.csv\"\n",
    "# test_file = \"Dummy_Test.csv\"\n",
    "\n",
    "train_dataset = fpnn.FinalProjectEEGDataset(train_file, root_directory)\n",
    "validate_dataset = fpnn.FinalProjectEEGDataset(validate_file, root_directory)\n",
    "test_dataset = fpnn.FinalProjectEEGDataset(test_file, root_directory)\n",
    "print(train_dataset.__len__())\n",
    "train_dataset.__getitem__(0)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a360238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c653ec23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (Linear2): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = fpnn.NeuralNetwork(28 * 28, [512, 512], 10).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aceb2dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0452, -0.0094, -0.0277,  0.0657,  0.0079,  0.0026,  0.0191,  0.0068,\n",
      "          0.0008, -0.0493]], grad_fn=<AddmmBackward>)\n",
      "Predicted class: tensor([3])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "print(logits)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55e5e4ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    # first = True\n",
    "    for batch, (X, T) in enumerate(dataloader):\n",
    "        X, T = X.to(device), T.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        Y = model(X.float())\n",
    "        # if first:\n",
    "        #     first = False\n",
    "        #     print('Y.shape', Y.shape)\n",
    "        #     print('T.shape', T.shape)\n",
    "        #     print('T: ', T[0])\n",
    "        loss = loss_fn(Y, T.long())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "#         if batch % 50 == 0:\n",
    "#             loss, current = loss.item(), batch * len(X)\n",
    "# print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17432e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, T in dataloader:\n",
    "            X, T = X.to(device), T.to(device)\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, T.long()).item()\n",
    "            correct += (pred.argmax(1) == T).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(\n",
    "        f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32ade0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_epochs(\n",
    "    epochs, train_dataloader, _validate_dataloader, model, loss_fn, optimizer\n",
    "):\n",
    "    start = time.time()\n",
    "    for t in range(epochs):\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        if t % 50 == 0:\n",
    "            print(f\"Epoch {t}\\n-------------------------------\")\n",
    "            test(_validate_dataloader, model, loss_fn)\n",
    "    end = time.time()\n",
    "    print(\"Done: \", end - start, \" seconds\")\n",
    "    test(_validate_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "683a5455-6ed3-4285-82d7-812ceba68c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ones_and_zeros(file):\n",
    "    data = pd.read_csv(\n",
    "        os.path.join(file),\n",
    "        delimiter=\",\",\n",
    "    ).dropna(axis=0)\n",
    "    zeros = 0\n",
    "    ones = 1\n",
    "    for index, row in data.iterrows():\n",
    "        if row[len(data.columns) - 1] == 0:\n",
    "            zeros += 1\n",
    "        else:\n",
    "            ones += 1\n",
    "    print(\"Zeros: \", zeros)\n",
    "    print(\"Ones: \", ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41ba2342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fashion_training_data = datasets.FashionMNIST(\n",
    "#     root=\"data\",\n",
    "#     train=True,\n",
    "#     download=True,\n",
    "#     transform=ToTensor(),\n",
    "# )\n",
    "# fashion_train_dataloader = DataLoader(fashion_training_data, batch_size=64)\n",
    "# n_samples, n_inputs = train_dataset.__len__(), 28*28\n",
    "# n_outputs = 1\n",
    "# n_hiddens = [10, 10]\n",
    "# model = fpnn.NeuralNetwork(28*28, n_hiddens, 10).to(device)\n",
    "# print(model)\n",
    "\n",
    "# learning_rate = 0.5 / (n_samples * n_outputs)  ## Larger learning rate\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# train(fashion_train_dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3190dd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([64, 1, 14])\n",
      "Shape of y:  torch.Size([64]) torch.float64\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1082909f-a936-4f85-9713-3da18371ea33",
   "metadata": {},
   "source": [
    "I started out with a dummy run using dummy data that had a trivial pattern which the neural network should easily be able to learn. I did this to simply test that the network was correctly setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "319f87a6-a129-4ecf-b550-4e3a60b4cfb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AF3             int64\n",
      "F7              int64\n",
      "F3              int64\n",
      "FC5             int64\n",
      "T7              int64\n",
      "P7              int64\n",
      "O1              int64\n",
      "O2              int64\n",
      "P8              int64\n",
      "T8              int64\n",
      "FC6             int64\n",
      "F4              int64\n",
      "F8              int64\n",
      "AF4             int64\n",
      "eyeDetection    int64\n",
      "dtype: object\n",
      "AF3             int64\n",
      "F7              int64\n",
      "F3              int64\n",
      "FC5             int64\n",
      "T7              int64\n",
      "P7              int64\n",
      "O1              int64\n",
      "O2              int64\n",
      "P8              int64\n",
      "T8              int64\n",
      "FC6             int64\n",
      "F4              int64\n",
      "F8              int64\n",
      "AF4             int64\n",
      "eyeDetection    int64\n",
      "dtype: object\n",
      "AF3             int64\n",
      "F7              int64\n",
      "F3              int64\n",
      "FC5             int64\n",
      "T7              int64\n",
      "P7              int64\n",
      "O1              int64\n",
      "O2              int64\n",
      "P8              int64\n",
      "T8              int64\n",
      "FC6             int64\n",
      "F4              int64\n",
      "F8              int64\n",
      "AF4             int64\n",
      "eyeDetection    int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "\n",
    "# train_file = \"EEG_Eye_State_Train.csv\"\n",
    "# validate_file = \"EEG_Eye_State_Validate.csv\"\n",
    "# test_file = \"EEG_Eye_State_Test.csv\"\n",
    "\n",
    "train_file = \"Dummy_Train.csv\"\n",
    "validate_file = \"Dummy_Validate.csv\"\n",
    "test_file = \"Dummy_Test.csv\"\n",
    "\n",
    "train_dataset = fpnn.FinalProjectEEGDataset(train_file, root_directory)\n",
    "validate_dataset = fpnn.FinalProjectEEGDataset(validate_file, root_directory)\n",
    "test_dataset = fpnn.FinalProjectEEGDataset(test_file, root_directory)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9189f641-75c4-4724-a1ff-02c0a6df3a0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=14, out_features=5, bias=True)\n",
      "    (Sigmoid0): Sigmoid()\n",
      "    (Linear1): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (Sigmoid1): Sigmoid()\n",
      "    (Linear2): Linear(in_features=5, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 0.695648 \n",
      "\n",
      "Done:  1.8184118270874023  seconds\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.595044 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 14\n",
    "n_outputs = 2\n",
    "n_hiddens = [5, 5]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"sigmoid\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "224f1662-7926-4429-a065-3f4fa8ac3509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.534908 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(validate_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28130031-3767-4640-8111-bb30cf31dd73",
   "metadata": {},
   "source": [
    "Unsurprisingly in this dummy run the model learned to predict with 100% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8788dca1-9457-40d3-9e4b-8b2ff7667588",
   "metadata": {},
   "source": [
    "I then moved on to the actual data to see what kind of accuracy I could get from a classical classification neural network model. I tried several structures and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "186cd8ca-6ec0-4ab1-8758-9aae7df8b876",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AF3             float64\n",
      "F7              float64\n",
      "F3              float64\n",
      "FC5             float64\n",
      "T7              float64\n",
      "P7              float64\n",
      "O1              float64\n",
      "O2              float64\n",
      "P8              float64\n",
      "T8              float64\n",
      "FC6             float64\n",
      "F4              float64\n",
      "F8              float64\n",
      "AF4             float64\n",
      "eyeDetection      int64\n",
      "dtype: object\n",
      "AF3             float64\n",
      "F7              float64\n",
      "F3              float64\n",
      "FC5             float64\n",
      "T7              float64\n",
      "P7              float64\n",
      "O1              float64\n",
      "O2              float64\n",
      "P8              float64\n",
      "T8              float64\n",
      "FC6             float64\n",
      "F4              float64\n",
      "F8              float64\n",
      "AF4             float64\n",
      "eyeDetection      int64\n",
      "dtype: object\n",
      "AF3             float64\n",
      "F7              float64\n",
      "F3              float64\n",
      "FC5             float64\n",
      "T7              float64\n",
      "P7              float64\n",
      "O1              float64\n",
      "O2              float64\n",
      "P8              float64\n",
      "T8              float64\n",
      "FC6             float64\n",
      "F4              float64\n",
      "F8              float64\n",
      "AF4             float64\n",
      "eyeDetection      int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "\n",
    "train_file = \"EEG_Eye_State_Train.csv\"\n",
    "validate_file = \"EEG_Eye_State_Validate.csv\"\n",
    "test_file = \"EEG_Eye_State_Test.csv\"\n",
    "\n",
    "# train_file = \"Dummy_Train.csv\"\n",
    "# validate_file = \"Dummy_Validate.csv\"\n",
    "# test_file = \"Dummy_Test.csv\"\n",
    "\n",
    "train_dataset = fpnn.FinalProjectEEGDataset(train_file, root_directory)\n",
    "validate_dataset = fpnn.FinalProjectEEGDataset(validate_file, root_directory)\n",
    "test_dataset = fpnn.FinalProjectEEGDataset(test_file, root_directory)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "796d2033-3bac-4f43-be51-a1e49e188983",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=14, out_features=5, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (Linear2): Linear(in_features=5, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.687582 \n",
      "\n",
      "Done:  24.560352325439453  seconds\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.685850 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 14\n",
    "n_outputs = 2\n",
    "n_hiddens = [5, 5]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs).to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "211af310-64c5-4c9f-964d-9ec13b2c1bbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i == 0\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=14, out_features=10, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (Linear2): Linear(in_features=10, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 46.4%, Avg loss: 10.459651 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 3.271880 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 1.862043 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.685989 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.685970 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.685949 \n",
      "\n",
      "Done:  1008.1690013408661  seconds\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 14\n",
    "n_outputs = 2\n",
    "n_hiddens = [10, 10]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs).to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 501\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80d29813-9c38-4d11-9fff-51591a825449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i == 0\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=14, out_features=30, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (Linear2): Linear(in_features=20, out_features=15, bias=True)\n",
      "    (ReLU2): ReLU()\n",
      "    (Linear3): Linear(in_features=15, out_features=10, bias=True)\n",
      "    (ReLU3): ReLU()\n",
      "    (Linear4): Linear(in_features=10, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 1.173773 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.685931 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.685854 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.685854 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.685854 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.685854 \n",
      "\n",
      "Done:  1033.4101610183716  seconds\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 14\n",
    "n_outputs = 2\n",
    "n_hiddens = [30, 20, 15, 10]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs).to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 501\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0647adb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i == 0\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=14, out_features=500, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (Linear2): Linear(in_features=500, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 6.962534 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 1.713657 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 1.713657 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 1.713657 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 1.713657 \n",
      "\n",
      "Done:  1214.9345533847809  seconds\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 14\n",
    "n_outputs = 2\n",
    "n_hiddens = [500, 500]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs).to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 500\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59d3a9c2-790c-4189-afa0-71999820f1b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i == 0\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=14, out_features=500, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (Linear2): Linear(in_features=500, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 288.282979 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 54.4%, Avg loss: 0.785713 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 0.756815 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.1%, Avg loss: 0.722233 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.7%, Avg loss: 0.705566 \n",
      "\n",
      "Done:  1155.8505518436432  seconds\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 14\n",
    "n_outputs = 2\n",
    "n_hiddens = [500, 500]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs).to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.01 / (n_samples * n_outputs)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 500\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9571821-5d3d-4dd7-8c10-0ac977e515d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i == 0\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=14, out_features=500, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (Linear2): Linear(in_features=500, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss:      nan \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss:      nan \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss:      nan \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss:      nan \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss:      nan \n",
      "\n",
      "Done:  1099.8602554798126  seconds\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 14\n",
    "n_outputs = 2\n",
    "n_hiddens = [500, 500]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs).to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.01 / (n_samples * n_outputs)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 500\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bd522c6-9a7e-4118-a2fd-5dee071dce3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i == 0\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=14, out_features=500, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (Linear2): Linear(in_features=500, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 77656832.685102 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 77656832.683104 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 77656832.681246 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 77656832.679520 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 77656832.677915 \n",
      "\n",
      "Done:  1064.0559866428375  seconds\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 14\n",
    "n_outputs = 2\n",
    "n_hiddens = [500, 500]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs).to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.01 / (n_samples * n_outputs)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 500\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14307458-ea6d-45d2-8d60-d871ad8aefbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=14, out_features=500, bias=True)\n",
      "    (Tanh0): Tanh()\n",
      "    (Linear1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (Tanh1): Tanh()\n",
      "    (Linear2): Linear(in_features=500, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 0.755901 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.686656 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 0.686432 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 0.686410 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Avg loss: 0.686408 \n",
      "\n",
      "Done:  1249.532361984253  seconds\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 14\n",
    "n_outputs = 2\n",
    "n_hiddens = [500, 500]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"tanh\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.01 / (n_samples * n_outputs)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 500\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec6a867d-a40b-4f0e-bf62-3768ef8fd61a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=14, out_features=500, bias=True)\n",
      "    (Sigmoid0): Sigmoid()\n",
      "    (Linear1): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (Sigmoid1): Sigmoid()\n",
      "    (Linear2): Linear(in_features=500, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 0.670922 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.006674 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.002450 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.001424 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000983 \n",
      "\n",
      "Done:  94.19774389266968  seconds\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 14\n",
    "n_outputs = 2\n",
    "n_hiddens = [500, 500]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"sigmoid\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.01 / (n_samples * n_outputs)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 500\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b9ab2-f244-4dfd-8b51-c45a8c7e334f",
   "metadata": {},
   "source": [
    "Unfortunately this approach didn't seem to produce very good results. The models were barely better than chance, with he best of them achieving a final accuracy of 56%. They also didn't seem to learn much after the initial few epochs. I decided to try a different approach going forward. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020bb429-3120-42a6-827a-04690303d340",
   "metadata": {},
   "source": [
    "## Including Mean and Standard Deviation Approach Experimentation ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd50c9e6-b442-4021-9db7-15c993e2ba91",
   "metadata": {},
   "source": [
    "I started by removing any glaring outliers from the data. I thought this might make it a little easier for the neural network to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45c6c874-4cc1-459b-ace9-28575ee0e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# fp_csv_file = \"EEG_Eye_State.csv\"\n",
    "# data = pd.read_csv(\n",
    "#     os.path.join(root_directory, fp_csv_file), delimiter=\",\", usecols=range(15)\n",
    "# ).dropna(axis=0)\n",
    "\n",
    "# # data = pd.read_csv(os.path.join(root_directory, fp_csv_file), header=None)\n",
    "# # retrieve data as numpy array\n",
    "# values = data.values\n",
    "\n",
    "\n",
    "# print(values.shape)\n",
    "# print(values[0:5])\n",
    "# # step over each EEG column\n",
    "# for i in range(values.shape[1] - 1):\n",
    "#     # calculate column mean and standard deviation\n",
    "#     data_mean, data_std = np.mean(values[:, i]), np.std(values[:, i])\n",
    "#     # define outlier bounds\n",
    "#     cut_off = data_std * 4\n",
    "#     lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "#     # remove too small\n",
    "#     too_small = [j for j in range(values.shape[0]) if values[j, i] < lower]\n",
    "#     values = np.delete(values, too_small, 0)\n",
    "#     print(\">deleted %d rows\" % len(too_small))\n",
    "#     # remove too large\n",
    "#     too_large = [j for j in range(values.shape[0]) if values[j, i] > upper]\n",
    "#     values = np.delete(values, too_large, 0)\n",
    "#     print(\">deleted %d rows\" % len(too_large))\n",
    "# # save the results to a new file\n",
    "# col_labels = [\n",
    "#     \"AF3\",\n",
    "#     \"F7\",\n",
    "#     \"F3\",\n",
    "#     \"FC5\",\n",
    "#     \"T7\",\n",
    "#     \"P7\",\n",
    "#     \"O1\",\n",
    "#     \"O2\",\n",
    "#     \"P8\",\n",
    "#     \"T8\",\n",
    "#     \"FC6\",\n",
    "#     \"F4\",\n",
    "#     \"F8\",\n",
    "#     \"AF4\",\n",
    "#     \"eyeDetection\",\n",
    "# ]\n",
    "# values = pd.DataFrame(values, columns=data.columns)\n",
    "# values.to_csv(\n",
    "#     \"EEG_Eye_State_No_Outliers.csv\",\n",
    "#     encoding=\"utf-8\",\n",
    "#     index_label=False,\n",
    "#     index=False,\n",
    "#     quoting=csv.QUOTE_NONNUMERIC,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e61655-837e-4b78-9593-28d690219a08",
   "metadata": {},
   "source": [
    "After that I decided to try and investigate the data at a more meta level. I calculated the mean, standard deviation, maximum value, and minimum value for the eye-open state data and eye-closed state data. I also calculated the mean and standard deviation for the data as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73c5bad3-8878-4e7d-b1b2-196a32da76ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(file_directory):\n",
    "    data = pd.read_csv(file_directory, delimiter=\",\", usecols=range(15)).dropna(axis=0)\n",
    "\n",
    "    metadata_labels = [\n",
    "        \"Channel\",\n",
    "        \"Mean\",\n",
    "        \"Std\",\n",
    "        \"Max\",\n",
    "        \"Min\",\n",
    "        \"Eye-Open Mean\",\n",
    "        \"Eye-Open Std\",\n",
    "        \"Eye-Open Max\",\n",
    "        \"Eye-Open Min\",\n",
    "        \"Eye-Closed Mean\",\n",
    "        \"Eye-Closed Std\",\n",
    "        \"Eye-Closed Max\",\n",
    "        \"Eye-Closed Min\",\n",
    "    ]\n",
    "\n",
    "    metadata = [\n",
    "        [\"AF3\"],\n",
    "        [\"F7\"],\n",
    "        [\"F3\"],\n",
    "        [\"FC5\"],\n",
    "        [\"T7\"],\n",
    "        [\"P7\"],\n",
    "        [\"O1\"],\n",
    "        [\"O2\"],\n",
    "        [\"P8\"],\n",
    "        [\"T8\"],\n",
    "        [\"FC6\"],\n",
    "        [\"F4\"],\n",
    "        [\"F8\"],\n",
    "        [\"AF4\"],\n",
    "    ]\n",
    "    values = data.values\n",
    "    print(values.shape)\n",
    "    # step over each EEG column\n",
    "    for i in range(values.shape[1] - 1):\n",
    "        # calculate column mean and standard deviation\n",
    "        data_mean, data_std, data_max, data_min = (\n",
    "            np.mean(values[:, i]),\n",
    "            np.std(values[:, i]),\n",
    "            np.max(values[:, i]),\n",
    "            np.min(values[:, i]),\n",
    "        )\n",
    "        metadata[i].append(data_mean)\n",
    "        metadata[i].append(data_std)\n",
    "        metadata[i].append(data_max)\n",
    "        metadata[i].append(data_min)\n",
    "\n",
    "    zeros = []\n",
    "    ones = []\n",
    "    for sample in values:\n",
    "        if sample[14] == 0:\n",
    "            zeros.append(sample)\n",
    "        else:\n",
    "            ones.append(sample)\n",
    "    zeros = pd.DataFrame(zeros)\n",
    "    ones = pd.DataFrame(ones)\n",
    "\n",
    "    zeros.columns = data.columns\n",
    "    ones.columns = data.columns\n",
    "    print(zeros.shape)\n",
    "    zeros_values = zeros.values\n",
    "    for i in range(zeros_values.shape[1] - 1):\n",
    "        # calculate column mean and standard deviation\n",
    "        data_mean, data_std, data_max, data_min = (\n",
    "            np.mean(zeros_values[:, i]),\n",
    "            np.std(zeros_values[:, i]),\n",
    "            np.max(zeros_values[:, i]),\n",
    "            np.min(zeros_values[:, i]),\n",
    "        )\n",
    "        metadata[i].append(data_mean)\n",
    "        metadata[i].append(data_std)\n",
    "        metadata[i].append(data_max)\n",
    "        metadata[i].append(data_min)\n",
    "\n",
    "    print(ones.shape)\n",
    "    ones_values = ones.values\n",
    "    for i in range(ones_values.shape[1] - 1):\n",
    "        # calculate column mean and standard deviation\n",
    "        data_mean, data_std, data_max, data_min = (\n",
    "            np.mean(ones_values[:, i]),\n",
    "            np.std(ones_values[:, i]),\n",
    "            np.max(ones_values[:, i]),\n",
    "            np.min(ones_values[:, i]),\n",
    "        )\n",
    "        metadata[i].append(data_mean)\n",
    "        metadata[i].append(data_std)\n",
    "        metadata[i].append(data_max)\n",
    "        metadata[i].append(data_min)\n",
    "\n",
    "    metadata = pd.DataFrame(metadata)\n",
    "    metadata.columns = metadata_labels\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e012b50f-cc3b-4100-927b-dd55e0c004ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14304, 15)\n",
      "(7855, 15)\n",
      "(6449, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "      <th>Max</th>\n",
       "      <th>Min</th>\n",
       "      <th>Eye-Open Mean</th>\n",
       "      <th>Eye-Open Std</th>\n",
       "      <th>Eye-Open Max</th>\n",
       "      <th>Eye-Open Min</th>\n",
       "      <th>Eye-Closed Mean</th>\n",
       "      <th>Eye-Closed Std</th>\n",
       "      <th>Eye-Closed Max</th>\n",
       "      <th>Eye-Closed Min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AF3</td>\n",
       "      <td>4298.144530</td>\n",
       "      <td>32.383173</td>\n",
       "      <td>4466.15</td>\n",
       "      <td>4198.97</td>\n",
       "      <td>4294.614386</td>\n",
       "      <td>33.537700</td>\n",
       "      <td>4466.15</td>\n",
       "      <td>4206.67</td>\n",
       "      <td>4302.444309</td>\n",
       "      <td>30.369437</td>\n",
       "      <td>4441.03</td>\n",
       "      <td>4198.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F7</td>\n",
       "      <td>4007.199968</td>\n",
       "      <td>27.222342</td>\n",
       "      <td>4154.36</td>\n",
       "      <td>3913.33</td>\n",
       "      <td>4009.736631</td>\n",
       "      <td>28.256648</td>\n",
       "      <td>4154.36</td>\n",
       "      <td>3924.10</td>\n",
       "      <td>4004.110265</td>\n",
       "      <td>25.569113</td>\n",
       "      <td>4138.97</td>\n",
       "      <td>3913.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F3</td>\n",
       "      <td>4261.656384</td>\n",
       "      <td>16.219297</td>\n",
       "      <td>4349.23</td>\n",
       "      <td>4197.44</td>\n",
       "      <td>4260.402014</td>\n",
       "      <td>16.607684</td>\n",
       "      <td>4349.23</td>\n",
       "      <td>4197.44</td>\n",
       "      <td>4263.184229</td>\n",
       "      <td>15.597616</td>\n",
       "      <td>4333.85</td>\n",
       "      <td>4212.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FC5</td>\n",
       "      <td>4120.176654</td>\n",
       "      <td>16.904783</td>\n",
       "      <td>4191.79</td>\n",
       "      <td>4067.18</td>\n",
       "      <td>4120.948880</td>\n",
       "      <td>16.536333</td>\n",
       "      <td>4187.18</td>\n",
       "      <td>4073.33</td>\n",
       "      <td>4119.236069</td>\n",
       "      <td>17.296483</td>\n",
       "      <td>4191.79</td>\n",
       "      <td>4067.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T7</td>\n",
       "      <td>4339.673482</td>\n",
       "      <td>11.994233</td>\n",
       "      <td>4397.95</td>\n",
       "      <td>4308.72</td>\n",
       "      <td>4339.912341</td>\n",
       "      <td>11.199247</td>\n",
       "      <td>4395.90</td>\n",
       "      <td>4308.72</td>\n",
       "      <td>4339.382548</td>\n",
       "      <td>12.890536</td>\n",
       "      <td>4397.95</td>\n",
       "      <td>4309.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P7</td>\n",
       "      <td>4618.165981</td>\n",
       "      <td>12.923532</td>\n",
       "      <td>4672.31</td>\n",
       "      <td>4566.15</td>\n",
       "      <td>4619.437780</td>\n",
       "      <td>13.217759</td>\n",
       "      <td>4671.28</td>\n",
       "      <td>4566.15</td>\n",
       "      <td>4616.616906</td>\n",
       "      <td>12.380614</td>\n",
       "      <td>4672.31</td>\n",
       "      <td>4581.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>O1</td>\n",
       "      <td>4070.977384</td>\n",
       "      <td>17.707910</td>\n",
       "      <td>4138.97</td>\n",
       "      <td>4026.15</td>\n",
       "      <td>4070.531945</td>\n",
       "      <td>14.417633</td>\n",
       "      <td>4124.10</td>\n",
       "      <td>4027.18</td>\n",
       "      <td>4071.519936</td>\n",
       "      <td>21.018584</td>\n",
       "      <td>4138.97</td>\n",
       "      <td>4026.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>O2</td>\n",
       "      <td>4614.060973</td>\n",
       "      <td>14.398261</td>\n",
       "      <td>4672.82</td>\n",
       "      <td>4567.18</td>\n",
       "      <td>4613.460053</td>\n",
       "      <td>13.655753</td>\n",
       "      <td>4672.82</td>\n",
       "      <td>4567.18</td>\n",
       "      <td>4614.792904</td>\n",
       "      <td>15.221895</td>\n",
       "      <td>4672.82</td>\n",
       "      <td>4567.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P8</td>\n",
       "      <td>4199.641958</td>\n",
       "      <td>13.807744</td>\n",
       "      <td>4255.90</td>\n",
       "      <td>4147.69</td>\n",
       "      <td>4198.939306</td>\n",
       "      <td>12.424151</td>\n",
       "      <td>4255.90</td>\n",
       "      <td>4152.31</td>\n",
       "      <td>4200.497801</td>\n",
       "      <td>15.281592</td>\n",
       "      <td>4255.38</td>\n",
       "      <td>4147.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>T8</td>\n",
       "      <td>4229.291521</td>\n",
       "      <td>15.005626</td>\n",
       "      <td>4288.21</td>\n",
       "      <td>4170.26</td>\n",
       "      <td>4227.845602</td>\n",
       "      <td>14.217735</td>\n",
       "      <td>4287.69</td>\n",
       "      <td>4170.26</td>\n",
       "      <td>4231.052678</td>\n",
       "      <td>15.734208</td>\n",
       "      <td>4288.21</td>\n",
       "      <td>4174.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FC6</td>\n",
       "      <td>4199.729761</td>\n",
       "      <td>18.338192</td>\n",
       "      <td>4271.28</td>\n",
       "      <td>4125.13</td>\n",
       "      <td>4197.989321</td>\n",
       "      <td>17.985427</td>\n",
       "      <td>4264.10</td>\n",
       "      <td>4125.13</td>\n",
       "      <td>4201.849648</td>\n",
       "      <td>18.539501</td>\n",
       "      <td>4271.28</td>\n",
       "      <td>4130.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F4</td>\n",
       "      <td>4276.866449</td>\n",
       "      <td>15.089147</td>\n",
       "      <td>4332.31</td>\n",
       "      <td>4216.41</td>\n",
       "      <td>4274.783659</td>\n",
       "      <td>15.042807</td>\n",
       "      <td>4329.74</td>\n",
       "      <td>4216.41</td>\n",
       "      <td>4279.403325</td>\n",
       "      <td>14.753427</td>\n",
       "      <td>4332.31</td>\n",
       "      <td>4225.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>F8</td>\n",
       "      <td>4603.123432</td>\n",
       "      <td>25.535726</td>\n",
       "      <td>4716.41</td>\n",
       "      <td>4490.77</td>\n",
       "      <td>4599.781686</td>\n",
       "      <td>23.835106</td>\n",
       "      <td>4711.79</td>\n",
       "      <td>4490.77</td>\n",
       "      <td>4607.193740</td>\n",
       "      <td>26.910407</td>\n",
       "      <td>4716.41</td>\n",
       "      <td>4510.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AF4</td>\n",
       "      <td>4358.072562</td>\n",
       "      <td>31.876578</td>\n",
       "      <td>4491.28</td>\n",
       "      <td>4236.41</td>\n",
       "      <td>4353.527338</td>\n",
       "      <td>32.223892</td>\n",
       "      <td>4491.28</td>\n",
       "      <td>4236.41</td>\n",
       "      <td>4363.608728</td>\n",
       "      <td>30.548114</td>\n",
       "      <td>4487.69</td>\n",
       "      <td>4246.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Channel         Mean        Std      Max      Min  Eye-Open Mean  \\\n",
       "0      AF3  4298.144530  32.383173  4466.15  4198.97    4294.614386   \n",
       "1       F7  4007.199968  27.222342  4154.36  3913.33    4009.736631   \n",
       "2       F3  4261.656384  16.219297  4349.23  4197.44    4260.402014   \n",
       "3      FC5  4120.176654  16.904783  4191.79  4067.18    4120.948880   \n",
       "4       T7  4339.673482  11.994233  4397.95  4308.72    4339.912341   \n",
       "5       P7  4618.165981  12.923532  4672.31  4566.15    4619.437780   \n",
       "6       O1  4070.977384  17.707910  4138.97  4026.15    4070.531945   \n",
       "7       O2  4614.060973  14.398261  4672.82  4567.18    4613.460053   \n",
       "8       P8  4199.641958  13.807744  4255.90  4147.69    4198.939306   \n",
       "9       T8  4229.291521  15.005626  4288.21  4170.26    4227.845602   \n",
       "10     FC6  4199.729761  18.338192  4271.28  4125.13    4197.989321   \n",
       "11      F4  4276.866449  15.089147  4332.31  4216.41    4274.783659   \n",
       "12      F8  4603.123432  25.535726  4716.41  4490.77    4599.781686   \n",
       "13     AF4  4358.072562  31.876578  4491.28  4236.41    4353.527338   \n",
       "\n",
       "    Eye-Open Std  Eye-Open Max  Eye-Open Min  Eye-Closed Mean  Eye-Closed Std  \\\n",
       "0      33.537700       4466.15       4206.67      4302.444309       30.369437   \n",
       "1      28.256648       4154.36       3924.10      4004.110265       25.569113   \n",
       "2      16.607684       4349.23       4197.44      4263.184229       15.597616   \n",
       "3      16.536333       4187.18       4073.33      4119.236069       17.296483   \n",
       "4      11.199247       4395.90       4308.72      4339.382548       12.890536   \n",
       "5      13.217759       4671.28       4566.15      4616.616906       12.380614   \n",
       "6      14.417633       4124.10       4027.18      4071.519936       21.018584   \n",
       "7      13.655753       4672.82       4567.18      4614.792904       15.221895   \n",
       "8      12.424151       4255.90       4152.31      4200.497801       15.281592   \n",
       "9      14.217735       4287.69       4170.26      4231.052678       15.734208   \n",
       "10     17.985427       4264.10       4125.13      4201.849648       18.539501   \n",
       "11     15.042807       4329.74       4216.41      4279.403325       14.753427   \n",
       "12     23.835106       4711.79       4490.77      4607.193740       26.910407   \n",
       "13     32.223892       4491.28       4236.41      4363.608728       30.548114   \n",
       "\n",
       "    Eye-Closed Max  Eye-Closed Min  \n",
       "0          4441.03         4198.97  \n",
       "1          4138.97         3913.33  \n",
       "2          4333.85         4212.31  \n",
       "3          4191.79         4067.18  \n",
       "4          4397.95         4309.74  \n",
       "5          4672.31         4581.03  \n",
       "6          4138.97         4026.15  \n",
       "7          4672.82         4567.69  \n",
       "8          4255.38         4147.69  \n",
       "9          4288.21         4174.36  \n",
       "10         4271.28         4130.77  \n",
       "11         4332.31         4225.64  \n",
       "12         4716.41         4510.26  \n",
       "13         4487.69         4246.15  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "fp_csv_file = \"EEG_Eye_State_No_Outliers.csv\"\n",
    "metadata = get_metadata(os.path.join(root_directory, fp_csv_file))\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034266ed-f22d-41ab-91cf-731e332f7488",
   "metadata": {},
   "source": [
    "The differences between various channel metadata values in the eye-open versus eye-closed date tended to be very small. They did seem to be present though. Given this I thought it would be interesting to add these values as inputs to the neural network. I thought adding these as a feature might give the neural network some information that would make it easier to identify trends between the states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0585ad-872d-46f9-8e65-36ee2f5bbc73",
   "metadata": {},
   "source": [
    "I started out by just adding the mean and standard deviation for the data as a whole as one of the data points for each sample. To avoid passing information from validate or test into train I first split up the data into train, validate, and test sets. I decided to preserve the temporal ordering this time when splitting up the data. For each of these new datasets I then calculated the overall mean and standard deviation for each channel and added those as column data in each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a93f8a1-1515-4981-bcde-f5e7c93c496b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AF3', 4298.144529502238, 32.383172751002164, 4466.15, 4198.97,\n",
       "       4294.6143857415655, 33.53769989310272, 4466.15, 4206.67,\n",
       "       4302.444309195224, 30.369436627795682, 4441.03, 4198.97],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.iloc[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57ca8af8-705b-4cf4-b710-933c32ca12aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14304, 15)\n",
      "['AF3', 'AF3 Mean', 'AF3 Std', 'AF3 Max', 'AF3 Min', 'F7', 'F7 Mean', 'F7 Std', 'F7 Max', 'F7 Min', 'F3', 'F3 Mean', 'F3 Std', 'F3 Max', 'F3 Min', 'FC5', 'FC5 Mean', 'FC5 Std', 'FC5 Max', 'FC5 Min', 'T7', 'T7 Mean', 'T7 Std', 'T7 Max', 'T7 Min', 'P7', 'P7 Mean', 'P7 Std', 'P7 Max', 'P7 Min', 'O1', 'O1 Mean', 'O1 Std', 'O1 Max', 'O1 Min', 'O2', 'O2 Mean', 'O2 Std', 'O2 Max', 'O2 Min', 'P8', 'P8 Mean', 'P8 Std', 'P8 Max', 'P8 Min', 'T8', 'T8 Mean', 'T8 Std', 'T8 Max', 'T8 Min', 'FC6', 'FC6 Mean', 'FC6 Std', 'FC6 Max', 'FC6 Min', 'F4', 'F4 Mean', 'F4 Std', 'F4 Max', 'F4 Min', 'F8', 'F8 Mean', 'F8 Std', 'F8 Max', 'F8 Min', 'AF4', 'AF4 Mean', 'AF4 Std', 'AF4 Max', 'AF4 Min', 'eyeDetection']\n",
      "14304\n"
     ]
    }
   ],
   "source": [
    "# root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# fp_csv_file = \"EEG_Eye_State_No_Outliers.csv\"\n",
    "# data = pd.read_csv(\n",
    "#     os.path.join(root_directory, fp_csv_file), delimiter=\",\", usecols=range(15)\n",
    "# ).dropna(axis=0)\n",
    "\n",
    "# col_labels = []\n",
    "# print(data.shape)\n",
    "# for i in range(metadata.shape[0]):\n",
    "#     col_labels.append(metadata.iloc[i].values[0])\n",
    "#     col_labels.append(metadata.iloc[i].values[0] + \" \" + metadata.columns[1])\n",
    "#     col_labels.append(metadata.iloc[i].values[0] + \" \" + metadata.columns[2])\n",
    "#     col_labels.append(metadata.iloc[i].values[0] + \" \" + metadata.columns[3])\n",
    "#     col_labels.append(metadata.iloc[i].values[0] + \" \" + metadata.columns[4])\n",
    "\n",
    "# col_labels.append(\"eyeDetection\")\n",
    "# print(col_labels)\n",
    "\n",
    "# new_data = []\n",
    "# # num = 0\n",
    "# for index, row in data.iterrows():\n",
    "#     # if num == 100:\n",
    "#     #     break\n",
    "#     # num += num\n",
    "#     temp_list = []\n",
    "#     for i in range(len(row)):\n",
    "#         temp_list.append(row[i])\n",
    "#         if i < len(row) - 1:\n",
    "#             temp_list.append(metadata.iloc[i].values[1])\n",
    "#             temp_list.append(metadata.iloc[i].values[2])\n",
    "#             temp_list.append(metadata.iloc[i].values[3])\n",
    "#             temp_list.append(metadata.iloc[i].values[4])\n",
    "#     new_data.append(temp_list)\n",
    "# print(len(new_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87864d6b-1e5c-42e4-a1c3-123e7e600bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AF3             4329.23\n",
      "F7              4009.23\n",
      "F3              4289.23\n",
      "FC5             4148.21\n",
      "T7              4350.26\n",
      "P7              4586.15\n",
      "O1              4096.92\n",
      "O2              4641.03\n",
      "P8              4222.05\n",
      "T8              4238.46\n",
      "FC6             4211.28\n",
      "F4              4280.51\n",
      "F8              4635.90\n",
      "AF4             4393.85\n",
      "eyeDetection       0.00\n",
      "Name: 0, dtype: float64\n",
      "[4329.23, 4298.144529502238, 32.383172751002164, 4466.15, 4198.97, 4009.23, 4007.1999678411635, 27.222342363737564, 4154.36, 3913.33, 4289.23, 4261.656383529083, 16.2192974906191, 4349.23, 4197.44, 4148.21, 4120.176654082774, 16.90478308219174, 4191.79, 4067.18, 4350.26, 4339.673482242729, 11.99423301793753, 4397.95, 4308.72, 4586.15, 4618.165980844519, 12.923532182692702, 4672.31, 4566.15, 4096.92, 4070.977383948546, 17.707909625094178, 4138.97, 4026.15, 4641.03, 4614.060973154362, 14.39826115365047, 4672.82, 4567.18, 4222.05, 4199.641958193512, 13.807743808585931, 4255.9, 4147.69, 4238.46, 4229.291521252797, 15.005626018237718, 4288.21, 4170.26, 4211.28, 4199.729760906041, 18.338191607879672, 4271.28, 4125.13, 4280.51, 4276.866448545861, 15.089147439628482, 4332.31, 4216.41, 4635.9, 4603.123431907159, 25.53572578844711, 4716.41, 4490.77, 4393.85, 4358.072562220358, 31.876578172532213, 4491.28, 4236.41, 0.0]\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "# print(data.iloc[0])\n",
    "# print(new_data[0])\n",
    "# print(len(new_data[0]))\n",
    "# new_data_df = pd.DataFrame(new_data, columns=col_labels)\n",
    "# new_data_df = new_data_df.round(2)\n",
    "# new_data_df = new_data_df.astype({\"eyeDetection\": int})\n",
    "# new_data_df.to_csv(\n",
    "#     \"EEG_Eye_State_Mean_Std.csv\",\n",
    "#     encoding=\"utf-8\",\n",
    "#     index_label=False,\n",
    "#     index=False,\n",
    "#     quoting=csv.QUOTE_NONNUMERIC,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23268110-f43e-4378-93a6-c985bfdf9fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AF3             4329.23\n",
      "AF3 Mean        4298.14\n",
      "AF3 Std           32.38\n",
      "AF3 Max         4466.15\n",
      "AF3 Min         4198.97\n",
      "                 ...   \n",
      "AF4 Mean        4358.07\n",
      "AF4 Std           31.88\n",
      "AF4 Max         4491.28\n",
      "AF4 Min         4236.41\n",
      "eyeDetection       0.00\n",
      "Name: 0, Length: 71, dtype: float64\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# fp_csv_file = \"EEG_Eye_State_Mean_Std.csv\"\n",
    "# data = pd.read_csv(\n",
    "#     os.path.join(root_directory, fp_csv_file), delimiter=\",\", usecols=range(71)\n",
    "# ).dropna(axis=0)\n",
    "\n",
    "# train_fraction = 0.8\n",
    "# validate_fraction = 0.1\n",
    "# X = (\n",
    "#     pd.read_csv(\n",
    "#         os.path.join(root_directory, fp_csv_file), delimiter=\",\", usecols=range(71)\n",
    "#     )\n",
    "#     .dropna(axis=0)\n",
    "#     .to_numpy()\n",
    "# )\n",
    "# n_samples = X.shape[0]\n",
    "\n",
    "# n_train = round(n_samples * train_fraction)\n",
    "# n_validate = round(n_samples * validate_fraction)\n",
    "\n",
    "# Xtrain = X[:n_train]\n",
    "# Xvalidate = X[n_train : n_train + n_validate]\n",
    "# Xtest = X[n_train + n_validate :]\n",
    "# Xtrain = pd.DataFrame(Xtrain, columns=col_labels)\n",
    "# Xvalidate = pd.DataFrame(Xvalidate, columns=col_labels)\n",
    "# Xtest = pd.DataFrame(Xtest, columns=col_labels)\n",
    "# Xtrain = Xtrain.astype({\"eyeDetection\": int})\n",
    "# Xvalidate = Xvalidate.astype({\"eyeDetection\": int})\n",
    "# Xtest = Xtest.astype({\"eyeDetection\": int})\n",
    "# print(Xtrain.iloc[0])\n",
    "# print(type(X))\n",
    "# Xtrain.to_csv(\n",
    "#     \"EEG_Eye_State_Mean_Std_Train.csv\",\n",
    "#     encoding=\"utf-8\",\n",
    "#     index_label=False,\n",
    "#     index=False,\n",
    "#     quoting=csv.QUOTE_NONNUMERIC,\n",
    "# )\n",
    "# Xvalidate.to_csv(\n",
    "#     \"EEG_Eye_State_Mean_Std_Validate.csv\",\n",
    "#     encoding=\"utf-8\",\n",
    "#     index_label=False,\n",
    "#     index=False,\n",
    "#     quoting=csv.QUOTE_NONNUMERIC,\n",
    "# )\n",
    "# Xtest.to_csv(\n",
    "#     \"EEG_Eye_State_Mean_Std_Test.csv\",\n",
    "#     encoding=\"utf-8\",\n",
    "#     index_label=False,\n",
    "#     index=False,\n",
    "#     quoting=csv.QUOTE_NONNUMERIC,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9e2d50a0-a7e0-493f-9c3d-475ff56d23c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AF3             float64\n",
      "AF3 Mean        float64\n",
      "AF3 Std         float64\n",
      "AF3 Max         float64\n",
      "AF3 Min         float64\n",
      "                 ...   \n",
      "AF4 Mean        float64\n",
      "AF4 Std         float64\n",
      "AF4 Max         float64\n",
      "AF4 Min         float64\n",
      "eyeDetection      int64\n",
      "Length: 71, dtype: object\n",
      "AF3             float64\n",
      "AF3 Mean        float64\n",
      "AF3 Std         float64\n",
      "AF3 Max         float64\n",
      "AF3 Min         float64\n",
      "                 ...   \n",
      "AF4 Mean        float64\n",
      "AF4 Std         float64\n",
      "AF4 Max         float64\n",
      "AF4 Min         float64\n",
      "eyeDetection      int64\n",
      "Length: 71, dtype: object\n",
      "AF3             float64\n",
      "AF3 Mean        float64\n",
      "AF3 Std         float64\n",
      "AF3 Max         float64\n",
      "AF3 Min         float64\n",
      "                 ...   \n",
      "AF4 Mean        float64\n",
      "AF4 Std         float64\n",
      "AF4 Max         float64\n",
      "AF4 Min         float64\n",
      "eyeDetection      int64\n",
      "Length: 71, dtype: object\n"
     ]
    }
   ],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "\n",
    "train_file = \"EEG_Eye_State_Mean_Std_Train.csv\"\n",
    "validate_file = \"EEG_Eye_State_Mean_Std_Validate.csv\"\n",
    "test_file = \"EEG_Eye_State_Mean_Std_Test.csv\"\n",
    "\n",
    "# train_file = \"Dummy_Train.csv\"\n",
    "# validate_file = \"Dummy_Validate.csv\"\n",
    "# test_file = \"Dummy_Test.csv\"\n",
    "\n",
    "train_dataset = fpnn.FinalProjectEEGDataset(train_file, root_directory)\n",
    "validate_dataset = fpnn.FinalProjectEEGDataset(validate_file, root_directory)\n",
    "test_dataset = fpnn.FinalProjectEEGDataset(test_file, root_directory)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "50c60fd8-8d75-4d5f-8b2d-bce352c41cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=70, out_features=5, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (Linear2): Linear(in_features=5, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 93.4%, Avg loss: 0.298307 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 6.6%, Avg loss: 0.778059 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 6.6%, Avg loss: 0.778044 \n",
      "\n",
      "Done:  254.56435704231262  seconds\n",
      "Test Error: \n",
      " Accuracy: 6.6%, Avg loss: 0.778044 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 70\n",
    "n_outputs = 2\n",
    "n_hiddens = [5, 5]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs).to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 101\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "00da12db-6e44-4267-945e-0653fdcde92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 6.5%, Avg loss: 0.773588 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd86fc2-0056-4a8b-b327-870ec76ea810",
   "metadata": {},
   "source": [
    "This attempt went poorly. I quickly found that my model did not produce very good results. Imagine this was likely because the data in each particular sample was so similar. I decided to abandon this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18c9999-11b5-47fb-8528-d05abe6fd9cd",
   "metadata": {},
   "source": [
    "## Lumping the Data Into Chunks of Time ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0f3ca7-523a-4fc9-83df-23abb52e587e",
   "metadata": {},
   "source": [
    "\n",
    "I decided I would try lumping the data into chunks of time. I thought it might be useful to lump the data in this manner so that the machine learning model could get input that was collected over a greater period of time and would hopefully be able to use this to make better predictions. In order to make this work I first needed to appropriately lump the data. I would need to ensure that each chunk only contained data for eyes-open or eyes-closed states. I would throw out ambiguous chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3b0c1228-67ce-4eda-a586-7d9fb6b7c870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14304, 15)\n",
      "['AF3_0', 'F7_0', 'F3_0', 'FC5_0', 'T7_0', 'P7_0', 'O1_0', 'O2_0', 'P8_0', 'T8_0', 'FC6_0', 'F4_0', 'F8_0', 'AF4_0', 'AF3_1', 'F7_1', 'F3_1', 'FC5_1', 'T7_1', 'P7_1', 'O1_1', 'O2_1', 'P8_1', 'T8_1', 'FC6_1', 'F4_1', 'F8_1', 'AF4_1', 'AF3_2', 'F7_2', 'F3_2', 'FC5_2', 'T7_2', 'P7_2', 'O1_2', 'O2_2', 'P8_2', 'T8_2', 'FC6_2', 'F4_2', 'F8_2', 'AF4_2', 'AF3_3', 'F7_3', 'F3_3', 'FC5_3', 'T7_3', 'P7_3', 'O1_3', 'O2_3', 'P8_3', 'T8_3', 'FC6_3', 'F4_3', 'F8_3', 'AF4_3', 'AF3_4', 'F7_4', 'F3_4', 'FC5_4', 'T7_4', 'P7_4', 'O1_4', 'O2_4', 'P8_4', 'T8_4', 'FC6_4', 'F4_4', 'F8_4', 'AF4_4', 'AF3_5', 'F7_5', 'F3_5', 'FC5_5', 'T7_5', 'P7_5', 'O1_5', 'O2_5', 'P8_5', 'T8_5', 'FC6_5', 'F4_5', 'F8_5', 'AF4_5', 'AF3_6', 'F7_6', 'F3_6', 'FC5_6', 'T7_6', 'P7_6', 'O1_6', 'O2_6', 'P8_6', 'T8_6', 'FC6_6', 'F4_6', 'F8_6', 'AF4_6', 'AF3_7', 'F7_7', 'F3_7', 'FC5_7', 'T7_7', 'P7_7', 'O1_7', 'O2_7', 'P8_7', 'T8_7', 'FC6_7', 'F4_7', 'F8_7', 'AF4_7', 'AF3_8', 'F7_8', 'F3_8', 'FC5_8', 'T7_8', 'P7_8', 'O1_8', 'O2_8', 'P8_8', 'T8_8', 'FC6_8', 'F4_8', 'F8_8', 'AF4_8', 'AF3_9', 'F7_9', 'F3_9', 'FC5_9', 'T7_9', 'P7_9', 'O1_9', 'O2_9', 'P8_9', 'T8_9', 'FC6_9', 'F4_9', 'F8_9', 'AF4_9', 'eyeDetection']\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "Deleted Ambiguous Chunk\n",
      "1411\n",
      "776\n",
      "635\n",
      "Deleted: 19\n"
     ]
    }
   ],
   "source": [
    "# root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# fp_csv_file = \"EEG_Eye_State_No_Outliers.csv\"\n",
    "# data = pd.read_csv(\n",
    "#     os.path.join(root_directory, fp_csv_file), delimiter=\",\", usecols=range(15)\n",
    "# ).dropna(axis=0)\n",
    "\n",
    "# num_samples_in_chunk = 10\n",
    "# col_labels = []\n",
    "# print(data.shape)\n",
    "# for i in range(num_samples_in_chunk):\n",
    "\n",
    "#     for label in data.columns:\n",
    "#         if label == \"eyeDetection\":\n",
    "#             continue\n",
    "#         col_labels.append(label + \"_\" + str(i))\n",
    "\n",
    "# col_labels.append(\"eyeDetection\")\n",
    "# print(col_labels)\n",
    "\n",
    "# new_data = []\n",
    "\n",
    "# temp_list = []\n",
    "# count = 0\n",
    "# has_zero = False\n",
    "# has_one = False\n",
    "# zeros = 0\n",
    "# ones = 0\n",
    "# deleted = 0\n",
    "# for index, row in data.iterrows():\n",
    "\n",
    "#     if count == num_samples_in_chunk:\n",
    "#         if has_zero and has_one:\n",
    "#             count = 0\n",
    "#             temp_list = []\n",
    "#             print(\"Deleted Ambiguous Chunk\")\n",
    "#             deleted += 1\n",
    "#         elif has_zero:\n",
    "#             temp_list.append(0)\n",
    "#             new_data.append(temp_list)\n",
    "#             count = 0\n",
    "#             temp_list = []\n",
    "#             zeros += 1\n",
    "#         else:\n",
    "#             temp_list.append(1)\n",
    "#             new_data.append(temp_list)\n",
    "#             count = 0\n",
    "#             temp_list = []\n",
    "#             ones += 1\n",
    "#         has_zero = False\n",
    "#         has_one = False\n",
    "\n",
    "#     for i in range(len(row)):\n",
    "\n",
    "#         if i < len(row) - 1:\n",
    "#             temp_list.append(row[i])\n",
    "#         else:\n",
    "#             if row[i] == 0:\n",
    "#                 has_zero = True\n",
    "#             else:\n",
    "#                 has_one = True\n",
    "#     count += 1\n",
    "# print(len(new_data))\n",
    "# print(zeros)\n",
    "# print(ones)\n",
    "# print(\"Deleted: \" + str(deleted))\n",
    "\n",
    "# new_data_df = pd.DataFrame(new_data, columns=col_labels)\n",
    "# new_data_df = new_data_df.round(2)\n",
    "# new_data_df = new_data_df.astype({\"eyeDetection\": int})\n",
    "# new_data_df.to_csv(\n",
    "#     \"EEG_Eye_State_Lumped_\" + str(num_samples_in_chunk) + \".csv\",\n",
    "#     encoding=\"utf-8\",\n",
    "#     index_label=False,\n",
    "#     index=False,\n",
    "#     quoting=csv.QUOTE_NONNUMERIC,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "637b5882-6deb-4725-9582-1ef8fd53e9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AF3_0           4296.92\n",
      "F7_0            4012.31\n",
      "F3_0            4264.62\n",
      "FC5_0           4128.21\n",
      "T7_0            4336.92\n",
      "                 ...   \n",
      "FC6_9           4222.56\n",
      "F4_9            4291.28\n",
      "F8_9            4631.79\n",
      "AF4_9           4369.74\n",
      "eyeDetection       1.00\n",
      "Name: 0, Length: 141, dtype: float64\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "num_samples_in_chunk = 10\n",
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "fp_csv_file = \"EEG_Eye_State_Lumped_\" + str(num_samples_in_chunk) + \".csv\"\n",
    "train_fraction = 0.8\n",
    "validate_fraction = 0.1\n",
    "X_pandas = pd.read_csv(\n",
    "    os.path.join(root_directory, fp_csv_file),\n",
    "    delimiter=\",\",\n",
    ").dropna(axis=0)\n",
    "X = X_pandas.to_numpy()\n",
    "\n",
    "n_samples = X.shape[0]\n",
    "rows = np.arange(n_samples)\n",
    "np.random.shuffle(rows)\n",
    "\n",
    "n_train = round(n_samples * train_fraction)\n",
    "n_validate = round(n_samples * validate_fraction)\n",
    "\n",
    "Xtrain = X[rows[:n_train], :]\n",
    "Xvalidate = X[rows[n_train : n_train + n_validate], :]\n",
    "Xtest = X[rows[n_train + n_validate :], :]\n",
    "Xtrain = pd.DataFrame(Xtrain, columns=X_pandas.columns)\n",
    "Xvalidate = pd.DataFrame(Xvalidate, columns=X_pandas.columns)\n",
    "Xtest = pd.DataFrame(Xtest, columns=X_pandas.columns)\n",
    "Xtrain = Xtrain.astype({\"eyeDetection\": int})\n",
    "Xvalidate = Xvalidate.astype({\"eyeDetection\": int})\n",
    "Xtest = Xtest.astype({\"eyeDetection\": int})\n",
    "print(Xtrain.iloc[0])\n",
    "print(type(X))\n",
    "Xtrain.to_csv(\n",
    "    \"EEG_Eye_State_Lumped_\" + str(num_samples_in_chunk) + \"_Train.csv\",\n",
    "    encoding=\"utf-8\",\n",
    "    index_label=False,\n",
    "    index=False,\n",
    "    quoting=csv.QUOTE_NONNUMERIC,\n",
    ")\n",
    "Xvalidate.to_csv(\n",
    "    \"EEG_Eye_State_Lumped_\" + str(num_samples_in_chunk) + \"_Validate.csv\",\n",
    "    encoding=\"utf-8\",\n",
    "    index_label=False,\n",
    "    index=False,\n",
    "    quoting=csv.QUOTE_NONNUMERIC,\n",
    ")\n",
    "Xtest.to_csv(\n",
    "    \"EEG_Eye_State_Lumped_\" + str(num_samples_in_chunk) + \"_Test.csv\",\n",
    "    encoding=\"utf-8\",\n",
    "    index_label=False,\n",
    "    index=False,\n",
    "    quoting=csv.QUOTE_NONNUMERIC,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d32807f-16e9-46d5-ab07-711bb7580253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeros:  612\n",
      "Ones:  518\n",
      "Zeros:  88\n",
      "Ones:  54\n",
      "Zeros:  76\n",
      "Ones:  66\n",
      "AF3_0           float64\n",
      "F7_0            float64\n",
      "F3_0            float64\n",
      "FC5_0           float64\n",
      "T7_0            float64\n",
      "                 ...   \n",
      "FC6_9           float64\n",
      "F4_9            float64\n",
      "F8_9            float64\n",
      "AF4_9           float64\n",
      "eyeDetection      int64\n",
      "Length: 141, dtype: object\n",
      "AF3_0           float64\n",
      "F7_0            float64\n",
      "F3_0            float64\n",
      "FC5_0           float64\n",
      "T7_0            float64\n",
      "                 ...   \n",
      "FC6_9           float64\n",
      "F4_9            float64\n",
      "F8_9            float64\n",
      "AF4_9           float64\n",
      "eyeDetection      int64\n",
      "Length: 141, dtype: object\n",
      "AF3_0           float64\n",
      "F7_0            float64\n",
      "F3_0            float64\n",
      "FC5_0           float64\n",
      "T7_0            float64\n",
      "                 ...   \n",
      "FC6_9           float64\n",
      "F4_9            float64\n",
      "F8_9            float64\n",
      "AF4_9           float64\n",
      "eyeDetection      int64\n",
      "Length: 141, dtype: object\n"
     ]
    }
   ],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "\n",
    "train_file = \"EEG_Eye_State_Lumped_10_Train.csv\"\n",
    "validate_file = \"EEG_Eye_State_Lumped_10_Validate.csv\"\n",
    "test_file = \"EEG_Eye_State_Lumped_10_Test.csv\"\n",
    "\n",
    "# train_file = \"Dummy_Train.csv\"\n",
    "# validate_file = \"Dummy_Validate.csv\"\n",
    "# test_file = \"Dummy_Test.csv\"\n",
    "count_ones_and_zeros(train_file)\n",
    "count_ones_and_zeros(validate_file)\n",
    "count_ones_and_zeros(test_file)\n",
    "train_dataset = fpnn.FinalProjectEEGDataset(train_file, root_directory)\n",
    "validate_dataset = fpnn.FinalProjectEEGDataset(validate_file, root_directory)\n",
    "test_dataset = fpnn.FinalProjectEEGDataset(test_file, root_directory)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "464f658c-8357-419b-988d-cb206588d5b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=140, out_features=5, bias=True)\n",
      "    (Sigmoid0): Sigmoid()\n",
      "    (Linear1): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (Sigmoid1): Sigmoid()\n",
      "    (Linear2): Linear(in_features=5, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.691872 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.689113 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.689113 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.689113 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.689113 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.689113 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.689113 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.689113 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.689113 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.689113 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.689113 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.689113 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.689113 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.689113 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.689113 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.689113 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.689113 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.689113 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.689113 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.689113 \n",
      "\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.689113 \n",
      "\n",
      "Done:  271.01529598236084  seconds\n",
      "Test Error: \n",
      " Accuracy: 54.2%, Avg loss: 0.689113 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 140\n",
    "n_outputs = 2\n",
    "n_hiddens = [5, 5]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"sigmoid\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 1001\n",
    "train_for_epochs(epochs, train_dataloader, train_dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee9ea173-951b-4e89-92b9-6300e859a50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667735 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(validate_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b1bec51-05d0-4b5e-ad4c-d44b7b4204f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=140, out_features=20, bias=True)\n",
      "    (Sigmoid0): Sigmoid()\n",
      "    (Linear1): Linear(in_features=20, out_features=15, bias=True)\n",
      "    (Sigmoid1): Sigmoid()\n",
      "    (Linear2): Linear(in_features=15, out_features=10, bias=True)\n",
      "    (Sigmoid2): Sigmoid()\n",
      "    (Linear3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 37.6%, Avg loss: 0.694391 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.665792 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.665787 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.665785 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.665785 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.665785 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.665785 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.665785 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.665785 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.665785 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.665785 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.665785 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.665785 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.665785 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.665785 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.665785 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.665785 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.665785 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.665785 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.665785 \n",
      "\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.665785 \n",
      "\n",
      "Done:  276.6492304801941  seconds\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.665785 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 140\n",
    "n_outputs = 2\n",
    "n_hiddens = [20, 15, 10]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"sigmoid\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.05\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 1001\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4118b1f3-547f-4af4-a84d-4f9c81e46740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.665785 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(validate_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e7f47cd-0c2f-46bf-9a04-a25217ae59c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=140, out_features=20, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=20, out_features=15, bias=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (Linear2): Linear(in_features=15, out_features=10, bias=True)\n",
      "    (ReLU2): ReLU()\n",
      "    (Linear3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.678976 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667083 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667083 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667083 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667083 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667083 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667083 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667083 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667083 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667083 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667083 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667083 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667083 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667083 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667083 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667083 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667083 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667083 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667083 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667083 \n",
      "\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667083 \n",
      "\n",
      "Done:  273.7759130001068  seconds\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667083 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 140\n",
    "n_outputs = 2\n",
    "n_hiddens = [20, 15, 10]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"relu\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 1001\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18358684-34d3-4aa9-bfa6-31ce1456cdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667083 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(validate_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0386452-ff1f-4111-a174-a9450a75e5b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=140, out_features=20, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=20, out_features=15, bias=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (Linear2): Linear(in_features=15, out_features=10, bias=True)\n",
      "    (ReLU2): ReLU()\n",
      "    (Linear3): Linear(in_features=10, out_features=5, bias=True)\n",
      "    (ReLU3): ReLU()\n",
      "    (Linear4): Linear(in_features=5, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.690325 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.666413 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.666431 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.666440 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.666445 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.666447 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.666448 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.666449 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.666449 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.666450 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.666450 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.666450 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.666450 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.666450 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.666450 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.666450 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.666450 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.666450 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.666450 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.666450 \n",
      "\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.666450 \n",
      "\n",
      "Done:  273.3590278625488  seconds\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.666450 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 140\n",
    "n_outputs = 2\n",
    "n_hiddens = [20, 15, 10, 5]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"relu\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.2\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 1001\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7e75468-8d91-4079-bed9-cb5b0a7e173c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.666450 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(validate_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "251a3cb5-d950-421d-8011-ddc65f3d38c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=140, out_features=20, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=20, out_features=15, bias=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (Linear2): Linear(in_features=15, out_features=10, bias=True)\n",
      "    (ReLU2): ReLU()\n",
      "    (Linear3): Linear(in_features=10, out_features=5, bias=True)\n",
      "    (ReLU3): ReLU()\n",
      "    (Linear4): Linear(in_features=5, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.664654 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.663878 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.663878 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.663878 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.663878 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.663878 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.663878 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.663878 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.663878 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.663878 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.663878 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.663878 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.663878 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.663878 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.663878 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.663878 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.663878 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.663878 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.663878 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.663878 \n",
      "\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.663878 \n",
      "\n",
      "Done:  273.90955567359924  seconds\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.663878 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 140\n",
    "n_outputs = 2\n",
    "n_hiddens = [20, 15, 10, 5]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"relu\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.5\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 1001\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d523ff80-bb51-47b0-abdf-a389359a226e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=140, out_features=20, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=20, out_features=15, bias=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (Linear2): Linear(in_features=15, out_features=10, bias=True)\n",
      "    (ReLU2): ReLU()\n",
      "    (Linear3): Linear(in_features=10, out_features=5, bias=True)\n",
      "    (ReLU3): ReLU()\n",
      "    (Linear4): Linear(in_features=5, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 37.6%, Avg loss: 0.727988 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 37.6%, Avg loss: 0.713202 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 37.6%, Avg loss: 0.702363 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 37.6%, Avg loss: 0.694331 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.688317 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.683768 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.680298 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.677629 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.675563 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.673953 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.672694 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.671704 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.670924 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.670307 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.669818 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.669430 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.669122 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.668876 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.668681 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.668525 \n",
      "\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.668400 \n",
      "\n",
      "Done:  276.1405906677246  seconds\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.668400 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 140\n",
    "n_outputs = 2\n",
    "n_hiddens = [20, 15, 10, 5]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"relu\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.0005\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 1001\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa3357a9-0cb1-4c8d-8fd5-a0ff9d87d9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.668400 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(validate_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b1a7a2e-ff53-46be-abd6-77beeca95606",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=140, out_features=20, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=20, out_features=15, bias=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (Linear2): Linear(in_features=15, out_features=10, bias=True)\n",
      "    (ReLU2): ReLU()\n",
      "    (Linear3): Linear(in_features=10, out_features=5, bias=True)\n",
      "    (ReLU3): ReLU()\n",
      "    (Linear4): Linear(in_features=5, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.675453 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.673630 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.672261 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.671229 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.670447 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.669851 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.669397 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.669050 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.668785 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.668581 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.668425 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.668305 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.668212 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.668141 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.668087 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.668045 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.668012 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667988 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667969 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667954 \n",
      "\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667942 \n",
      "\n",
      "Epoch 1050\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667934 \n",
      "\n",
      "Epoch 1100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667927 \n",
      "\n",
      "Epoch 1150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667922 \n",
      "\n",
      "Epoch 1200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667918 \n",
      "\n",
      "Epoch 1250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667915 \n",
      "\n",
      "Epoch 1300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667912 \n",
      "\n",
      "Epoch 1350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667910 \n",
      "\n",
      "Epoch 1400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667909 \n",
      "\n",
      "Epoch 1450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667908 \n",
      "\n",
      "Epoch 1500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667907 \n",
      "\n",
      "Epoch 1550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667906 \n",
      "\n",
      "Epoch 1600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667905 \n",
      "\n",
      "Epoch 1650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667905 \n",
      "\n",
      "Epoch 1700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667905 \n",
      "\n",
      "Epoch 1750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 1800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 1850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 1900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 1950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 2000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 2050\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 2100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 2150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 2200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 2250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 2300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 2350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 2400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 2450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 2500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 2550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 2600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 2650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 2700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 2750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 2800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 2850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 2900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 2950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 3000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 3050\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 3100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 3150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 3200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 3250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 3300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 3350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 3400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 3450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 3500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 3550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 3600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 3650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 3700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 3750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 3800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 3850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 3900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 3950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 4000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 4050\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 4100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 4150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 4200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 4250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 4300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 4350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 4400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 4450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 4500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 4550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 4600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 4650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 4700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 4750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 4800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 4850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 4900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 4950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 5000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 5050\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 5100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 5150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 5200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 5250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 5300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 5350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 5400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 5450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 5500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 5550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 5600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 5650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 5700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 5750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 5800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 5850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 5900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 5950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 6000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 6050\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 6100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 6150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 6200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 6250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 6300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 6350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 6400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 6450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 6500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 6550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 6600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 6650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 6700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 6750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 6800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 6850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 6900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 6950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 7000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 7050\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 7100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 7150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 7200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 7250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 7300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 7350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 7400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667903 \n",
      "\n",
      "Epoch 7450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 7500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 7550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 7600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 7650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 7700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 7750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 7800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 7850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 7900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 7950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 8000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 8050\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 8100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 8150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 8200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 8250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n",
      "Epoch 8300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667904 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\RANDAL~1\\AppData\\Local\\Temp/ipykernel_12324/2920103868.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m train_for_epochs(\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m )\n",
      "\u001b[1;32mC:\\Users\\RANDAL~1\\AppData\\Local\\Temp/ipykernel_12324/704544453.py\u001b[0m in \u001b[0;36mtrain_for_epochs\u001b[1;34m(epochs, train_dataloader, _validate_dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m50\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {t}\\n-------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\RANDAL~1\\AppData\\Local\\Temp/ipykernel_12324/851795240.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# first = True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\fp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\fp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\fp\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda3\\envs\\fp\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\fp\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\fp\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# scalars\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\fp\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# scalars\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 140\n",
    "n_outputs = 2\n",
    "n_hiddens = [20, 15, 10, 5]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"relu\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.0005\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 10001\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50daad90-540c-470a-948a-256b3e36916e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.667896 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(validate_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a05ac60a-aee1-475f-bece-98f3e476cac8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=140, out_features=20, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=20, out_features=15, bias=True)\n",
      "    (Softplus1): Softplus(beta=1, threshold=20)\n",
      "    (Linear2): Linear(in_features=15, out_features=10, bias=True)\n",
      "    (Softplus2): Softplus(beta=1, threshold=20)\n",
      "    (Linear3): Linear(in_features=10, out_features=5, bias=True)\n",
      "    (Softplus3): Softplus(beta=1, threshold=20)\n",
      "    (Linear4): Linear(in_features=5, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 37.6%, Avg loss: 0.780831 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 37.6%, Avg loss: 0.751333 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 37.6%, Avg loss: 0.730130 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 37.6%, Avg loss: 0.714763 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 37.6%, Avg loss: 0.703513 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 37.6%, Avg loss: 0.695188 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.688961 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.684258 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.680673 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.677918 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.675787 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.674129 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.672831 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.671812 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.671009 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.670375 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.669872 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.669473 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.669156 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.668903 \n",
      "\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.668702 \n",
      "\n",
      "Done:  274.9537630081177  seconds\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.668702 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 140\n",
    "n_outputs = 2\n",
    "n_hiddens = [20, 15, 10, 5]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"softplus\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.0005\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 1001\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f6a63f2-59c1-4005-879c-540ae934b7ac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.668702 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(validate_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9dc6d0-8cde-43c5-a718-9778ea63955a",
   "metadata": {},
   "source": [
    "https://github.com/scarca/EEG-Eyes\n",
    "https://www.researchgate.net/publication/325126184_Deep_Learning_of_EEG_Time-Frequency_Representations_for_Identifying_Eye_States"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0938d7e-0147-4c59-831a-abf8248a4a62",
   "metadata": {},
   "source": [
    "The results I achieved were disappointing so I decided to try a new approach. This time I decided I would normalize the data before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6c614128-6f62-4a4c-b31b-186522420612",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# num_samples_in_chunk = 10\n",
    "# root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# fp_csv_file = \"EEG_Eye_State_Lumped_\" + str(num_samples_in_chunk) + \".csv\"\n",
    "# train_fraction = 0.8\n",
    "# validate_fraction = 0.1\n",
    "# X_pandas = pd.read_csv(\n",
    "#     os.path.join(root_directory, fp_csv_file),\n",
    "#     delimiter=\",\",\n",
    "# ).dropna(axis=0)\n",
    "\n",
    "# X.to_csv(\n",
    "#     \"EEG_Eye_State_Lumped_\" + str(num_samples_in_chunk) + \"_Normalized.csv\",\n",
    "#     encoding=\"utf-8\",\n",
    "#     index_label=False,\n",
    "#     index=False,\n",
    "#     quoting=csv.QUOTE_NONNUMERIC,\n",
    "# )\n",
    "\n",
    "# X = X_pandas.to_numpy()\n",
    "# n_samples = X.shape[0]\n",
    "# rows = np.arange(n_samples)\n",
    "# np.random.shuffle(rows)\n",
    "\n",
    "# n_train = round(n_samples * train_fraction)\n",
    "# n_validate = round(n_samples * validate_fraction)\n",
    "\n",
    "# Xtrain = X[rows[:n_train], :]\n",
    "# Xvalidate = X[rows[n_train : n_train + n_validate], :]\n",
    "# Xtest = X[rows[n_train + n_validate :], :]\n",
    "# Xtrain = pd.DataFrame(Xtrain, columns=X_pandas.columns)\n",
    "# Xvalidate = pd.DataFrame(Xvalidate, columns=X_pandas.columns)\n",
    "# Xtest = pd.DataFrame(Xtest, columns=X_pandas.columns)\n",
    "# Xtrain = Xtrain.astype({\"eyeDetection\": int})\n",
    "# Xvalidate = Xvalidate.astype({\"eyeDetection\": int})\n",
    "# Xtest = Xtest.astype({\"eyeDetection\": int})\n",
    "\n",
    "# new = {}\n",
    "# for i in Xtrain:\n",
    "#     if i != \"eyeDetection\":\n",
    "#         new[i] = (Xtrain[i] - Xtrain[i].median()) / Xtrain[i].std()\n",
    "#     else:\n",
    "#         new[i] = Xtrain[i]\n",
    "\n",
    "# Xtrain = pd.DataFrame(new)\n",
    "\n",
    "# new = {}\n",
    "# for i in Xvalidate:\n",
    "#     if i != \"eyeDetection\":\n",
    "#         new[i] = (Xvalidate[i] - Xvalidate[i].median()) / Xvalidate[i].std()\n",
    "#     else:\n",
    "#         new[i] = Xvalidate[i]\n",
    "\n",
    "# Xvalidate = pd.DataFrame(new)\n",
    "\n",
    "# new = {}\n",
    "# for i in Xtest:\n",
    "#     if i != \"eyeDetection\":\n",
    "#         new[i] = (Xtest[i] - Xtest[i].median()) / Xtest[i].std()\n",
    "#     else:\n",
    "#         new[i] = Xtest[i]\n",
    "\n",
    "# Xtest = pd.DataFrame(new)\n",
    "\n",
    "# Xtrain.to_csv(\n",
    "#     \"EEG_Eye_State_Lumped_\" + str(num_samples_in_chunk) + \"_Normalized_Train.csv\",\n",
    "#     encoding=\"utf-8\",\n",
    "#     index_label=False,\n",
    "#     index=False,\n",
    "#     quoting=csv.QUOTE_NONNUMERIC,\n",
    "# )\n",
    "# Xvalidate.to_csv(\n",
    "#     \"EEG_Eye_State_Lumped_\" + str(num_samples_in_chunk) + \"_Normalized_Validate.csv\",\n",
    "#     encoding=\"utf-8\",\n",
    "#     index_label=False,\n",
    "#     index=False,\n",
    "#     quoting=csv.QUOTE_NONNUMERIC,\n",
    "# )\n",
    "# Xtest.to_csv(\n",
    "#     \"EEG_Eye_State_Lumped_\" + str(num_samples_in_chunk) + \"_Normalized_Test.csv\",\n",
    "#     encoding=\"utf-8\",\n",
    "#     index_label=False,\n",
    "#     index=False,\n",
    "#     quoting=csv.QUOTE_NONNUMERIC,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "659cec60-f736-4e7e-b39c-c5c1a73b2c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeros:  621\n",
      "Ones:  509\n",
      "Zeros:  70\n",
      "Ones:  72\n",
      "Zeros:  85\n",
      "Ones:  57\n",
      "AF3_0           float64\n",
      "F7_0            float64\n",
      "F3_0            float64\n",
      "FC5_0           float64\n",
      "T7_0            float64\n",
      "                 ...   \n",
      "FC6_9           float64\n",
      "F4_9            float64\n",
      "F8_9            float64\n",
      "AF4_9           float64\n",
      "eyeDetection      int64\n",
      "Length: 141, dtype: object\n",
      "AF3_0           float64\n",
      "F7_0            float64\n",
      "F3_0            float64\n",
      "FC5_0           float64\n",
      "T7_0            float64\n",
      "                 ...   \n",
      "FC6_9           float64\n",
      "F4_9            float64\n",
      "F8_9            float64\n",
      "AF4_9           float64\n",
      "eyeDetection      int64\n",
      "Length: 141, dtype: object\n",
      "AF3_0           float64\n",
      "F7_0            float64\n",
      "F3_0            float64\n",
      "FC5_0           float64\n",
      "T7_0            float64\n",
      "                 ...   \n",
      "FC6_9           float64\n",
      "F4_9            float64\n",
      "F8_9            float64\n",
      "AF4_9           float64\n",
      "eyeDetection      int64\n",
      "Length: 141, dtype: object\n"
     ]
    }
   ],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "\n",
    "train_file = \"EEG_Eye_State_Lumped_10_Normalized_Train.csv\"\n",
    "validate_file = \"EEG_Eye_State_Lumped_10_Normalized_Validate.csv\"\n",
    "test_file = \"EEG_Eye_State_Lumped_10_Normalized_Test.csv\"\n",
    "\n",
    "# train_file = \"Dummy_Train.csv\"\n",
    "# validate_file = \"Dummy_Validate.csv\"\n",
    "# test_file = \"Dummy_Test.csv\"\n",
    "count_ones_and_zeros(train_file)\n",
    "count_ones_and_zeros(validate_file)\n",
    "count_ones_and_zeros(test_file)\n",
    "train_dataset = fpnn.FinalProjectEEGDataset(train_file, root_directory)\n",
    "validate_dataset = fpnn.FinalProjectEEGDataset(validate_file, root_directory)\n",
    "test_dataset = fpnn.FinalProjectEEGDataset(test_file, root_directory)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a965de0b-bcf9-43fc-bbfb-c29832c10b42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=140, out_features=20, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=20, out_features=15, bias=True)\n",
      "    (Softplus1): Softplus(beta=1, threshold=20)\n",
      "    (Linear2): Linear(in_features=15, out_features=10, bias=True)\n",
      "    (Softplus2): Softplus(beta=1, threshold=20)\n",
      "    (Linear3): Linear(in_features=10, out_features=5, bias=True)\n",
      "    (Softplus3): Softplus(beta=1, threshold=20)\n",
      "    (Linear4): Linear(in_features=5, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 40.4%, Avg loss: 0.718604 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 40.4%, Avg loss: 0.696739 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.687572 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.683372 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.681303 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.680228 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.679649 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.679328 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.679148 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.679044 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.678984 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.678947 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.678924 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.678908 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.678897 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.678888 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.678880 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.678873 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.678866 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.678860 \n",
      "\n",
      "Done:  274.6565582752228  seconds\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.678853 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 140\n",
    "n_outputs = 2\n",
    "n_hiddens = [20, 15, 10, 5]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"softplus\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.0005\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 1000\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c22db1d-1bd0-4997-8520-add661166857",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=140, out_features=6, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=6, out_features=3, bias=True)\n",
      "    (Softplus1): Softplus(beta=1, threshold=20)\n",
      "    (Linear2): Linear(in_features=3, out_features=1000, bias=True)\n",
      "    (Softplus2): Softplus(beta=1, threshold=20)\n",
      "    (Linear3): Linear(in_features=1000, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.679144 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.676659 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.676233 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.675771 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.675189 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.674380 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.673224 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 0.671550 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.669058 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.665533 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.660729 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 0.654865 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 0.648699 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.7%, Avg loss: 0.642956 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.636750 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 0.629383 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.621204 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.612665 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.602028 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.586862 \n",
      "\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.568341 \n",
      "\n",
      "Epoch 1050\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.549349 \n",
      "\n",
      "Epoch 1100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.531484 \n",
      "\n",
      "Epoch 1150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.511080 \n",
      "\n",
      "Epoch 1200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.494450 \n",
      "\n",
      "Epoch 1250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.476892 \n",
      "\n",
      "Epoch 1300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.462469 \n",
      "\n",
      "Epoch 1350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.447284 \n",
      "\n",
      "Epoch 1400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.437546 \n",
      "\n",
      "Epoch 1450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.427951 \n",
      "\n",
      "Epoch 1500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.417047 \n",
      "\n",
      "Epoch 1550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.406999 \n",
      "\n",
      "Epoch 1600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.396525 \n",
      "\n",
      "Epoch 1650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.388307 \n",
      "\n",
      "Epoch 1700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.380185 \n",
      "\n",
      "Epoch 1750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.372902 \n",
      "\n",
      "Epoch 1800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.368956 \n",
      "\n",
      "Epoch 1850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.367979 \n",
      "\n",
      "Epoch 1900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.365420 \n",
      "\n",
      "Epoch 1950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.364422 \n",
      "\n",
      "Epoch 2000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.362238 \n",
      "\n",
      "Epoch 2050\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.359707 \n",
      "\n",
      "Epoch 2100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.357074 \n",
      "\n",
      "Epoch 2150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.355151 \n",
      "\n",
      "Epoch 2200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.351835 \n",
      "\n",
      "Epoch 2250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.350302 \n",
      "\n",
      "Epoch 2300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.347719 \n",
      "\n",
      "Epoch 2350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.347037 \n",
      "\n",
      "Epoch 2400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.348129 \n",
      "\n",
      "Epoch 2450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.349065 \n",
      "\n",
      "Epoch 2500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.351643 \n",
      "\n",
      "Epoch 2550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.353846 \n",
      "\n",
      "Epoch 2600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.355381 \n",
      "\n",
      "Epoch 2650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.357844 \n",
      "\n",
      "Epoch 2700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.359370 \n",
      "\n",
      "Epoch 2750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.360325 \n",
      "\n",
      "Epoch 2800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.362398 \n",
      "\n",
      "Epoch 2850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.364178 \n",
      "\n",
      "Epoch 2900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.365910 \n",
      "\n",
      "Epoch 2950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.367173 \n",
      "\n",
      "Epoch 3000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.366528 \n",
      "\n",
      "Epoch 3050\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.365971 \n",
      "\n",
      "Epoch 3100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.365363 \n",
      "\n",
      "Epoch 3150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.365050 \n",
      "\n",
      "Epoch 3200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.364181 \n",
      "\n",
      "Epoch 3250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.364217 \n",
      "\n",
      "Epoch 3300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.364207 \n",
      "\n",
      "Epoch 3350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.364979 \n",
      "\n",
      "Epoch 3400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.364340 \n",
      "\n",
      "Epoch 3450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.364984 \n",
      "\n",
      "Epoch 3500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.366009 \n",
      "\n",
      "Epoch 3550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.368653 \n",
      "\n",
      "Epoch 3600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.371611 \n",
      "\n",
      "Epoch 3650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.375649 \n",
      "\n",
      "Epoch 3700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.380548 \n",
      "\n",
      "Epoch 3750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.385474 \n",
      "\n",
      "Epoch 3800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.389940 \n",
      "\n",
      "Epoch 3850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.395980 \n",
      "\n",
      "Epoch 3900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.402527 \n",
      "\n",
      "Epoch 3950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.409752 \n",
      "\n",
      "Epoch 4000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.416189 \n",
      "\n",
      "Epoch 4050\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.422617 \n",
      "\n",
      "Epoch 4100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.429273 \n",
      "\n",
      "Epoch 4150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.436217 \n",
      "\n",
      "Epoch 4200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.443317 \n",
      "\n",
      "Epoch 4250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.450723 \n",
      "\n",
      "Epoch 4300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.458435 \n",
      "\n",
      "Epoch 4350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.466130 \n",
      "\n",
      "Epoch 4400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.471929 \n",
      "\n",
      "Epoch 4450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.479609 \n",
      "\n",
      "Epoch 4500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.494276 \n",
      "\n",
      "Epoch 4550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.504504 \n",
      "\n",
      "Epoch 4600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.513954 \n",
      "\n",
      "Epoch 4650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.523454 \n",
      "\n",
      "Epoch 4700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.531241 \n",
      "\n",
      "Epoch 4750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.539506 \n",
      "\n",
      "Epoch 4800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.547708 \n",
      "\n",
      "Epoch 4850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.556798 \n",
      "\n",
      "Epoch 4900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.566045 \n",
      "\n",
      "Epoch 4950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.574990 \n",
      "\n",
      "Epoch 5000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.585144 \n",
      "\n",
      "Epoch 5050\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.595144 \n",
      "\n",
      "Epoch 5100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.605795 \n",
      "\n",
      "Epoch 5150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.616347 \n",
      "\n",
      "Epoch 5200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.625631 \n",
      "\n",
      "Epoch 5250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.636040 \n",
      "\n",
      "Epoch 5300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.645814 \n",
      "\n",
      "Epoch 5350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.656363 \n",
      "\n",
      "Epoch 5400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.667281 \n",
      "\n",
      "Epoch 5450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.677798 \n",
      "\n",
      "Epoch 5500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.689703 \n",
      "\n",
      "Epoch 5550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.701239 \n",
      "\n",
      "Epoch 5600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.712006 \n",
      "\n",
      "Epoch 5650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.723873 \n",
      "\n",
      "Epoch 5700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.736762 \n",
      "\n",
      "Epoch 5750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.748056 \n",
      "\n",
      "Epoch 5800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.760210 \n",
      "\n",
      "Epoch 5850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.772109 \n",
      "\n",
      "Epoch 5900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.784982 \n",
      "\n",
      "Epoch 5950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.797243 \n",
      "\n",
      "Epoch 6000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.809537 \n",
      "\n",
      "Epoch 6050\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.822121 \n",
      "\n",
      "Epoch 6100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.834033 \n",
      "\n",
      "Epoch 6150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.846656 \n",
      "\n",
      "Epoch 6200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.860056 \n",
      "\n",
      "Epoch 6250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.872875 \n",
      "\n",
      "Epoch 6300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.884934 \n",
      "\n",
      "Epoch 6350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.897131 \n",
      "\n",
      "Epoch 6400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.908911 \n",
      "\n",
      "Epoch 6450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.920776 \n",
      "\n",
      "Epoch 6500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.933554 \n",
      "\n",
      "Epoch 6550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.945748 \n",
      "\n",
      "Epoch 6600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.956936 \n",
      "\n",
      "Epoch 6650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.967912 \n",
      "\n",
      "Epoch 6700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.979238 \n",
      "\n",
      "Epoch 6750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.990778 \n",
      "\n",
      "Epoch 6800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 1.001747 \n",
      "\n",
      "Epoch 6850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 1.014208 \n",
      "\n",
      "Epoch 6900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 1.024681 \n",
      "\n",
      "Epoch 6950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 1.037320 \n",
      "\n",
      "Epoch 7000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 1.047811 \n",
      "\n",
      "Epoch 7050\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 1.059567 \n",
      "\n",
      "Epoch 7100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 1.071188 \n",
      "\n",
      "Epoch 7150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 1.082735 \n",
      "\n",
      "Epoch 7200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 1.094013 \n",
      "\n",
      "Epoch 7250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 1.106012 \n",
      "\n",
      "Epoch 7300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 1.117349 \n",
      "\n",
      "Epoch 7350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 1.128835 \n",
      "\n",
      "Epoch 7400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 1.139782 \n",
      "\n",
      "Epoch 7450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 1.151529 \n",
      "\n",
      "Epoch 7500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 1.161636 \n",
      "\n",
      "Epoch 7550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 1.172872 \n",
      "\n",
      "Epoch 7600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 1.183670 \n",
      "\n",
      "Epoch 7650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 1.194077 \n",
      "\n",
      "Epoch 7700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 1.204605 \n",
      "\n",
      "Epoch 7750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 1.215190 \n",
      "\n",
      "Epoch 7800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 1.225227 \n",
      "\n",
      "Epoch 7850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 1.235908 \n",
      "\n",
      "Epoch 7900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 1.247006 \n",
      "\n",
      "Epoch 7950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 1.256441 \n",
      "\n",
      "Epoch 8000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 1.267073 \n",
      "\n",
      "Epoch 8050\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 1.276727 \n",
      "\n",
      "Epoch 8100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 1.286224 \n",
      "\n",
      "Epoch 8150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 1.295201 \n",
      "\n",
      "Epoch 8200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 1.304617 \n",
      "\n",
      "Epoch 8250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 1.314164 \n",
      "\n",
      "Epoch 8300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 1.323292 \n",
      "\n",
      "Epoch 8350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 1.332201 \n",
      "\n",
      "Epoch 8400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 1.341281 \n",
      "\n",
      "Epoch 8450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 1.350923 \n",
      "\n",
      "Epoch 8500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 1.358952 \n",
      "\n",
      "Epoch 8550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 1.368101 \n",
      "\n",
      "Epoch 8600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 1.376598 \n",
      "\n",
      "Epoch 8650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 1.385435 \n",
      "\n",
      "Epoch 8700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 1.393984 \n",
      "\n",
      "Epoch 8750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 1.403074 \n",
      "\n",
      "Epoch 8800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 1.410942 \n",
      "\n",
      "Epoch 8850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 1.418779 \n",
      "\n",
      "Epoch 8900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 1.427893 \n",
      "\n",
      "Epoch 8950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 1.435027 \n",
      "\n",
      "Epoch 9000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 1.443058 \n",
      "\n",
      "Epoch 9050\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 1.451479 \n",
      "\n",
      "Epoch 9100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 1.458680 \n",
      "\n",
      "Epoch 9150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 1.466368 \n",
      "\n",
      "Epoch 9200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 1.474240 \n",
      "\n",
      "Epoch 9250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 1.482387 \n",
      "\n",
      "Epoch 9300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 1.490505 \n",
      "\n",
      "Epoch 9350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 1.497633 \n",
      "\n",
      "Epoch 9400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 1.504452 \n",
      "\n",
      "Epoch 9450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 1.511684 \n",
      "\n",
      "Epoch 9500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 1.517835 \n",
      "\n",
      "Epoch 9550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 1.524795 \n",
      "\n",
      "Epoch 9600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 1.530430 \n",
      "\n",
      "Epoch 9650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 1.537235 \n",
      "\n",
      "Epoch 9700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 1.543180 \n",
      "\n",
      "Epoch 9750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 1.549292 \n",
      "\n",
      "Epoch 9800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 1.556042 \n",
      "\n",
      "Epoch 9850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 1.561499 \n",
      "\n",
      "Epoch 9900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 1.566860 \n",
      "\n",
      "Epoch 9950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 1.572246 \n",
      "\n",
      "Done:  2839.133053779602  seconds\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 1.577860 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 140\n",
    "n_outputs = 2\n",
    "n_hiddens = [\n",
    "    int(np.sqrt(np.prod(14))) * 2,\n",
    "    int(np.sqrt(np.prod(14))),\n",
    "    1000,\n",
    "]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"softplus\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.0005\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 10000\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef4e0d1e-ab48-405a-9a77-1f1c71f00eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.375153 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(validate_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d0cd0-b94e-4aa5-b6f7-837298511db0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 140\n",
    "n_outputs = 2\n",
    "n_hiddens = [\n",
    "    int(np.sqrt(np.prod(117))) * 2,\n",
    "    int(np.sqrt(np.prod(117))),\n",
    "    1000,\n",
    "]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"softplus\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.0005\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 10000\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8376040-6478-433e-b7a6-8789e4675128",
   "metadata": {},
   "source": [
    "The results achieved here were good but I realized I had made a mistake. I had normalized the data before I split it into train, validate, and test. I needed to lump the data and then split the data and then normalize each segment to ensure that I wasn't introducing future information into my \n",
    "validate and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "75a5bd5f-f1bd-40c9-b507-58b61793c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_samples_in_chunk = 10\n",
    "# root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# fp_csv_file = \"EEG_Eye_State_Lumped_\" + str(num_samples_in_chunk) + \".csv\"\n",
    "# train_fraction = 0.8\n",
    "# validate_fraction = 0.1\n",
    "# X_pandas = pd.read_csv(\n",
    "#     os.path.join(root_directory, fp_csv_file),\n",
    "#     delimiter=\",\",\n",
    "# ).dropna(axis=0)\n",
    "\n",
    "# X.to_csv(\n",
    "#     \"EEG_Eye_State_Lumped_\" + str(num_samples_in_chunk) + \"_Normalized.csv\",\n",
    "#     encoding=\"utf-8\",\n",
    "#     index_label=False,\n",
    "#     index=False,\n",
    "#     quoting=csv.QUOTE_NONNUMERIC,\n",
    "# )\n",
    "\n",
    "# X = X_pandas.to_numpy()\n",
    "# n_samples = X.shape[0]\n",
    "# rows = np.arange(n_samples)\n",
    "# np.random.shuffle(rows)\n",
    "\n",
    "# n_train = round(n_samples * train_fraction)\n",
    "# n_validate = round(n_samples * validate_fraction)\n",
    "\n",
    "# Xtrain = X[rows[:n_train], :]\n",
    "# Xvalidate = X[rows[n_train : n_train + n_validate], :]\n",
    "# Xtest = X[rows[n_train + n_validate :], :]\n",
    "# Xtrain = pd.DataFrame(Xtrain, columns=X_pandas.columns)\n",
    "# Xvalidate = pd.DataFrame(Xvalidate, columns=X_pandas.columns)\n",
    "# Xtest = pd.DataFrame(Xtest, columns=X_pandas.columns)\n",
    "# Xtrain = Xtrain.astype({\"eyeDetection\": int})\n",
    "# Xvalidate = Xvalidate.astype({\"eyeDetection\": int})\n",
    "# Xtest = Xtest.astype({\"eyeDetection\": int})\n",
    "\n",
    "# new = {}\n",
    "# for i in Xtrain:\n",
    "#     if i != \"eyeDetection\":\n",
    "#         new[i] = (Xtrain[i] - Xtrain[i].median()) / Xtrain[i].std()\n",
    "#     else:\n",
    "#         new[i] = Xtrain[i]\n",
    "\n",
    "# Xtrain = pd.DataFrame(new)\n",
    "\n",
    "# new = {}\n",
    "# for i in Xvalidate:\n",
    "#     if i != \"eyeDetection\":\n",
    "#         new[i] = (Xvalidate[i] - Xvalidate[i].median()) / Xvalidate[i].std()\n",
    "#     else:\n",
    "#         new[i] = Xvalidate[i]\n",
    "\n",
    "# Xvalidate = pd.DataFrame(new)\n",
    "\n",
    "# new = {}\n",
    "# for i in Xtest:\n",
    "#     if i != \"eyeDetection\":\n",
    "#         new[i] = (Xtest[i] - Xtest[i].median()) / Xtest[i].std()\n",
    "#     else:\n",
    "#         new[i] = Xtest[i]\n",
    "\n",
    "# Xtest = pd.DataFrame(new)\n",
    "\n",
    "# Xtrain.to_csv(\n",
    "#     \"EEG_Eye_State_Lumped_\" + str(num_samples_in_chunk) + \"_Normalized_Train.csv\",\n",
    "#     encoding=\"utf-8\",\n",
    "#     index_label=False,\n",
    "#     index=False,\n",
    "#     quoting=csv.QUOTE_NONNUMERIC,\n",
    "# )\n",
    "# Xvalidate.to_csv(\n",
    "#     \"EEG_Eye_State_Lumped_\" + str(num_samples_in_chunk) + \"_Normalized_Validate.csv\",\n",
    "#     encoding=\"utf-8\",\n",
    "#     index_label=False,\n",
    "#     index=False,\n",
    "#     quoting=csv.QUOTE_NONNUMERIC,\n",
    "# )\n",
    "# Xtest.to_csv(\n",
    "#     \"EEG_Eye_State_Lumped_\" + str(num_samples_in_chunk) + \"_Normalized_Test.csv\",\n",
    "#     encoding=\"utf-8\",\n",
    "#     index_label=False,\n",
    "#     index=False,\n",
    "#     quoting=csv.QUOTE_NONNUMERIC,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54ac75fa-a925-435b-a965-656c70bb1995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeros:  621\n",
      "Ones:  509\n",
      "Zeros:  70\n",
      "Ones:  72\n",
      "Zeros:  85\n",
      "Ones:  57\n",
      "AF3_0           float64\n",
      "F7_0            float64\n",
      "F3_0            float64\n",
      "FC5_0           float64\n",
      "T7_0            float64\n",
      "                 ...   \n",
      "FC6_9           float64\n",
      "F4_9            float64\n",
      "F8_9            float64\n",
      "AF4_9           float64\n",
      "eyeDetection      int64\n",
      "Length: 141, dtype: object\n",
      "AF3_0           float64\n",
      "F7_0            float64\n",
      "F3_0            float64\n",
      "FC5_0           float64\n",
      "T7_0            float64\n",
      "                 ...   \n",
      "FC6_9           float64\n",
      "F4_9            float64\n",
      "F8_9            float64\n",
      "AF4_9           float64\n",
      "eyeDetection      int64\n",
      "Length: 141, dtype: object\n",
      "AF3_0           float64\n",
      "F7_0            float64\n",
      "F3_0            float64\n",
      "FC5_0           float64\n",
      "T7_0            float64\n",
      "                 ...   \n",
      "FC6_9           float64\n",
      "F4_9            float64\n",
      "F8_9            float64\n",
      "AF4_9           float64\n",
      "eyeDetection      int64\n",
      "Length: 141, dtype: object\n"
     ]
    }
   ],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "\n",
    "train_file = \"EEG_Eye_State_Lumped_10_Normalized_Train.csv\"\n",
    "validate_file = \"EEG_Eye_State_Lumped_10_Normalized_Validate.csv\"\n",
    "test_file = \"EEG_Eye_State_Lumped_10_Normalized_Test.csv\"\n",
    "\n",
    "# train_file = \"Dummy_Train.csv\"\n",
    "# validate_file = \"Dummy_Validate.csv\"\n",
    "# test_file = \"Dummy_Test.csv\"\n",
    "count_ones_and_zeros(train_file)\n",
    "count_ones_and_zeros(validate_file)\n",
    "count_ones_and_zeros(test_file)\n",
    "train_dataset = fpnn.FinalProjectEEGDataset(train_file, root_directory)\n",
    "validate_dataset = fpnn.FinalProjectEEGDataset(validate_file, root_directory)\n",
    "test_dataset = fpnn.FinalProjectEEGDataset(test_file, root_directory)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "887ecdd5-b789-40c5-8a34-fcf430af154e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=140, out_features=6, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=6, out_features=3, bias=True)\n",
      "    (Softplus1): Softplus(beta=1, threshold=20)\n",
      "    (Linear2): Linear(in_features=3, out_features=1000, bias=True)\n",
      "    (Softplus2): Softplus(beta=1, threshold=20)\n",
      "    (Linear3): Linear(in_features=1000, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 49.6%, Avg loss: 0.701164 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 51.1%, Avg loss: 0.698166 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 0.690155 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Avg loss: 0.677860 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 0.662290 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.645834 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.630336 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.617198 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.605842 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.594254 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.580270 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.564877 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.548748 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 72.3%, Avg loss: 0.533642 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.515692 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.500122 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.484278 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.471342 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.459053 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.447805 \n",
      "\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.434904 \n",
      "\n",
      "Epoch 1050\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.422204 \n",
      "\n",
      "Epoch 1100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.416277 \n",
      "\n",
      "Epoch 1150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.411058 \n",
      "\n",
      "Epoch 1200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.405844 \n",
      "\n",
      "Epoch 1250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.399432 \n",
      "\n",
      "Epoch 1300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.392085 \n",
      "\n",
      "Epoch 1350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.384566 \n",
      "\n",
      "Epoch 1400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.378621 \n",
      "\n",
      "Epoch 1450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.370141 \n",
      "\n",
      "Epoch 1500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.363282 \n",
      "\n",
      "Epoch 1550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.357857 \n",
      "\n",
      "Epoch 1600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.353310 \n",
      "\n",
      "Epoch 1650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.350419 \n",
      "\n",
      "Epoch 1700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.347072 \n",
      "\n",
      "Epoch 1750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.344270 \n",
      "\n",
      "Epoch 1800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.342666 \n",
      "\n",
      "Epoch 1850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.342659 \n",
      "\n",
      "Epoch 1900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.340833 \n",
      "\n",
      "Epoch 1950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.340039 \n",
      "\n",
      "Epoch 2000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.339744 \n",
      "\n",
      "Epoch 2050\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.340332 \n",
      "\n",
      "Epoch 2100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.341717 \n",
      "\n",
      "Epoch 2150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.342498 \n",
      "\n",
      "Epoch 2200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.344266 \n",
      "\n",
      "Epoch 2250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.347255 \n",
      "\n",
      "Epoch 2300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.349975 \n",
      "\n",
      "Epoch 2350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.352814 \n",
      "\n",
      "Epoch 2400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.356047 \n",
      "\n",
      "Epoch 2450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.359699 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\RANDAL~1\\AppData\\Local\\Temp/ipykernel_5804/3515030732.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m train_for_epochs(\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m )\n",
      "\u001b[1;32mC:\\Users\\RANDAL~1\\AppData\\Local\\Temp/ipykernel_5804/704544453.py\u001b[0m in \u001b[0;36mtrain_for_epochs\u001b[1;34m(epochs, train_dataloader, _validate_dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m50\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {t}\\n-------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\RANDAL~1\\AppData\\Local\\Temp/ipykernel_5804/851795240.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# first = True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\fp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\fp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\fp\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda3\\envs\\fp\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\fp\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\fp\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# scalars\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\fp\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# scalars\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 140\n",
    "n_outputs = 2\n",
    "n_hiddens = [\n",
    "    int(np.sqrt(np.prod(14))) * 2,\n",
    "    int(np.sqrt(np.prod(14))),\n",
    "    1000,\n",
    "]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"softplus\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.0005\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 1000\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "956383d8-2907-4b41-ad83-a0fc07f6988d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=140, out_features=20, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (Softplus1): Softplus(beta=1, threshold=20)\n",
      "    (Linear2): Linear(in_features=10, out_features=1000, bias=True)\n",
      "    (Softplus2): Softplus(beta=1, threshold=20)\n",
      "    (Linear3): Linear(in_features=1000, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 49.6%, Avg loss: 0.693468 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.640390 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.536710 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.405395 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.319825 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.292438 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.301074 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.331564 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.377041 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.422997 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.459650 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.490919 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.521337 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.547662 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.571782 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.592336 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.613110 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.629852 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.643970 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.657435 \n",
      "\n",
      "Done:  290.6734154224396  seconds\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.668920 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 140\n",
    "n_outputs = 2\n",
    "n_hiddens = [\n",
    "    int(np.sqrt(np.prod(117))) * 2,\n",
    "    int(np.sqrt(np.prod(117))),\n",
    "    1000,\n",
    "]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"softplus\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.005\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 1000\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0a62f255-b8d1-4292-8c1d-5ba7633cb49e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=140, out_features=20, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (ReLU1): ReLU()\n",
      "    (Linear2): Linear(in_features=10, out_features=1000, bias=True)\n",
      "    (ReLU2): ReLU()\n",
      "    (Linear3): Linear(in_features=1000, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 49.6%, Avg loss: 0.701064 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.573757 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.394340 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.316205 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.304453 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.328360 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.348512 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.361915 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.376284 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.390867 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.407563 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.417167 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.425909 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.434410 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.444350 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.455730 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.462138 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.474518 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.474697 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.481126 \n",
      "\n",
      "Done:  283.43296337127686  seconds\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.488598 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 140\n",
    "n_outputs = 2\n",
    "n_hiddens = [\n",
    "    int(np.sqrt(np.prod(117))) * 2,\n",
    "    int(np.sqrt(np.prod(117))),\n",
    "    1000,\n",
    "]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"relu\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.005\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 1000\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5f396ac1-fe74-4de9-a742-ae6b2df378ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=140, out_features=20, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (Softplus1): Softplus(beta=1, threshold=20)\n",
      "    (Linear2): Linear(in_features=10, out_features=1000, bias=True)\n",
      "    (Softplus2): Softplus(beta=1, threshold=20)\n",
      "    (Linear3): Linear(in_features=1000, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 49.6%, Avg loss: 1.033051 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.376740 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.583964 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.650910 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.698233 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.737749 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.773527 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.806819 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.837340 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.866569 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.895060 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.923010 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.950702 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.978792 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 1.006450 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 1.035130 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 1.062968 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 1.090946 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 1.117020 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 1.136981 \n",
      "\n",
      "Done:  300.24270820617676  seconds\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 1.157106 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 140\n",
    "n_outputs = 2\n",
    "n_hiddens = [\n",
    "    int(np.sqrt(np.prod(117))) * 2,\n",
    "    int(np.sqrt(np.prod(117))),\n",
    "    1000,\n",
    "]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"softplus\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 1000\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "01f52146-37af-4ea5-b913-52ae310a1c3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=140, out_features=20, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (Softplus1): Softplus(beta=1, threshold=20)\n",
      "    (Linear2): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (Softplus2): Softplus(beta=1, threshold=20)\n",
      "    (Linear3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Avg loss: 0.647764 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.251493 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.476307 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.537607 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.577488 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.608657 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.636173 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.661523 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.685133 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.706858 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.727732 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.748524 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.769409 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.790092 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.810259 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.830134 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.850541 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.868664 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.889703 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.904920 \n",
      "\n",
      "Done:  280.8695442676544  seconds\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.918234 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 140\n",
    "n_outputs = 2\n",
    "n_hiddens = [\n",
    "    int(np.sqrt(np.prod(117))) * 2,\n",
    "    int(np.sqrt(np.prod(117))),\n",
    "    10,\n",
    "]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"softplus\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 1000\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f829b66f-f553-42ab-a4d8-bb2501ade5e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=140, out_features=20, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (Softplus1): Softplus(beta=1, threshold=20)\n",
      "    (Linear2): Linear(in_features=10, out_features=100, bias=True)\n",
      "    (Softplus2): Softplus(beta=1, threshold=20)\n",
      "    (Linear3): Linear(in_features=100, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 0.640629 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.471098 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.614597 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.700515 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.765607 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.821153 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.871476 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.918313 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.962793 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 1.006300 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 1.049077 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 1.092895 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 1.134720 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 1.176878 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 1.217680 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 1.258502 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 1.296349 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 1.326659 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 1.366210 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 1.391897 \n",
      "\n",
      "Done:  284.5982406139374  seconds\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 1.420714 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 140\n",
    "n_outputs = 2\n",
    "n_hiddens = [\n",
    "    int(np.sqrt(np.prod(117))) * 2,\n",
    "    int(np.sqrt(np.prod(117))),\n",
    "    100,\n",
    "]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"softplus\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 1000\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "27ce014c-a936-4c41-9616-9e9fbf90f2ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=140, out_features=20, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (Softplus1): Softplus(beta=1, threshold=20)\n",
      "    (Linear2): Linear(in_features=10, out_features=1000, bias=True)\n",
      "    (Softplus2): Softplus(beta=1, threshold=20)\n",
      "    (Linear3): Linear(in_features=1000, out_features=10, bias=True)\n",
      "    (Softplus3): Softplus(beta=1, threshold=20)\n",
      "    (Linear4): Linear(in_features=10, out_features=5, bias=True)\n",
      "    (Softplus4): Softplus(beta=1, threshold=20)\n",
      "    (Linear5): Linear(in_features=5, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 49.6%, Avg loss: 0.706832 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.355756 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.537157 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.576466 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.605220 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.628945 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.649520 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.667442 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.683184 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.696640 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.708361 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.719369 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.732000 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.741765 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.752409 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.763059 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.773450 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.783694 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.789470 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.795838 \n",
      "\n",
      "Done:  303.2899913787842  seconds\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.798623 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 140\n",
    "n_outputs = 2\n",
    "n_hiddens = [int(np.sqrt(np.prod(117))) * 2, int(np.sqrt(np.prod(117))), 1000, 10, 5]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"softplus\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 1000\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8e6ddb-7947-4320-bb37-6b0adfd562c6",
   "metadata": {},
   "source": [
    "I achieved pretty good results using this method. I got as high as 90.1% on validate using a fairly small number of units in one particular run. I decided to use this run as my final run against my test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9314fdc9-8d5b-45b5-aad8-33b775163d73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_stack): Sequential(\n",
      "    (Linear0): Linear(in_features=140, out_features=20, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (Linear1): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (Softplus1): Softplus(beta=1, threshold=20)\n",
      "    (Linear2): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (Softplus2): Softplus(beta=1, threshold=20)\n",
      "    (Linear3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.658107 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.562668 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.712021 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.792777 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.853813 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.908340 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.959812 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 1.010056 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 1.057287 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 1.102989 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 1.148283 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 1.193366 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 1.238062 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 1.282534 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 1.326139 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 1.369888 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 1.413373 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 1.457725 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 1.500115 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 1.542584 \n",
      "\n",
      "Done:  279.8443524837494  seconds\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 1.571700 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 140\n",
    "n_outputs = 2\n",
    "n_hiddens = [\n",
    "    int(np.sqrt(np.prod(117))) * 2,\n",
    "    int(np.sqrt(np.prod(117))),\n",
    "    10,\n",
    "]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs, \"softplus\").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 1000\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1299825-07de-4e3d-92f6-699dc03336c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 1.461865 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ed7a723-924b-4d9a-95b9-99ead60f5e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[75., 10.],\n",
      "        [ 4., 52.]])\n",
      "Percent Correct\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_e7d9a2eb_455e_11ec_bd03_00d861bee616row0_col0,#T_e7d9a2eb_455e_11ec_bd03_00d861bee616row1_col1{\n",
       "            background-color:  #08306b;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_e7d9a2eb_455e_11ec_bd03_00d861bee616row0_col1,#T_e7d9a2eb_455e_11ec_bd03_00d861bee616row1_col0{\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_e7d9a2eb_455e_11ec_bd03_00d861bee616\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >open</th>        <th class=\"col_heading level0 col1\" >closed</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_e7d9a2eb_455e_11ec_bd03_00d861bee616level0_row0\" class=\"row_heading level0 row0\" >open</th>\n",
       "                        <td id=\"T_e7d9a2eb_455e_11ec_bd03_00d861bee616row0_col0\" class=\"data row0 col0\" >88.2</td>\n",
       "                        <td id=\"T_e7d9a2eb_455e_11ec_bd03_00d861bee616row0_col1\" class=\"data row0 col1\" >11.8</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e7d9a2eb_455e_11ec_bd03_00d861bee616level0_row1\" class=\"row_heading level0 row1\" >closed</th>\n",
       "                        <td id=\"T_e7d9a2eb_455e_11ec_bd03_00d861bee616row1_col0\" class=\"data row1 col0\" >7.1</td>\n",
       "                        <td id=\"T_e7d9a2eb_455e_11ec_bd03_00d861bee616row1_col1\" class=\"data row1 col1\" >92.9</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1a9d7ea7640>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classes = 2\n",
    "\n",
    "confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, classes) in enumerate(test_dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        classes = classes.to(device)\n",
    "        outputs = model(inputs.float())\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        for t, p in zip(classes.view(-1), preds.view(-1)):\n",
    "            confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "print(confusion_matrix)\n",
    "new_confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "for i, row in enumerate(confusion_matrix):\n",
    "    sum = row.sum()\n",
    "    new_confusion_matrix[i, 0] = 100 * row[0] / sum\n",
    "    new_confusion_matrix[i, 1] = 100 * row[1] / sum\n",
    "conf_matrix = pd.DataFrame(\n",
    "    new_confusion_matrix.numpy(), index=[\"open\", \"closed\"], columns=[\"open\", \"closed\"]\n",
    ")\n",
    "# cf.style.background_gradient(cmap='Blues').format(\"{:.1f} %\")\n",
    "print(\"Percent Correct\")\n",
    "conf_matrix.style.background_gradient(cmap=\"Blues\").format(\"{:.1f}\")\n",
    "# print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b33b7d-3bf7-4cef-a849-88d72190a5be",
   "metadata": {},
   "source": [
    "The results I achieved were pretty good. Overall on the test data I had 90.1% accuracy.When I broke it out as open versus closed my model was correctly identifying cases of open eyes 88.2% of the time and closed eyes 92.9% of the time. This was a fairly even split and showed that the model was not using a strategy biased in favor of one State versus the other. I was pretty satisfied with these results at classifying using a standard classification neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d965164-538d-472f-a4a8-39f586e46870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp",
   "language": "python",
   "name": "fp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
