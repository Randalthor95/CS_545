{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Eye-State Based on EEG Data #\n",
    "### Traditional Classification Neural Networks vs convolutional Neural Networks ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*by Anthony Feudale, October 27, 2021*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguably one of the most exciting aspects of machine learning is its potential use for taking over certain types of cognitive work. Neural networks have made highly cognitive tasks, such as identifying objects in images, computational tasks as opposed to tasks which previously only humans could do. The accuracy and speed of such methods has proven to be game changing in some cases.</br>\n",
    "\n",
    "A large part of what people do on a day-to-day basis in today's economy is cognitive work. While machine learning is highly limited in its scope (machine learning algorithms can only handle narrowly constrained tasks) it can serve to amplify the amount of work people are able to accomplish. No potential usage of machine learning better highlights this then Brain-Computer Interfaces or BCIs. Currently the ability of people to work with machines is mediated through devices like keyboards, mice, and touchscreens. All of these rely on direct physical manipulation to interface with a computer. Not everybody has full use of their limbs and these people have significant difficulty in using computers. New technologies like  speech-to-text powered by machine learning have made large gains in  ameliorating this problem. Additionally, traditional computer interfaces limit the speed with which people can interact with their devices. You can only code or write only as fast as you can type, which can be a severe limitation for some individuals. Additionally, increasing the speed with which we interface with computers could serve to spark a quantum leap in humanity's productive capacity similar to the explosion in human knowledge brought about by the proliferation of knowledge via the printing press or the Internet [T. Urban, 2017]. Direct Brain-Computer Interfaces could help close this speed gap, and light the match for another such explosion.</br>\n",
    "\n",
    "One Potential Avenue for BCIs is the EEG or Electroencephalogram [Johns Hopkins, 2021]. Via electrical sensors placed on the scalp this device senses changes in electrical impulses coming from the brain. Unfortunately, such signals are quite noisy and can be hard to interpret. This is where machine learning comes in. Using machine learning techniques it is possible to utilize EEG signals to detect when a user wants to do a particular action. For example, closing one's eyes, and the resulting changes in EEG activity, could signal that a user wants to \"click\" as if they were clicking the mouse button on a physical mouse. </br>\n",
    "\n",
    "I found this idea intriguing and so decided to investigate it as part of my final project. I looked through various EEG data sets. A fair number of such sets exist. Many deal with motor imagery, emotion recognition, various potentials, and movement [Meagmohit, 2020]. After reviewing several data sets I eventually settled on one from the UCI Machine Learning Repository regarding eye-state [O. Roesler, 2013]. I did so because I could envision eye-state being used as an early means of interfacing with a computer and because the data was well curated for the learning task I had planned. I intend to utilize both a traditional classification neural network as well as a convolutional neural network to attempt to determine eye-state from this EEG data. I will utilize PyTorch to do so [A. Paszke, S. Gross, S. Chantala, and G. Chanan, 2021]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset I will be working with is from the UCI Machine Learning Repository, and is in regard to eye-state (open or closed) [O. Roesler, 2013]. It is one unbroken time series. 14 EEG channels were captured over the course of 117 seconds for a total of 14,980 captured instances. For each instance, eye-state was captured via a camera and later manually added to the data set as  a 15th column where '0' indicates the eye-open state and 1' indicates the eye-closed state. The data file is in .arff format. I will convert this into .csv and then create new .csv files for train, validation, and test data by random selection from the main dataset. </br>\n",
    "\n",
    "I will utilize PyTorch to train a model to predict eye-state based on the 14 EEG channels. I intend to utilize both a traditional classification neural network. I will vary things like layer structure, learning rates, methods, and training epochs to try and find an optimal network. </br>\n",
    "\n",
    "I had thought it would be interesting to use a convolutional neural network  over time as well. However, I am not sure it will be possible with the data I have as each individual moment has a distinct eye-state associated with it. If I were to convolve over time it would be possible that a given kernel could have more than one eye-state classification associated with it as each entry has a specific discrete eye-state. I could split my data into segments where each segment contained only eyes open or eyes closed samples. I could then convolve over time in each segment. However two samples in a given segment that are adjacent to each other in the segment would not be guaranteed to have been adjacent in time in the original data set, they would only be guaranteed to be ordered in time. The signal information would have gaps in its history. This seems to defeat the larger feature finding purpose of a CNN. Since the segments are broken up in time, features of the EEG signal would also be broken up in time. Additionally, in the wild in real time it would be more or less impossible to use such a CNN as the data that would be captured from an EEG device would not necessarily be broken up into segments where eye state was guaranteed to be the same across a given segment length. For example, if our segment length was six samples taken every 100 milliseconds, there is no guarantee that during a given 600 ms string the subject would always have their eyes open or closed. Thus the captured state would be ambiguous making the task of classification not well defined.</br>\n",
    "\n",
    "I don't see a good way to untangle these issues in a way that would result in a meaningful outcome for distinguishing eye-state using a CNN convolved over time. The only way around the problem that I can think of would be to redefine the task as simply detecting the presence of one of the eye-states (say eyes open) in a given sample. So instead of trying to classify eyes open versus eyes closed we would instead be trying to classify the presence of eyes open in a given segment. Two separate neural networks would need to be trained for presence of eyes open versus presence of eyes closed and the data would need to be significantly modified to fit each problem separately. The regular classification neural network would also need to be modified to take in segments of data of the same size and would need to be set up to classify for presence as well, in order to ensure that the two separate methods were solving the same problem and could be compared apples to apples. The regular classification neural network would also need to be split in two for each case. I personally think I'm more interested in the discrete classification (i.e. is this sample eyes open or eyes closed) as originally defined. </br>\n",
    "\n",
    "## TODO ##\n",
    "echo state networks,\n",
    "resevoir computing,\n",
    "CNN split into time segments and throw out chunks that are ambiguous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Milestones</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date | Milestone | Description \n",
    "| :- | :- | :- |\n",
    "| 11/5 | Project Proposal | Complete research on topic, methods, and data. Hand in final project proposal\n",
    "| 11/12 | Complete Preliminary Coding | Implement initial code for training of data\n",
    "| 11/19 | Complete Final Coding and Training Experiments | Work through training data with various methods. Find what is most accurate without over-training\n",
    "| 11/26 | Complete Preliminary Report | Complete preliminary report including data from training\n",
    "| 12/12 | Final Project Complete | Hand in final project, including finalized report, data, and project files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I expect the traditional classification neural network to have trouble accurately classifying eye-state. Given that I only have about 15,000 samples I expect it will take a significant amount of training to get the traditional classification neural network to a decent accuracy on train and validation data. I fully expect that this will not generalize particularly well to the test data as I expect the traditional classification neural network will need to over-fit in order to predict the training data well. </br>\n",
    "\n",
    "I imagine that I will need to utilize techniques such as pre-training in order to get this model to learn well. I may need to dig into the literature as I go in order to get some ideas about how to make this train well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I expect that I will learn a great deal about the intricacies in setting up networks. I expect I will also get an object lesson in the problems that often arise when trying to get a neural network to learn from a noisy data set. I will be interested to see if the relatively small size of my data set has a deleterious impact on each neural network's ability to learn. I hope I learn that 15,000 samples can be enough, but we will see.</br>\n",
    "\n",
    "While many aspects of this project will be challenging, I expect the most challenging piece will be identifying the parameters which best allow the neural networks to correctly classify eye-state. I expect this to take many attempts and involve a lot of trial and error. Second place in terms of challenge will likely be getting the neural network set up to run at all initially. This will probably take some time, but it will likely be easier due to the plethora of material and tutorials on PyTorch available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [T. Urban, 2017] T. Urban, “Neuralink and the brain's magical future,” Wait But Why, 20-Apr-2017. [Online]. Available: https://waitbutwhy.com/2017/04/neuralink.html. [Accessed: 29-Oct-2021]. \n",
    "* [Johns Hopkins, 2021] Johns Hopkins, “Electroencephalogram (EEG),” Johns Hopkins Medicine, 2021. [Online]. Available: https://www.hopkinsmedicine.org/health/treatment-tests-and-therapies/electroencephalogram-eeg. [Accessed: 29-Oct-2021]. \n",
    "* [Meagmohit, 2020] Meagmohit, “Meagmohit/EEG-datasets: A list of all public EEG-datasets,” GitHub, 23-Nov-2020. [Online]. Available: https://github.com/meagmohit/EEG-Datasets. [Accessed: 29-Oct-2021]. \n",
    "* [O. Roesler, 2013] O. Roesler, “EEG eye-state Data Set,” UCI Machine Learning Repository: Eeg eye-state Data set, 10-Jun-2013. [Online]. Available: https://archive.ics.uci.edu/ml/datasets/EEG+Eye+State. [Accessed: 29-Oct-2021]. \n",
    "* [A. Paszke, S. Gross, S. Chantala, and G. Chanan, 2021] A. Paszke, S. Gross, S. Chantala, and G. Chanan, “Pytorch/Pytorch: Tensors and dynamic neural networks in python with strong GPU acceleration,” GitHub, 29-Oct-2021. [Online]. Available: https://github.com/pytorch/pytorch. [Accessed: 29-Oct-2021]. \n",
    "* F. William and F. Zhu, “CNN models for eye state classification using EEG with temporal ordering,” Proceedings of the 12th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics, pp. 1–8, Aug. 2021. doi: https://doi.org/10.1145/3459930.3471160. [Online]. Available: https://dl.acm.org/doi/abs/10.1145/3459930.3471160. [Accessed: 21-Nov-2021]. \n",
    "* K. Sabancı and M. Köklü, “The classification of Eye State by using KNN and MLP classification models according to the EEG signals,” International Journal of Intelligent Systems and Applications in Engineering, vol. 3, no. 4, p. 127, 2015. doi: http://dx.doi.org/10.18201/ijisae.75836. [Online]. Available: https://www.researchgate.net/publication/289685013_The_Classification_of_Eye_State_by_Using_kNN_and_MLP_Classification_Models_According_to_the_EEG_Signals. [Accessed: 21-Nov-2021]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T14:49:15.869360Z",
     "start_time": "2021-10-27T14:49:15.860735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count for file Feudale_Final_Project_Report.ipynb is 1845\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import nbformat\n",
    "import glob\n",
    "\n",
    "nbfile = glob.glob(\"Feudale_Final_Project_Report.ipynb\")\n",
    "if len(nbfile) > 1:\n",
    "    print(\"More than one ipynb file. Using the first one.  nbfile=\", nbfile)\n",
    "with io.open(nbfile[0], \"r\", encoding=\"utf-8\") as f:\n",
    "    nb = nbformat.read(f, nbformat.NO_CONVERT)\n",
    "word_count = 0\n",
    "for cell in nb.cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell[\"source\"].replace(\"#\", \"\").lstrip().split(\" \"))\n",
    "print(\"Word count for file\", nbfile[0], \"is\", word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp",
   "language": "python",
   "name": "fp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
