{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Eye State Based on EEG Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*by Anthony Feudale, October 27, 2021*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguably one of the most exciting aspects of machine learning is its potential use for taking over certain types of cognitive work. Convolutional Neural Networks have made things highly cognitive tasks, such as identifying objects in images, computational tasks as opposed to tasks which previously only humans could do. The accuracy and speed of such methods has proven to be game changing in some cases.</br>\n",
    "\n",
    "A large part of what people do on a day-to-day basis in today's economy is cognitive work. While machine learning is highly limited in its scope (machine learning algorithms can only handle narrowly constrained tasks) it can serve to amplify the amount of work people are able to accomplish. No potential usage of machine learning better highlights this then Brain-Computer Interfaces or BCIs. Currently the ability of people to work with machines is mediated through devices like keyboards, mice, and touchscreens. All of these rely on direct physical manipulation to interface with a computer. Not everybody has full use of their limbs and these people have significant difficulty in using computers. New technologies like  speech-to-text powered by machine learning have made large gains in  ameliorating this problem. Additionally, traditional computer interfaces limit the speed with which people can interact with their devices. You can only code or right as fast as you can type, which can be a severe limitation for some individuals. Additionally, increasing the speed with which we interface with computers could serve  as a quantum leap in humanity's productive capacity similar to the explosion in human knowledge brought about by the proliferation of knowledge via the printing press or the Internet [Wait But Why]. Direct Brain-Computer Interfaces could help close this speed gap, and light the match for another such explosion.</br>\n",
    "\n",
    "One Potential Avenue for BCIs is the EEG or Electroencephalogram [Hopkins]. Via electrical sensors placed on the scalp this device senses changes in electrical impulses coming from the brain. Unfortunately, such signals are quite noisy and can be hard to interpret. This is where machine learning comes in. Using machine learning techniques it is possible to utilize EEG signals to detect when a user wants to do a particular action. For example, closing one's eyes, and the resulting changes in EEG activity, could signal that a user wants to \"click\" as if they were clicking the mouse button on a physical mouse. </br>\n",
    "\n",
    "I found this idea intriguing and so decided to investigate it as part of my final project. I looked through various EEG data sets. A fair number of such sets exist. Many deal with motor imagery, emotion recognition, various potentials, and movement [GitHub]. After reviewing several data sets I eventually settled on one from the UCI Machine Learning Repository regarding eye state [UCI].  I intend to utilize both a traditional classification Neural Network as well as a Convolutional Neural Network to attempt to determine Eye State from this EEG data. I will utilize PyTorch to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe in some detail the algorithms and data you will use.  In your final report cite articles and github cites like this [Goodfellow, et al., 2016].\n",
    "\n",
    "In your proposal, make a table here with at least 5 milestones for your project with expected dates.\n",
    "\n",
    "REQUIRED: If this is a team project, clearly describe in detail what each team member will do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Milestones</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the proposal, summarize what you expect your results to be.  \n",
    "\n",
    "In the final report, show all results.  Intermediate results might be shown in above Methods section.  Plots, tables, whatever is needed to tell your story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your proposal, describe what you expect to learn and what you expect will be most difficult.\n",
    "\n",
    "In your project report, describe what you learned, and what was most difficult.  Summarize any surprises you encontered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Goodfellow, et al., 2016] Ian Goodfellow and Yoshua Bengio and Aaron Courville, [Deep Learning](http://www.deeplearningbook.org), MIT Press. 2014.\n",
    "* https://www.hopkinsmedicine.org/health/treatment-tests-and-therapies/electroencephalogram-eeg\n",
    "* https://github.com/meagmohit/EEG-Datasets\n",
    "* https://archive.ics.uci.edu/ml/datasets/EEG+Eye+State\n",
    "* https://github.com/pytorch/pytorch\n",
    "* https://waitbutwhy.com/2017/04/neuralink.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is now word count expectations for your proposal.\n",
    "\n",
    "Your final report should contain 2,000 to 3,000 words, times the number of team members.  Projects with two people, for example, should contain 4,000 to 6,000 words.\n",
    "\n",
    "Count words by running the following python code in your report directory.  Do this before you check-in this notebook so the word count appears as the output of the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T14:49:15.869360Z",
     "start_time": "2021-10-27T14:49:15.860735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count for file Feudale_Final_Project_Proposal.ipynb is 759\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import nbformat\n",
    "import glob\n",
    "\n",
    "nbfile = glob.glob(\"Feudale_Final_Project_Proposal.ipynb\")\n",
    "if len(nbfile) > 1:\n",
    "    print(\"More than one ipynb file. Using the first one.  nbfile=\", nbfile)\n",
    "with io.open(nbfile[0], \"r\", encoding=\"utf-8\") as f:\n",
    "    nb = nbformat.read(f, nbformat.NO_CONVERT)\n",
    "word_count = 0\n",
    "for cell in nb.cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell[\"source\"].replace(\"#\", \"\").lstrip().split(\" \"))\n",
    "print(\"Word count for file\", nbfile[0], \"is\", word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp",
   "language": "python",
   "name": "fp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
