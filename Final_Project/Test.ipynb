{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff227eb5-aa75-43cc-a156-01f052299576",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10659f7f-3867-4987-a15e-f9188774ac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import finalprojectneuralnetworks as fpnn\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db8f1e0-4c84-4ad5-907c-08eb6235def6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5  # number of samples/batch size\n",
    "l = 5  # sentence len\n",
    "d = 3  # embedding dimension\n",
    "rand_arr = torch.rand(n, l, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "244cf131-3622-45b1-b7a6-56bdc90ebc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:  Conv1d(1, 2, kernel_size=(3,), stride=(1,))\n",
      "Input:  torch.Size([1, 140])\n",
      "Input unsqueezed:  torch.Size([1, 1, 140])\n",
      "tensor([[[ 0.9818, -0.5621,  0.6250,  1.1725, -0.6883, -1.1592, -1.0358,\n",
      "           1.5947, -0.3664, -0.5379,  0.1732,  0.1164, -0.1166,  0.9325,\n",
      "           1.1040, -1.3489, -0.1892,  0.9851,  1.3359, -0.4393, -0.4778,\n",
      "           0.3597,  1.4018, -0.9472, -0.0310, -0.0696,  1.0199,  1.1905,\n",
      "           1.9292,  0.4771,  1.6502, -0.3107,  0.0039,  0.7897,  0.2355,\n",
      "          -0.0045,  1.7045, -1.5602,  0.9935,  0.9516,  2.0607, -0.0906,\n",
      "          -0.4886,  2.1094,  0.5660, -0.5469, -0.0251, -0.0609, -0.7429,\n",
      "           0.8930,  0.1181,  1.3321,  0.3881,  0.4368, -0.9118,  1.7250,\n",
      "           0.1298,  0.1148, -0.4777, -0.9992,  0.8362,  0.5460, -0.0478,\n",
      "           0.5141,  0.0694,  0.1780, -1.1466, -0.4028, -1.9904,  1.3534,\n",
      "           1.4870,  1.6848,  1.3210,  1.1253,  1.8726, -0.7751,  0.2243,\n",
      "           0.5229,  0.2561,  0.4091, -2.8773, -0.0865, -0.3889, -0.5978,\n",
      "          -0.9697, -0.8552,  0.2705,  0.0253, -0.7109,  1.8176, -0.9626,\n",
      "          -0.7548, -1.8175, -1.7917,  0.0817,  0.1636,  0.1137, -1.0761,\n",
      "           1.4525,  2.5785, -0.9285,  0.1703,  0.0262, -1.3354,  1.2064,\n",
      "           0.6156,  1.1548,  0.1027, -0.5653, -0.0618, -1.3266,  0.2644,\n",
      "           0.5585, -1.9856, -0.1312,  0.8322, -0.4970, -1.4686,  0.0660,\n",
      "           1.9081, -1.4474,  0.8140, -1.1804,  1.3249, -1.1679, -0.8442,\n",
      "          -0.2073,  1.2098, -0.6649, -0.2775,  0.3322, -2.3894, -1.4323,\n",
      "           0.7651,  1.5157, -0.1818,  1.0076, -0.6117,  0.3503,  0.2922]]])\n",
      "output.size:  torch.Size([1, 2, 138])\n",
      "output:  tensor([[[ 4.5433e-01,  8.5619e-01, -4.2383e-01, -2.7217e-02,  1.1270e+00,\n",
      "           1.9577e+00,  3.0321e-01, -2.1973e-01,  1.0030e+00,  7.7431e-01,\n",
      "           3.8727e-01,  7.7247e-01,  4.8805e-01, -7.1068e-01,  5.2466e-01,\n",
      "           1.5446e+00,  5.5918e-01, -6.1041e-01, -4.5491e-02,  1.0599e+00,\n",
      "           9.8270e-01, -4.5247e-01,  2.4619e-01,  1.0181e+00,  8.4747e-01,\n",
      "           4.4919e-01,  1.7691e-02, -7.3679e-01, -2.0235e-01, -4.5866e-01,\n",
      "          -1.3721e-01,  8.9879e-01,  2.7424e-01,  5.8159e-02,  8.5636e-01,\n",
      "          -5.5023e-01,  6.0685e-01,  1.1425e+00,  1.6397e-01, -8.1345e-01,\n",
      "          -5.5561e-01,  1.3225e+00,  4.3993e-02, -8.7114e-01,  4.9432e-01,\n",
      "           8.1872e-01,  4.0621e-01,  1.1151e+00,  5.7308e-01,  3.8587e-01,\n",
      "           2.6313e-02, -1.6710e-01, -5.0795e-02,  1.1440e+00,  3.0897e-01,\n",
      "          -3.3287e-01,  3.2049e-01,  4.4928e-01,  1.4164e+00,  8.2992e-01,\n",
      "          -1.0663e-01,  4.2666e-01,  3.7525e-01,  3.1026e-01,  1.5892e-01,\n",
      "           8.4707e-01,  8.0046e-01,  1.9248e+00,  1.3366e+00, -3.3348e-01,\n",
      "          -5.7307e-01, -5.6621e-01, -1.1854e-01, -9.8928e-01,  2.4495e-03,\n",
      "           9.7092e-01,  2.8228e-01,  2.8408e-01, -4.5812e-01,  1.5402e+00,\n",
      "           1.9224e+00,  6.1087e-01,  7.5767e-01,  1.0466e+00,  1.4608e+00,\n",
      "           8.6861e-01,  2.3079e-01,  1.2867e+00, -9.9347e-02, -1.3254e-01,\n",
      "           9.0009e-01,  1.2514e+00,  2.2308e+00,  1.4479e+00,  4.6967e-01,\n",
      "           1.5665e-01,  1.3068e+00,  1.1093e+00, -1.4875e+00, -2.9712e-01,\n",
      "           9.4750e-01,  1.2619e-01,  1.3990e+00,  8.5814e-01, -2.3505e-02,\n",
      "          -2.1667e-01, -2.0566e-01,  7.2342e-01,  5.3129e-01,  1.2067e+00,\n",
      "           1.2370e+00, -3.0537e-01,  1.0788e+00,  1.7990e+00,  1.4289e-01,\n",
      "          -1.5001e-02,  1.4341e+00,  1.7240e+00, -6.4331e-01,  4.1379e-01,\n",
      "           6.3648e-01,  9.7115e-01,  2.9130e-01,  1.7698e-01,  1.4369e+00,\n",
      "           1.3567e+00, -1.9937e-02,  1.6179e-01,  1.0808e+00, -4.0082e-02,\n",
      "           1.0408e+00,  2.5321e+00,  1.3144e+00, -5.1334e-01,  1.2275e-01,\n",
      "           6.5788e-02,  3.9466e-01,  7.7968e-01],\n",
      "         [ 1.4482e-01, -5.4802e-01,  1.9729e-01,  7.5640e-01,  3.8802e-01,\n",
      "          -5.4499e-01, -3.3026e-01,  6.0571e-01, -1.3085e-02, -1.4555e-01,\n",
      "           9.0016e-02, -2.0889e-01, -4.7865e-01,  4.7873e-01,  5.4709e-01,\n",
      "          -5.3244e-01, -5.7431e-01,  1.7299e-01,  5.4244e-01, -9.5983e-02,\n",
      "          -5.5832e-01,  1.8307e-01,  4.9948e-01, -1.4493e-01, -2.7532e-01,\n",
      "          -5.0893e-01, -5.2665e-01, -1.5886e-01, -1.2972e-01, -2.6985e-02,\n",
      "           4.4194e-01, -2.7688e-01, -1.6203e-01,  1.7077e-01, -4.3875e-01,\n",
      "           2.4484e-01,  3.4661e-01, -7.5547e-01, -5.3507e-01, -5.6644e-02,\n",
      "           6.4825e-01, -5.5781e-01, -5.8019e-01,  5.7278e-01,  2.5237e-01,\n",
      "          -6.1433e-02,  2.6808e-01, -1.3604e-01, -3.0474e-01, -2.0026e-01,\n",
      "          -2.7017e-01,  1.2855e-01,  3.3121e-01, -2.5751e-01, -4.7690e-01,\n",
      "           3.5423e-01,  1.9216e-01,  4.4353e-01, -1.6864e-01, -4.8342e-01,\n",
      "           1.4501e-01,  2.7167e-03, -7.8409e-02,  8.0808e-02,  3.7524e-01,\n",
      "           3.7940e-01,  4.6279e-01, -1.5475e-01, -1.0712e+00, -4.2570e-01,\n",
      "          -3.1579e-01, -1.5478e-01, -4.3333e-01,  2.2201e-01,  4.9595e-01,\n",
      "          -3.3060e-01, -7.8169e-02, -1.8088e-02,  9.1285e-01,  6.0634e-01,\n",
      "          -4.5770e-01,  2.6193e-01,  3.4388e-01,  3.2204e-01, -1.2409e-01,\n",
      "          -2.0194e-01,  3.0884e-01, -4.0742e-01, -1.1104e-01,  8.1556e-01,\n",
      "           5.0530e-01,  7.1090e-01, -1.0200e-01, -4.1842e-01, -8.3590e-03,\n",
      "           3.8417e-01, -2.1779e-01, -1.2244e+00,  2.2867e-01,  6.9050e-01,\n",
      "          -2.0230e-01,  4.7947e-01, -1.1983e-01, -6.3671e-01, -1.5624e-01,\n",
      "          -4.5822e-02,  4.4417e-01,  1.6579e-01,  3.3060e-01,  1.5004e-01,\n",
      "          -4.6792e-01,  6.1606e-01,  5.1139e-01, -6.3306e-01,  2.7653e-02,\n",
      "           7.4744e-01,  1.3898e-01, -8.8348e-01,  1.9315e-01,  4.2839e-01,\n",
      "          -4.5152e-02, -9.5305e-03, -7.1864e-02,  7.6846e-01, -2.1658e-02,\n",
      "          -4.8903e-01,  3.0547e-03,  4.8898e-01, -1.6833e-01,  6.5849e-01,\n",
      "           9.2727e-01, -4.9410e-01, -8.6538e-01,  1.7236e-02,  8.2808e-02,\n",
      "           2.4232e-02,  2.4300e-01, -2.4386e-01]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Conv1d(1, 2, 3, stride=1, padding=0)\n",
    "print(\"layer: \", layer)\n",
    "input = torch.randn(1, 140)\n",
    "print(\"Input: \", input.size())\n",
    "input = input.unsqueeze(0)\n",
    "print(\"Input unsqueezed: \", input.size())\n",
    "print(input)\n",
    "output = layer(input)\n",
    "print(\"output.size: \", output.size())\n",
    "print(\"output: \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01ef9172-fb80-4323-95d5-a602f45f275c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "\n",
    "train_file = \"EEG_Eye_State_Lumped_10_Normalized_Train.csv\"\n",
    "validate_file = \"EEG_Eye_State_Lumped_10_Normalized_Validate.csv\"\n",
    "test_file = \"EEG_Eye_State_Lumped_10_Normalized_Test.csv\"\n",
    "\n",
    "train_dataset = fpnn.FinalProjectEEGDataset(train_file, root_directory)\n",
    "validate_dataset = fpnn.FinalProjectEEGDataset(validate_file, root_directory)\n",
    "test_dataset = fpnn.FinalProjectEEGDataset(test_file, root_directory)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57e0a0f1-d338-4dbf-aa0a-64fc7baa9636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_inputs = 140\n",
    "num_channels = 1\n",
    "kernel_size = 3\n",
    "num_outputs = 2\n",
    "num_outputs2 = 5\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(num_channels, num_outputs, kernel_size)\n",
    "        self.conv2 = nn.Conv1d(num_outputs, num_outputs2, kernel_size)\n",
    "        print((kernel_size - 1) * 2)\n",
    "        self.fc1 = nn.Linear(\n",
    "            (num_inputs - (kernel_size - 1) * 2) * num_outputs2, num_outputs\n",
    "        )\n",
    "        self.fc2 = nn.Linear(num_outputs, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"x.shape1\", x.shape)\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        # print(\"x.shape2\", x.shape)\n",
    "\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        # print(\"x.shape3\", x.shape)\n",
    "\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        # print(\"x.shape4\", x.shape)\n",
    "\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03fdd98d-f67c-4907-8171-a0f882b271f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv1d(1, 2, kernel_size=(3,), stride=(1,))\n",
      "  (conv2): Conv1d(2, 5, kernel_size=(3,), stride=(1,))\n",
      "  (fc1): Linear(in_features=680, out_features=2, bias=True)\n",
      "  (fc2): Linear(in_features=2, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38901ebb-62d7-4962-9e90-afd9d968bf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs.float())\n",
    "        loss = loss_fn(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "            print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fbcec68-0da1-40a1-bb17-c5c27f4cc1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test inputs: 50 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in validate_dataloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = model(images.float())\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(\"Accuracy of the network on the test inputs: %d %%\" % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627c8197-020c-4037-9e6c-c24805e23d34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp",
   "language": "python",
   "name": "fp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
