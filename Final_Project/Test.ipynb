{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff227eb5-aa75-43cc-a156-01f052299576",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10659f7f-3867-4987-a15e-f9188774ac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import finalprojectneuralnetworks as fpnn\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db8f1e0-4c84-4ad5-907c-08eb6235def6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5  # number of samples/batch size\n",
    "l = 5  # sentence len\n",
    "d = 3  # embedding dimension\n",
    "rand_arr = torch.rand(n, l, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "244cf131-3622-45b1-b7a6-56bdc90ebc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:  Conv1d(1, 2, kernel_size=(3,), stride=(1,))\n",
      "Input:  torch.Size([1, 140])\n",
      "Input unsqueezed:  torch.Size([1, 1, 140])\n",
      "tensor([[[-0.7431, -1.3231, -0.2798, -0.6969, -0.3014,  0.4693,  2.3976,\n",
      "           0.6462,  1.1255, -0.0327,  0.2744,  2.3752,  0.5119,  2.5705,\n",
      "           2.2331,  0.5832, -1.1167, -0.8156, -1.5347,  0.5259,  0.5914,\n",
      "          -0.4788, -0.7259, -0.5637, -0.0317,  2.3276,  0.5045, -1.1667,\n",
      "          -0.2914, -1.3644,  0.8889,  0.7055, -0.2919,  0.8258,  0.0621,\n",
      "          -0.2271,  0.3037, -0.8957,  0.5149,  1.4387,  0.2754,  0.3283,\n",
      "           1.2408,  1.7800, -0.6539,  1.5210,  1.1424,  0.4710, -0.3416,\n",
      "           0.8642, -0.4981,  1.8093,  1.1451,  1.9847, -1.1298, -0.1469,\n",
      "           0.0666, -0.0117,  0.1301,  0.0468,  0.1862, -0.5317,  1.0391,\n",
      "          -0.4714, -0.0728,  0.2183, -1.7872, -2.0351, -0.6912, -0.8638,\n",
      "          -1.5914,  1.2461, -1.1371,  1.0467,  1.3965, -0.7456, -0.7665,\n",
      "          -0.4021,  0.0674,  0.3842,  0.5576,  0.4484,  0.9717, -0.6368,\n",
      "           2.2619, -0.8448,  0.0344,  0.7414, -0.5795, -0.0418,  0.6559,\n",
      "          -0.5676, -0.3689, -0.2055, -0.3484, -0.7641,  1.4339,  1.7735,\n",
      "           0.6654,  1.0046,  0.2894,  0.3685,  0.4229, -0.0412, -1.5438,\n",
      "           2.4733, -0.6210, -0.1585, -0.6719,  0.0109,  0.3818,  1.2581,\n",
      "          -0.0747, -0.4826,  0.4502,  1.6462,  1.4428, -0.7257, -0.4542,\n",
      "           0.0406, -1.0955, -0.5452,  1.2878, -0.1311,  0.9454,  1.4474,\n",
      "          -1.3395,  0.2720, -0.9068, -1.4594, -1.1425,  0.6662, -1.4944,\n",
      "           0.3075,  1.9850, -0.8338,  2.2292, -0.4244, -1.1066,  0.7130]]])\n",
      "output.size:  torch.Size([1, 2, 138])\n",
      "output:  tensor([[[ 0.4221,  0.0156,  0.1052,  0.1602,  0.1755, -1.0328, -0.5316,\n",
      "          -0.6914, -0.2467,  0.2087, -1.0219, -0.1948, -0.7984, -1.2845,\n",
      "          -0.9189,  0.0451,  0.0331,  0.6722, -0.0194, -0.5512, -0.1889,\n",
      "           0.0944,  0.1699,  0.3964, -0.9605, -0.9114,  0.1780, -0.1322,\n",
      "           0.6001, -0.1643, -0.6121,  0.0229, -0.4197, -0.3380, -0.0109,\n",
      "          -0.4104,  0.2575,  0.0572, -0.7354, -0.4024, -0.0775, -0.3383,\n",
      "          -1.1549,  0.1446, -0.4300, -0.7266, -0.5664,  0.0838, -0.5364,\n",
      "           0.2720, -0.5651, -0.4752, -1.3138, -0.0225,  0.0840, -0.1572,\n",
      "          -0.1298, -0.1903, -0.1510, -0.3332,  0.2336, -0.5724, -0.1296,\n",
      "          -0.0109, -0.5725,  0.1239,  0.7725,  0.2476, -0.0205,  0.8414,\n",
      "          -0.6297,  0.3192, -0.1247, -0.9942, -0.2085,  0.1803,  0.1317,\n",
      "          -0.0436, -0.2021, -0.3372, -0.2177, -0.7198,  0.3989, -1.1094,\n",
      "          -0.1391,  0.1028, -0.5585, -0.0381,  0.0777, -0.5116, -0.0940,\n",
      "           0.0368, -0.0838, -0.1322,  0.4788, -0.2573, -0.9238, -0.4723,\n",
      "          -0.5841, -0.3372, -0.2543, -0.3772, -0.4951,  0.9423, -1.0177,\n",
      "          -0.2950, -0.1286,  0.1386,  0.0171, -0.0568, -0.7133, -0.3965,\n",
      "           0.1376,  0.0631, -0.5805, -1.0952, -0.1623,  0.1410, -0.3132,\n",
      "           0.1679,  0.4726, -0.6018, -0.1011, -0.2206, -1.1149,  0.2189,\n",
      "          -0.2392, -0.1173,  0.3316,  0.6387, -0.5366,  0.3996,  0.3313,\n",
      "          -1.1353,  0.3232, -0.9864, -0.5211,  0.4840],\n",
      "         [-0.0331,  1.0203,  0.0658,  0.2528, -0.2068,  1.0500, -1.1757,\n",
      "           0.4746, -0.5485, -0.4585,  1.1931, -1.7403,  0.5717, -0.1957,\n",
      "          -0.3401, -0.4735,  0.8959, -0.3909,  1.1422,  0.3979, -0.1521,\n",
      "           0.2520,  0.2962, -0.3162,  1.3403, -0.4197, -0.6394,  1.1582,\n",
      "          -0.7149,  1.2105,  0.1938, -0.6502,  0.7987, -0.1536, -0.0727,\n",
      "           0.8029, -0.6555,  0.4843,  0.6165, -0.5700, -0.2065,  0.0873,\n",
      "           0.7243, -1.6955,  1.0152, -0.1802, -0.0620, -0.5602,  1.0426,\n",
      "          -1.1961,  1.0885, -0.8658,  1.0577, -1.4970,  0.7213,  0.3183,\n",
      "           0.1057,  0.2579,  0.0834,  0.4575, -0.5681,  1.2376, -0.6252,\n",
      "           0.3418,  0.9784, -0.2210,  0.2952,  1.2463,  0.6506, -0.6459,\n",
      "           2.1776, -1.5026,  0.9317,  0.6500, -0.7377,  0.3224,  0.3749,\n",
      "           0.3240,  0.1784,  0.1362, -0.2037,  0.7091, -1.4911,  2.1107,\n",
      "          -1.5556,  0.4290,  0.7919, -0.5300,  0.2677,  0.7825, -0.3618,\n",
      "           0.3771,  0.4071,  0.3800, -0.5363,  0.8089,  0.1543, -0.7738,\n",
      "           0.2843, -0.3333,  0.0922,  0.2396,  0.4713, -1.5057,  2.6587,\n",
      "          -1.4815,  0.6868, -0.0965,  0.4440, -0.0346,  0.7114, -0.3833,\n",
      "          -0.1901,  0.1448,  0.3633,  0.3336, -0.8609,  0.3323,  0.8521,\n",
      "          -0.2519,  0.0550,  1.2662, -0.9275,  0.2945,  0.9415, -1.4594,\n",
      "           1.4113,  0.0418,  0.2818,  0.1401,  1.7259, -1.1214,  0.5133,\n",
      "           1.4236, -2.1569,  2.0566, -0.9055, -0.4351]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Conv1d(1, 2, 3, stride=1, padding=0)\n",
    "print(\"layer: \", layer)\n",
    "input = torch.randn(1, 140)\n",
    "print(\"Input: \", input.size())\n",
    "input = input.unsqueeze(0)\n",
    "print(\"Input unsqueezed: \", input.size())\n",
    "print(input)\n",
    "output = layer(input)\n",
    "print(\"output.size: \", output.size())\n",
    "print(\"output: \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01ef9172-fb80-4323-95d5-a602f45f275c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "\n",
    "train_file = \"EEG_Eye_State_Lumped_10_Normalized_Train.csv\"\n",
    "validate_file = \"EEG_Eye_State_Lumped_10_Normalized_Validate.csv\"\n",
    "test_file = \"EEG_Eye_State_Lumped_10_Normalized_Test.csv\"\n",
    "\n",
    "train_dataset = fpnn.FinalProjectEEGDataset(train_file, root_directory)\n",
    "validate_dataset = fpnn.FinalProjectEEGDataset(validate_file, root_directory)\n",
    "test_dataset = fpnn.FinalProjectEEGDataset(test_file, root_directory)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e0a0f1-d338-4dbf-aa0a-64fc7baa9636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_inputs = 140\n",
    "num_channels = 1\n",
    "kernel_size = 3\n",
    "num_outputs = 2\n",
    "num_outputs2 = 5\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(num_channels, num_outputs, kernel_size)\n",
    "        self.conv2 = nn.Conv1d(num_outputs, num_outputs2, kernel_size)\n",
    "        print((kernel_size - 1) * 2)\n",
    "        self.fc1 = nn.Linear(\n",
    "            (num_inputs - (kernel_size - 1) * 2) * num_outputs2, num_outputs\n",
    "        )\n",
    "        self.fc2 = nn.Linear(num_outputs, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"x.shape1\", x.shape)\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        # print(\"x.shape2\", x.shape)\n",
    "\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        # print(\"x.shape3\", x.shape)\n",
    "\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        # print(\"x.shape4\", x.shape)\n",
    "\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dd2d1e7-4f08-453c-9180-91fbd5b3e73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_inputs = 140\n",
    "num_channels = 1\n",
    "kernel_size = 3\n",
    "num_outputs = 2\n",
    "num_outputs2 = 5\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 20, 2)\n",
    "        self.conv2 = nn.Conv1d(20, 20, 2)\n",
    "        print((kernel_size - 1) * 2)\n",
    "        self.fc1 = nn.Linear(2760, 20)\n",
    "        self.fc2 = nn.Linear(20, 15)\n",
    "        self.fc3 = nn.Linear(15, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"x.shape1\", x.shape)\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        # print(\"x.shape2\", x.shape)\n",
    "\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        # print(\"x.shape3\", x.shape)\n",
    "\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        # print(\"x.shape4\", x.shape)\n",
    "\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03fdd98d-f67c-4907-8171-a0f882b271f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv1d(1, 20, kernel_size=(2,), stride=(1,))\n",
      "  (conv2): Conv1d(20, 20, kernel_size=(2,), stride=(1,))\n",
      "  (fc1): Linear(in_features=2760, out_features=20, bias=True)\n",
      "  (fc2): Linear(in_features=20, out_features=15, bias=True)\n",
      "  (fc3): Linear(in_features=15, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a0424cc-1c57-4e5b-80a0-3653f73e7fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "\n",
    "# train_file = \"EEG_Eye_State_Train.csv\"\n",
    "# validate_file = \"EEG_Eye_State_Validate.csv\"\n",
    "# test_file = \"EEG_Eye_State_Test.csv\"\n",
    "\n",
    "train_file = \"Dummy_Lumped_Train.csv\"\n",
    "validate_file = \"Dummy_Lumped_Validate.csv\"\n",
    "test_file = \"Dummy_Lumped_Test.csv\"\n",
    "\n",
    "train_dataset = fpnn.FinalProjectEEGDataset(train_file, root_directory)\n",
    "validate_dataset = fpnn.FinalProjectEEGDataset(validate_file, root_directory)\n",
    "test_dataset = fpnn.FinalProjectEEGDataset(test_file, root_directory)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38901ebb-62d7-4962-9e90-afd9d968bf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs.float())\n",
    "        loss = loss_fn(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "            print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fbcec68-0da1-40a1-bb17-c5c27f4cc1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test inputs: 100 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in validate_dataloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = model(images.float())\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(\"Accuracy of the network on the test inputs: %d %%\" % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627c8197-020c-4037-9e6c-c24805e23d34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp",
   "language": "python",
   "name": "fp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
