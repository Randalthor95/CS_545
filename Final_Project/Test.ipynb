{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff227eb5-aa75-43cc-a156-01f052299576",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10659f7f-3867-4987-a15e-f9188774ac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import finalprojectneuralnetworks as fpnn\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db8f1e0-4c84-4ad5-907c-08eb6235def6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5  # number of samples/batch size\n",
    "l = 5  # sentence len\n",
    "d = 3  # embedding dimension\n",
    "rand_arr = torch.rand(n, l, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "244cf131-3622-45b1-b7a6-56bdc90ebc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:  Conv1d(1, 2, kernel_size=(3,), stride=(1,))\n",
      "Input:  torch.Size([1, 140])\n",
      "Input unsqueezed:  torch.Size([1, 1, 140])\n",
      "tensor([[[-1.0380,  0.9593, -1.7358,  0.3515, -0.3954, -1.0867,  0.2999,\n",
      "           0.7437, -0.8898, -1.4626,  0.2972, -0.3400,  0.0892, -0.7238,\n",
      "           1.5188, -0.5569,  0.3697,  0.1859, -0.3181,  0.0132,  0.7181,\n",
      "           0.5131,  0.3937, -1.6648,  1.5227,  0.6561, -0.1138,  0.9435,\n",
      "           0.2878, -0.3212, -1.8038, -0.0031,  0.4088,  0.4923,  0.4005,\n",
      "           0.7742,  1.1200,  1.2780, -0.9576,  0.2507, -1.2729, -1.0266,\n",
      "           1.3719, -0.2354,  1.4240, -0.3867, -0.2262, -0.1108,  0.3872,\n",
      "           0.5279, -1.1984, -0.1101, -0.7574,  0.0913,  0.3945, -0.0420,\n",
      "           1.2125,  0.9220,  0.7476,  1.5025,  1.1631,  0.3153, -1.1844,\n",
      "           1.2655,  1.2039, -0.4145,  1.4334,  0.2362,  1.4383,  0.7265,\n",
      "          -1.3268, -1.2090,  1.0142,  0.1164, -1.9747,  0.0876, -3.0293,\n",
      "           0.3912,  0.2769, -1.3204,  0.2490,  0.0884,  0.7548,  0.6329,\n",
      "           0.3045, -0.3413,  1.0122,  0.0120, -0.0113,  0.4309, -0.2771,\n",
      "          -0.4343, -1.1787, -1.1527, -2.6098, -1.0987, -0.2826,  0.1766,\n",
      "           0.6323,  1.4638,  0.8597,  0.0319, -1.0262, -0.7413,  0.3102,\n",
      "          -0.9184,  0.8021, -1.2466, -0.6472, -1.5942,  1.8095,  0.2808,\n",
      "          -1.4366,  0.2505,  1.2535, -0.2666,  1.0118, -1.3215,  3.0589,\n",
      "          -0.9820,  0.2539,  0.9331, -0.6240, -0.3139, -0.4977,  1.0233,\n",
      "           2.0311,  0.1416,  0.1195, -0.9935,  1.6263, -1.1874,  0.3423,\n",
      "          -0.5332,  0.8042,  2.6974,  1.3478, -1.5138, -0.8801, -0.3480]]])\n",
      "output.size:  torch.Size([1, 2, 138])\n",
      "output:  tensor([[[-8.4966e-01,  1.3729e+00, -4.1009e-01,  2.5999e-01,  7.3569e-01,\n",
      "           7.3055e-02, -1.9829e-01,  4.7606e-01,  7.7461e-01, -3.0329e-01,\n",
      "           5.2924e-01, -3.0439e-02,  1.0175e+00, -7.0678e-01,  1.0042e+00,\n",
      "           3.0205e-02,  2.1178e-01,  4.7183e-01,  3.8468e-01,  1.0708e-01,\n",
      "           3.4256e-01, -1.9442e-01,  1.5055e+00, -6.2842e-01,  3.5002e-01,\n",
      "           7.4390e-01, -8.1982e-02,  3.1085e-01,  2.4368e-02,  9.8795e-01,\n",
      "          -6.2576e-02,  2.3243e-01,  2.7561e-01,  4.3425e-01,  3.3845e-01,\n",
      "           3.2292e-01, -2.4276e-01,  1.0874e+00, -3.9957e-01,  6.3302e-01,\n",
      "           7.6870e-01, -6.3493e-01,  1.1024e+00, -4.9820e-01,  7.5061e-01,\n",
      "           2.5538e-01,  3.7549e-01,  2.2404e-01, -1.6281e-01,  9.0953e-01,\n",
      "          -1.6817e-01,  6.0935e-01,  1.5564e-01,  1.2292e-01,  7.1703e-01,\n",
      "          -1.5612e-02,  3.8096e-01,  5.8111e-01,  1.1940e-01,  2.3590e-01,\n",
      "           1.2868e-01,  1.2091e+00, -2.5242e-01, -3.2036e-02,  1.1407e+00,\n",
      "          -3.8468e-01,  9.1591e-01, -9.5154e-02, -1.8651e-02,  7.2857e-01,\n",
      "           7.4128e-01, -4.3297e-01, -2.8378e-02,  1.1962e+00, -1.0428e+00,\n",
      "           1.7281e+00, -5.7719e-01, -8.4167e-02,  9.9335e-01, -1.3460e-01,\n",
      "           5.0407e-01,  1.4128e-01,  2.7626e-01,  2.2017e-01,  7.7264e-01,\n",
      "          -2.4116e-01,  5.2929e-01,  4.0339e-01,  1.9831e-02,  3.9871e-01,\n",
      "           9.4895e-02,  3.8674e-01, -1.9226e-01,  8.4425e-01,  3.1967e-02,\n",
      "           1.7599e-01,  2.9995e-01,  4.3381e-01,  2.8070e-02,  2.8449e-01,\n",
      "           2.1728e-01,  5.4488e-01,  4.2912e-01, -2.7854e-01,  9.7085e-01,\n",
      "          -6.2328e-01,  8.5926e-01, -1.6459e-01,  1.2878e+00, -8.3373e-01,\n",
      "           2.4042e-01,  1.0454e+00,  1.3973e-01, -2.7030e-01,  9.7860e-01,\n",
      "          -5.7041e-01,  1.9124e+00, -1.6397e+00,  1.5467e+00,  1.6891e-01,\n",
      "          -2.2308e-01,  7.0774e-01,  1.3296e-01,  6.8820e-01,  2.4189e-01,\n",
      "          -3.0850e-01,  7.6316e-01,  7.0144e-03,  1.1709e+00, -9.8615e-01,\n",
      "           1.2991e+00, -3.0417e-01,  8.1151e-01,  5.0248e-01, -3.3920e-01,\n",
      "          -2.1761e-02,  1.0522e+00,  1.9516e-01],\n",
      "         [-7.3542e-01, -7.3326e-01, -9.0580e-01, -6.8869e-01, -8.8660e-01,\n",
      "          -6.4553e-01, -3.8960e-01, -7.6459e-01, -1.0966e+00, -8.5049e-01,\n",
      "          -5.5964e-01, -6.8579e-01, -5.5027e-01, -3.9279e-01, -3.0233e-01,\n",
      "          -5.6592e-01, -4.5335e-01, -5.8803e-01, -5.4374e-01, -3.1023e-01,\n",
      "          -2.1253e-01, -5.1339e-01, -7.1882e-01, -4.7757e-01, -4.5382e-02,\n",
      "          -3.2662e-01, -3.0664e-01, -2.9526e-01, -7.6226e-01, -1.0847e+00,\n",
      "          -9.2447e-01, -3.9489e-01, -2.8850e-01, -2.5204e-01, -1.4072e-01,\n",
      "           5.0631e-02, -7.1734e-02, -4.7272e-01, -8.4687e-01, -9.2858e-01,\n",
      "          -9.5771e-01, -4.6519e-01, -1.4016e-01, -2.8566e-01, -3.4553e-01,\n",
      "          -7.1032e-01, -5.8992e-01, -4.2138e-01, -4.5776e-01, -7.4625e-01,\n",
      "          -9.3831e-01, -7.5959e-01, -6.6037e-01, -4.3465e-01, -3.3964e-01,\n",
      "          -1.5292e-01,  4.3986e-02,  1.4417e-02,  1.2931e-01,  1.2540e-01,\n",
      "          -3.3114e-01, -6.4264e-01, -3.7265e-01,  1.9063e-03, -2.2350e-01,\n",
      "          -2.5711e-01, -4.5125e-03, -5.2444e-02, -1.7848e-01, -8.5218e-01,\n",
      "          -1.0553e+00, -5.6013e-01, -5.0179e-01, -1.0172e+00, -1.3136e+00,\n",
      "          -1.2591e+00, -1.1215e+00, -5.3389e-01, -7.9630e-01, -7.8346e-01,\n",
      "          -3.8967e-01, -2.7052e-01, -1.8324e-01, -3.6491e-01, -4.5822e-01,\n",
      "          -3.7161e-01, -3.1594e-01, -5.0487e-01, -4.7455e-01, -5.6952e-01,\n",
      "          -8.5369e-01, -1.0765e+00, -1.3997e+00, -1.6004e+00, -1.4641e+00,\n",
      "          -8.5788e-01, -5.0327e-01, -1.9120e-01,  5.9956e-02,  8.5818e-03,\n",
      "          -4.5612e-01, -8.8542e-01, -9.4330e-01, -7.4335e-01, -6.2644e-01,\n",
      "          -6.9474e-01, -7.5342e-01, -1.1767e+00, -9.1010e-01, -4.2901e-01,\n",
      "          -2.1799e-01, -8.2478e-01, -6.8361e-01, -2.0382e-01, -2.2014e-01,\n",
      "          -4.9903e-01, -3.2224e-01, -1.8528e-01, -6.7359e-02, -6.1253e-01,\n",
      "          -3.2325e-01, -5.2874e-01, -8.2937e-01, -6.3959e-01, -1.8600e-01,\n",
      "           2.1662e-01, -3.3520e-02, -5.9611e-01, -6.0020e-01, -4.9609e-01,\n",
      "          -4.4078e-01, -7.9643e-01, -5.2085e-01, -1.7769e-01,  4.6622e-01,\n",
      "           2.4997e-01, -7.2061e-01, -1.1625e+00]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Conv1d(1, 2, 3, stride=1, padding=0)\n",
    "print(\"layer: \", layer)\n",
    "input = torch.randn(1, 140)\n",
    "print(\"Input: \", input.size())\n",
    "input = input.unsqueeze(0)\n",
    "print(\"Input unsqueezed: \", input.size())\n",
    "print(input)\n",
    "output = layer(input)\n",
    "print(\"output.size: \", output.size())\n",
    "print(\"output: \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01ef9172-fb80-4323-95d5-a602f45f275c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "\n",
    "train_file = \"EEG_Eye_State_Lumped_10_Normalized_Train.csv\"\n",
    "validate_file = \"EEG_Eye_State_Lumped_10_Normalized_Validate.csv\"\n",
    "test_file = \"EEG_Eye_State_Lumped_10_Normalized_Test.csv\"\n",
    "\n",
    "train_dataset = fpnn.FinalProjectEEGDataset(train_file, root_directory)\n",
    "validate_dataset = fpnn.FinalProjectEEGDataset(validate_file, root_directory)\n",
    "test_dataset = fpnn.FinalProjectEEGDataset(test_file, root_directory)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57e0a0f1-d338-4dbf-aa0a-64fc7baa9636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_inputs = 140\n",
    "num_channels = 1\n",
    "kernel_size = 3\n",
    "num_outputs = 2\n",
    "num_outputs2 = 5\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(num_channels, num_outputs, kernel_size)\n",
    "        self.conv2 = nn.Conv1d(num_outputs, num_outputs2, kernel_size)\n",
    "        print((kernel_size - 1) * 2)\n",
    "        self.fc1 = nn.Linear(\n",
    "            (num_inputs - (kernel_size - 1) * 2) * num_outputs2, num_outputs\n",
    "        )\n",
    "        self.fc2 = nn.Linear(num_outputs, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"x.shape1\", x.shape)\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        # print(\"x.shape2\", x.shape)\n",
    "\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        # print(\"x.shape3\", x.shape)\n",
    "\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        # print(\"x.shape4\", x.shape)\n",
    "\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03fdd98d-f67c-4907-8171-a0f882b271f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv1d(1, 2, kernel_size=(3,), stride=(1,))\n",
      "  (conv2): Conv1d(2, 5, kernel_size=(3,), stride=(1,))\n",
      "  (fc1): Linear(in_features=680, out_features=2, bias=True)\n",
      "  (fc2): Linear(in_features=2, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38901ebb-62d7-4962-9e90-afd9d968bf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs.float())\n",
    "        loss = loss_fn(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "            print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fbcec68-0da1-40a1-bb17-c5c27f4cc1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test inputs: 49 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in validate_dataloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = model(images.float())\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(\"Accuracy of the network on the test inputs: %d %%\" % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627c8197-020c-4037-9e6c-c24805e23d34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp",
   "language": "python",
   "name": "fp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
