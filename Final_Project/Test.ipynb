{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff227eb5-aa75-43cc-a156-01f052299576",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10659f7f-3867-4987-a15e-f9188774ac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import finalprojectneuralnetworks as fpnn\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6530b0b9-5343-4dc7-90ba-69e1d9807ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(np.sqrt(np.prod(117))) * 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db8f1e0-4c84-4ad5-907c-08eb6235def6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5  # number of samples/batch size\n",
    "l = 5  # sentence len\n",
    "d = 3  # embedding dimension\n",
    "rand_arr = torch.rand(n, l, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "244cf131-3622-45b1-b7a6-56bdc90ebc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:  Conv1d(1, 2, kernel_size=(3,), stride=(1,))\n",
      "Input:  torch.Size([1, 140])\n",
      "Input unsqueezed:  torch.Size([1, 1, 140])\n",
      "tensor([[[-5.6112e-01,  1.0800e+00, -6.8690e-01,  8.2328e-02, -2.0984e-01,\n",
      "           8.0118e-01, -2.7412e+00, -4.6481e-01,  3.0627e-01,  1.5638e+00,\n",
      "          -1.4916e+00,  5.9073e-01,  4.9678e-01, -3.8896e-02, -6.1029e-01,\n",
      "          -2.4082e+00, -8.4455e-01,  3.3446e-01, -6.3489e-01, -1.7447e+00,\n",
      "           2.1614e+00,  6.4061e-01, -3.0775e-01, -2.6468e-02,  1.2324e-01,\n",
      "          -1.4419e-01, -6.5323e-02,  2.3487e+00,  1.2510e+00,  3.3979e-01,\n",
      "           4.1016e-01, -8.9422e-01,  1.2193e+00,  1.2881e+00, -1.6794e+00,\n",
      "          -6.0038e-01, -1.7180e-01,  1.0112e+00,  2.6521e+00,  8.9963e-01,\n",
      "           1.0619e+00,  9.2887e-01, -4.0007e-01, -1.3792e+00,  1.3930e+00,\n",
      "           1.9903e-01,  8.5264e-01, -1.3114e-01, -4.1769e-01,  4.0524e-02,\n",
      "           9.4494e-02,  2.7278e-02, -7.9579e-01,  3.2077e-01, -1.8214e+00,\n",
      "          -1.8354e+00, -7.2558e-01,  1.8842e-01, -1.2628e-01, -1.6650e+00,\n",
      "          -1.9814e+00,  1.5877e+00, -1.4011e+00,  4.4677e-01, -2.7075e-01,\n",
      "          -1.0222e+00,  1.1001e+00, -7.6445e-02,  3.4945e-01, -1.4617e+00,\n",
      "           1.6720e-03, -5.3476e-01, -6.8363e-01,  2.2885e-01, -1.1918e-02,\n",
      "          -1.3356e+00, -1.7365e-01,  3.4278e-01, -6.2501e-01,  5.5020e-01,\n",
      "          -1.1613e+00,  9.2695e-01,  9.2938e-01, -6.7121e-01, -1.0576e+00,\n",
      "           1.8895e+00,  5.9353e-01, -1.5197e+00,  4.4522e-01, -7.5036e-01,\n",
      "           4.6354e-01, -4.5152e-01,  1.2020e-01, -2.0071e-01, -1.3023e+00,\n",
      "           1.5376e+00, -4.3519e-01,  7.4064e-01, -1.4913e+00,  3.0097e-01,\n",
      "           4.8341e-01,  6.2207e-01, -5.4927e-01,  9.4796e-01, -3.3604e-02,\n",
      "           7.0226e-01, -1.7323e+00, -1.6221e-01,  6.0279e-01,  1.1783e+00,\n",
      "          -7.8993e-01, -1.8314e+00, -6.3187e-01,  1.0672e+00,  1.4538e+00,\n",
      "          -1.5601e+00, -8.6093e-01,  5.7173e-01,  1.6003e-01,  6.0912e-01,\n",
      "          -1.1480e+00, -1.2225e+00,  4.1779e-01,  1.8805e+00, -7.0455e-01,\n",
      "           1.9781e+00,  4.5892e-01, -9.2696e-01,  1.2270e+00,  3.5006e-03,\n",
      "          -5.3543e-01, -8.6913e-02, -1.0581e+00,  6.6972e-01,  4.4813e-01,\n",
      "           4.2800e-01, -1.3843e+00, -1.9724e+00,  7.3504e-01, -1.2843e+00]]])\n",
      "output.size:  torch.Size([1, 2, 138])\n",
      "output:  tensor([[[-0.5956,  0.1120, -0.1909,  0.1208, -0.9581,  0.6115,  0.0785,\n",
      "           0.1292, -0.9136,  0.4677, -0.1935, -0.2736, -0.2369, -0.4588,\n",
      "           0.4225,  0.2006, -0.3604, -0.3052,  0.8778, -0.6371, -0.3693,\n",
      "          -0.0279, -0.0839, -0.1863, -0.0878,  0.4138, -0.5538, -0.4116,\n",
      "          -0.1256, -0.4350,  0.4199, -0.2082, -0.8677,  0.2635,  0.0194,\n",
      "           0.1524,  0.1550, -0.7177, -0.1468, -0.2318, -0.4809, -0.2939,\n",
      "           0.6008, -0.5001,  0.0139, -0.4027, -0.1657,  0.0168, -0.1109,\n",
      "          -0.1400, -0.2990,  0.1926, -0.6148,  0.0331,  0.2707,  0.1351,\n",
      "          -0.2049, -0.4418, -0.0478,  0.8200, -0.9102,  0.4091, -0.3171,\n",
      "          -0.2570,  0.4298, -0.4703, -0.0139, -0.5421,  0.3253, -0.2406,\n",
      "          -0.1053,  0.1365, -0.1920, -0.4043,  0.2475,  0.0042, -0.3580,\n",
      "           0.1924, -0.5394,  0.4373, -0.1993, -0.5407, -0.1420,  0.6110,\n",
      "          -0.5626, -0.6206,  0.4405, -0.4217,  0.2118, -0.3589,  0.0468,\n",
      "          -0.1994, -0.3406,  0.6100, -0.6817,  0.1815, -0.6681,  0.4011,\n",
      "          -0.1090, -0.1266, -0.4232,  0.2574, -0.4132,  0.0495, -0.7075,\n",
      "           0.3726,  0.0559, -0.0431, -0.6430, -0.2739,  0.2941,  0.2985,\n",
      "          -0.1251, -0.8923,  0.1713,  0.2607, -0.2587, -0.0307, -0.5511,\n",
      "          -0.0353,  0.3379,  0.1616, -0.8371,  0.5340, -0.6173, -0.4503,\n",
      "           0.4316, -0.4908, -0.2305,  0.0251, -0.3243,  0.3475, -0.2262,\n",
      "          -0.1568, -0.5469, -0.1283,  0.6324, -0.6274],\n",
      "         [ 0.0807, -0.9894,  0.0433, -0.2528, -0.4960, -1.3080,  1.1207,\n",
      "           0.2586, -0.4246, -1.3105,  0.6810, -0.5321, -0.6746, -0.7908,\n",
      "          -0.5588,  0.8787,  0.1033, -0.8845,  0.0715,  1.1273, -1.3856,\n",
      "          -0.7041, -0.1234, -0.2897, -0.4054,  0.1493,  0.3690, -1.2665,\n",
      "          -0.8381, -0.5555, -0.4837,  0.6090, -0.9918, -1.3947,  0.4523,\n",
      "           0.1519,  0.4114, -0.2059, -1.3733, -0.4333, -0.7631, -1.0974,\n",
      "          -0.1131,  0.7192, -0.8665, -0.2737, -0.8490, -0.3028, -0.0574,\n",
      "          -0.3023, -0.4790, -0.4101, -0.1207, -1.1098,  0.2101,  0.5717,\n",
      "           0.0971, -0.7004, -0.8673,  0.4814,  0.8115, -1.3304,  0.4800,\n",
      "          -0.7584, -0.1647,  0.4302, -0.8442, -0.4388, -0.7562,  0.3870,\n",
      "          -0.5142, -0.1058,  0.1010, -0.6467, -0.5698,  0.4325, -0.2504,\n",
      "          -0.5088, -0.0607, -0.6554,  0.6383, -0.7359, -1.0944,  0.1720,\n",
      "           0.7043, -1.4494, -0.8236,  0.4637, -0.6006,  0.1071, -0.6121,\n",
      "          -0.0735, -0.6180, -0.1812,  0.6003, -1.0782, -0.1812, -0.9204,\n",
      "           0.6255, -0.2715, -0.5362, -0.5779,  0.1578, -0.6972, -0.4415,\n",
      "          -1.0205,  0.6885,  0.0884, -0.5374, -1.3761, -0.3224,  0.7316,\n",
      "           0.4688, -0.8609, -1.5039,  0.4636,  0.2866, -0.4770, -0.4653,\n",
      "          -1.0386,  0.1528,  0.7343, -0.2962, -1.1060,  0.5105, -1.4234,\n",
      "          -0.5144,  0.4160, -1.0429, -0.4170, -0.2082, -0.3400,  0.4569,\n",
      "          -0.5061, -0.6907, -1.1092,  0.1933,  0.6696]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Conv1d(1, 2, 3, stride=1, padding=0)\n",
    "print(\"layer: \", layer)\n",
    "input = torch.randn(1, 140)\n",
    "print(\"Input: \", input.size())\n",
    "input = input.unsqueeze(0)\n",
    "print(\"Input unsqueezed: \", input.size())\n",
    "print(input)\n",
    "output = layer(input)\n",
    "print(\"output.size: \", output.size())\n",
    "print(\"output: \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01ef9172-fb80-4323-95d5-a602f45f275c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "\n",
    "train_file = \"EEG_Eye_State_Lumped_10_Normalized_Train.csv\"\n",
    "validate_file = \"EEG_Eye_State_Lumped_10_Normalized_Validate.csv\"\n",
    "test_file = \"EEG_Eye_State_Lumped_10_Normalized_Test.csv\"\n",
    "\n",
    "train_dataset = fpnn.FinalProjectEEGDataset(train_file, root_directory)\n",
    "validate_dataset = fpnn.FinalProjectEEGDataset(validate_file, root_directory)\n",
    "test_dataset = fpnn.FinalProjectEEGDataset(test_file, root_directory)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57e0a0f1-d338-4dbf-aa0a-64fc7baa9636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_inputs = 140\n",
    "num_channels = 1\n",
    "kernel_size = 3\n",
    "num_outputs = 2\n",
    "num_outputs2 = 5\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(num_channels, num_outputs, kernel_size)\n",
    "        self.conv2 = nn.Conv1d(num_outputs, num_outputs2, kernel_size)\n",
    "        print((kernel_size - 1) * 2)\n",
    "        self.fc1 = nn.Linear(\n",
    "            (num_inputs - (kernel_size - 1) * 2) * num_outputs2, num_outputs\n",
    "        )\n",
    "        self.fc2 = nn.Linear(num_outputs, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"x.shape1\", x.shape)\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        # print(\"x.shape2\", x.shape)\n",
    "\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        # print(\"x.shape3\", x.shape)\n",
    "\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        # print(\"x.shape4\", x.shape)\n",
    "\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dd2d1e7-4f08-453c-9180-91fbd5b3e73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_inputs = 140\n",
    "num_channels = 1\n",
    "kernel_size = 3\n",
    "num_outputs = 2\n",
    "num_outputs2 = 5\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 20, 2)\n",
    "        self.conv2 = nn.Conv1d(20, 20, 2)\n",
    "        print((kernel_size - 1) * 2)\n",
    "        self.fc1 = nn.Linear(2760, 20)\n",
    "        self.fc2 = nn.Linear(20, 15)\n",
    "        self.fc3 = nn.Linear(15, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"x.shape1\", x.shape)\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        # print(\"x.shape2\", x.shape)\n",
    "\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        # print(\"x.shape3\", x.shape)\n",
    "\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        # print(\"x.shape4\", x.shape)\n",
    "\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03fdd98d-f67c-4907-8171-a0f882b271f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv1d(1, 20, kernel_size=(2,), stride=(1,))\n",
      "  (conv2): Conv1d(20, 20, kernel_size=(2,), stride=(1,))\n",
      "  (fc1): Linear(in_features=2760, out_features=20, bias=True)\n",
      "  (fc2): Linear(in_features=20, out_features=15, bias=True)\n",
      "  (fc3): Linear(in_features=15, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a0424cc-1c57-4e5b-80a0-3653f73e7fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "\n",
    "# train_file = \"EEG_Eye_State_Train.csv\"\n",
    "# validate_file = \"EEG_Eye_State_Validate.csv\"\n",
    "# test_file = \"EEG_Eye_State_Test.csv\"\n",
    "\n",
    "train_file = \"Dummy_Lumped_Train.csv\"\n",
    "validate_file = \"Dummy_Lumped_Validate.csv\"\n",
    "test_file = \"Dummy_Lumped_Test.csv\"\n",
    "\n",
    "train_dataset = fpnn.FinalProjectEEGDataset(train_file, root_directory)\n",
    "validate_dataset = fpnn.FinalProjectEEGDataset(validate_file, root_directory)\n",
    "test_dataset = fpnn.FinalProjectEEGDataset(test_file, root_directory)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38901ebb-62d7-4962-9e90-afd9d968bf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs.float())\n",
    "        loss = loss_fn(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "            print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fbcec68-0da1-40a1-bb17-c5c27f4cc1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test inputs: 100 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in validate_dataloader:\n",
    "        channel_data, labels = data\n",
    "        # calculate outputs by running channel_data through the network\n",
    "        outputs = model(channel_data.float())\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(\"Accuracy of the network on the test inputs: %d %%\" % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627c8197-020c-4037-9e6c-c24805e23d34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp",
   "language": "python",
   "name": "fp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
