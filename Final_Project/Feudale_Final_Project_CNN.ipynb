{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334a04dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c49f5edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import finalprojectneuralnetworks as fpnn\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb987a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AF3             4309.74\n",
      "F7              4052.82\n",
      "F3              4285.13\n",
      "FC5             4155.38\n",
      "T7              4369.23\n",
      "P7              4646.67\n",
      "O1              4100.00\n",
      "O2              4649.74\n",
      "P8              4221.54\n",
      "T8              4230.26\n",
      "FC6             4206.15\n",
      "F4              4292.82\n",
      "F8              4598.97\n",
      "AF4             4362.56\n",
      "eyeDetection       0.00\n",
      "Name: 0, dtype: float64\n",
      "0\n",
      "[[4309.74 4052.82 4285.13 4155.38 4369.23 4646.67 4100.   4649.74 4221.54\n",
      "  4230.26 4206.15 4292.82 4598.97 4362.56]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11984, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "fp_csv_file = \"EEG_Eye_State_Train.csv\"\n",
    "# fp_csv_file = \"Dummy_Train.csv\"\n",
    "eeg_data = pd.read_csv(\n",
    "    os.path.join(root_directory, fp_csv_file), delimiter=\",\", usecols=range(15)\n",
    ").dropna(axis=0)\n",
    "\n",
    "print(eeg_data.iloc[0, :])\n",
    "open_or_closed = eeg_data.iloc[0, 14]\n",
    "channel_data = eeg_data.iloc[0, 0:14]\n",
    "channel_data = np.array([channel_data])\n",
    "channel_data = channel_data.astype(\"float\").reshape(-1, 14)\n",
    "print(open_or_closed)\n",
    "print(channel_data)\n",
    "eeg_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d7acdc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AF3             float64\n",
      "F7              float64\n",
      "F3              float64\n",
      "FC5             float64\n",
      "T7              float64\n",
      "P7              float64\n",
      "O1              float64\n",
      "O2              float64\n",
      "P8              float64\n",
      "T8              float64\n",
      "FC6             float64\n",
      "F4              float64\n",
      "F8              float64\n",
      "AF4             float64\n",
      "eyeDetection      int64\n",
      "dtype: object\n",
      "AF3             float64\n",
      "F7              float64\n",
      "F3              float64\n",
      "FC5             float64\n",
      "T7              float64\n",
      "P7              float64\n",
      "O1              float64\n",
      "O2              float64\n",
      "P8              float64\n",
      "T8              float64\n",
      "FC6             float64\n",
      "F4              float64\n",
      "F8              float64\n",
      "AF4             float64\n",
      "eyeDetection      int64\n",
      "dtype: object\n",
      "AF3             float64\n",
      "F7              float64\n",
      "F3              float64\n",
      "FC5             float64\n",
      "T7              float64\n",
      "P7              float64\n",
      "O1              float64\n",
      "O2              float64\n",
      "P8              float64\n",
      "T8              float64\n",
      "FC6             float64\n",
      "F4              float64\n",
      "F8              float64\n",
      "AF4             float64\n",
      "eyeDetection      int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "\n",
    "train_file = \"EEG_Eye_State_Train.csv\"\n",
    "validate_file = \"EEG_Eye_State_Validate.csv\"\n",
    "test_file = \"EEG_Eye_State_Test.csv\"\n",
    "\n",
    "# train_file = \"Dummy_Train.csv\"\n",
    "# validate_file = \"Dummy_Validate.csv\"\n",
    "# test_file = \"Dummy_Test.csv\"\n",
    "\n",
    "train_dataset = fpnn.FinalProjectEEGDataset(train_file, root_directory)\n",
    "validate_dataset = fpnn.FinalProjectEEGDataset(validate_file, root_directory)\n",
    "test_dataset = fpnn.FinalProjectEEGDataset(test_file, root_directory)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a360238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c852c421-239e-4363-abbd-045ad9dc7de8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class ConvNet(torch.nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         n_inputs,\n",
    "#         n_hiddens_per_conv_layer,\n",
    "#         n_hiddens_per_fc_layer,\n",
    "#         n_outputs,\n",
    "#         patch_size_per_conv_layer,\n",
    "#         stride_per_conv_layer,\n",
    "#         activation_function=\"tanh\",\n",
    "#         device=\"cpu\",\n",
    "#     ):\n",
    "\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.device = device\n",
    "\n",
    "#         n_conv_layers = len(n_hiddens_per_conv_layer)\n",
    "#         if (\n",
    "#             len(patch_size_per_conv_layer) != n_conv_layers\n",
    "#             or len(stride_per_conv_layer) != n_conv_layers\n",
    "#         ):\n",
    "#             raise Exception(\n",
    "#                 \"The lengths of n_hiddens_per_conv_layer, patch_size_per_conv_layer, and stride_per_conv_layer must be equal.\"\n",
    "#             )\n",
    "\n",
    "#         self.activation_function = (\n",
    "#             torch.tanh if activation_function == \"tanh\" else torch.relu\n",
    "#         )\n",
    "\n",
    "#         # Create all convolutional layers\n",
    "#         # First argument to first Conv1d is number of channels for a sample\n",
    "#         n_in = 14\n",
    "#         input_hw = int(\n",
    "#             np.sqrt(n_inputs)\n",
    "#         )  # original input image height (=width because image assumed square)\n",
    "#         self.conv_layers = torch.nn.ModuleList()\n",
    "#         for nh, patch_size, stride in zip(\n",
    "#             n_hiddens_per_conv_layer, patch_size_per_conv_layer, stride_per_conv_layer\n",
    "#         ):\n",
    "#             self.conv_layers.append(\n",
    "#                 torch.nn.Conv1d(n_in, nh, kernel_size=patch_size, stride=stride)\n",
    "#             )\n",
    "#             conv_layer_output_hw = (input_hw - patch_size) // stride + 1\n",
    "#             input_hw = conv_layer_output_hw  # for next trip through this loop\n",
    "#             n_in = nh\n",
    "\n",
    "#         # Create all fully connected layers.  First must determine number of inputs to first\n",
    "#         # fully-connected layer that results from flattening the images coming out of the last\n",
    "#         # convolutional layer.\n",
    "#         n_in = input_hw ** 2 * n_in  # n_hiddens_per_fc_layer[0]\n",
    "#         self.fc_layers = torch.nn.ModuleList()\n",
    "#         for nh in n_hiddens_per_fc_layer:\n",
    "#             self.fc_layers.append(torch.nn.Linear(n_in, nh))\n",
    "#             n_in = nh\n",
    "#         self.fc_layers.append(torch.nn.Linear(n_in, n_outputs))\n",
    "\n",
    "#         self.to(self.device)\n",
    "\n",
    "#     def forward_all_outputs(self, X):\n",
    "#         n_samples = X.shape[0]\n",
    "#         Ys = [X]\n",
    "#         for conv_layer in self.conv_layers:\n",
    "#             Ys.append(self.activation_function(conv_layer(Ys[-1])))\n",
    "\n",
    "#         for layeri, fc_layer in enumerate(self.fc_layers[:-1]):\n",
    "#             if layeri == 0:\n",
    "#                 Ys.append(\n",
    "#                     self.activation_function(fc_layer(Ys[-1].reshape(n_samples, -1)))\n",
    "#                 )\n",
    "#             else:\n",
    "#                 Ys.append(self.activation_function(fc_layer(Ys[-1])))\n",
    "\n",
    "#         Ys.append(self.fc_layers[-1](Ys[-1]))\n",
    "#         return Ys\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         Ys = self.forward_all_outputs(X)\n",
    "#         return Ys[-1]\n",
    "\n",
    "#     def train(\n",
    "#         self, X, T, batch_size, n_epochs, learning_rate, method=\"sgd\", verbose=True\n",
    "#     ):\n",
    "\n",
    "#         # Set data matrices to torch.tensors if not already.\n",
    "#         if not isinstance(X, torch.Tensor):\n",
    "#             X = torch.from_numpy(X).float().to(self.device)\n",
    "#         if not isinstance(T, torch.Tensor):\n",
    "#             T = (\n",
    "#                 torch.from_numpy(T).long().to(self.device)\n",
    "#             )  # required for classification in pytorch\n",
    "\n",
    "#         X.requires_grad_(True)\n",
    "\n",
    "#         self.classes = torch.unique(T)\n",
    "\n",
    "#         if method == \"sgd\":\n",
    "#             optimizer = torch.optim.SGD(\n",
    "#                 self.parameters(), lr=learning_rate, momentum=0.9\n",
    "#             )\n",
    "#         else:\n",
    "#             optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "#         CELoss = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "#         self.error_trace = []\n",
    "\n",
    "#         for epoch in range(n_epochs):\n",
    "\n",
    "#             num_batches = X.shape[0] // batch_size\n",
    "#             loss_sum = 0\n",
    "\n",
    "#             for k in range(num_batches):\n",
    "#                 start = k * batch_size\n",
    "#                 end = (k + 1) * batch_size\n",
    "#                 X_batch = X[start:end, ...]\n",
    "#                 T_batch = T[start:end, ...]\n",
    "\n",
    "#                 Y = self.forward(X_batch)\n",
    "\n",
    "#                 loss = CELoss(Y, T_batch)\n",
    "#                 loss.backward()\n",
    "\n",
    "#                 # Update parameters\n",
    "#                 optimizer.step()\n",
    "#                 optimizer.zero_grad()\n",
    "\n",
    "#                 loss_sum += loss\n",
    "\n",
    "#             self.error_trace.append(loss_sum / num_batches)\n",
    "\n",
    "#             if n_epochs >= 10:\n",
    "#                 if verbose and (epoch + 1) % (n_epochs // 10) == 0:\n",
    "#                     print(\n",
    "#                         f\"{method}: Epoch {epoch + 1} Loss {self.error_trace[-1]:.3f}\"\n",
    "#                     )\n",
    "#             else:\n",
    "#                 print(f\"{method}: Epoch {epoch + 1} Loss {self.error_trace[-1]:.3f}\")\n",
    "\n",
    "#         return self\n",
    "\n",
    "#     def softmax(self, Y):\n",
    "#         \"\"\"Apply to final layer weighted sum outputs\"\"\"\n",
    "#         # Trick to avoid overflow\n",
    "#         maxY = torch.max(Y, axis=1)[0].reshape((-1, 1))\n",
    "#         expY = torch.exp(Y - maxY)\n",
    "#         denom = torch.sum(expY, axis=1).reshape((-1, 1))\n",
    "#         Y = expY / denom\n",
    "#         return Y\n",
    "\n",
    "#     def use(self, X):\n",
    "#         # Set input matrix to torch.tensors if not already.\n",
    "#         if not isinstance(X, torch.Tensor):\n",
    "#             X = torch.from_numpy(X).float().to(self.device)\n",
    "#         Y = self.forward(X)\n",
    "#         probs = self.softmax(Y)\n",
    "#         classes = self.classes[torch.argmax(probs, axis=1).cpu().numpy()]\n",
    "#         return classes.cpu().numpy(), probs.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94e3b846-55b1-4ef7-8e2d-8fdab8750ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i == 0\n",
      "CNN(\n",
      "  (fc): Linear(in_features=20, out_features=2, bias=True)\n",
      "  (activation): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = fpnn.CNN(\n",
    "    14,\n",
    "    [10, 20],\n",
    "    [5, 5],\n",
    "    2,\n",
    "    patch_size_per_conv_layer=[5, 7],\n",
    "    stride_per_conv_layer=[2, 2],\n",
    ").to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a301e97d-8669-4c34-9020-614e3f3246f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8374, 0.2545, 0.8252, 0.9551, 0.5692, 0.8358, 0.3712, 0.1554, 0.9819,\n",
      "         0.3232, 0.0615, 0.9851, 0.5938, 0.4462],\n",
      "        [0.6012, 0.9290, 0.6035, 0.2306, 0.6613, 0.6941, 0.2830, 0.3189, 0.5767,\n",
      "         0.8771, 0.8352, 0.0335, 0.9888, 0.0886],\n",
      "        [0.7118, 0.6284, 0.1953, 0.6187, 0.1716, 0.1476, 0.9010, 0.7459, 0.8579,\n",
      "         0.3937, 0.6685, 0.7662, 0.2431, 0.0261],\n",
      "        [0.3556, 0.5971, 0.6863, 0.8687, 0.5330, 0.1546, 0.5511, 0.3928, 0.7017,\n",
      "         0.3648, 0.2897, 0.8469, 0.3107, 0.2995],\n",
      "        [0.3732, 0.1033, 0.6393, 0.5755, 0.7412, 0.3926, 0.7376, 0.1767, 0.5333,\n",
      "         0.2684, 0.2327, 0.7756, 0.2946, 0.7938]])\n",
      "torch.Size([100, 14])\n",
      "torch.Size([100, 14, 1])\n",
      "Sequential(\n",
      "  (0): Conv1d(14, 10, kernel_size=(5,), stride=(2,))\n",
      "  (1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (1). Kernel size: (5). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\RANDAL~1\\AppData\\Local\\Temp/ipykernel_5360/2118295875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mpred_probab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\fp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\College\\CS_545\\Final_Project\\finalprojectneuralnetworks.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mconv_layer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0modc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0modc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconv_layer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0modc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconv_layer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\fp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\fp\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\fp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\fp\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\fp\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    292\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m--> 294\u001b[1;33m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    295\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (1). Kernel size: (5). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from operator import mul\n",
    "\n",
    "print(torch.rand(5, 14))\n",
    "X = torch.rand(100, 14, device=device)\n",
    "print(X.shape)\n",
    "T = torch.randint(0, 2, (100, 1), device=device)\n",
    "logits = model(X)\n",
    "print(logits)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e5e4ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    # first = True\n",
    "    for batch, (X, T) in enumerate(dataloader):\n",
    "        X, T = X.to(device), T.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        Y = model(X.float())\n",
    "        # if first:\n",
    "        #     first = False\n",
    "        #     print('Y.shape', Y.shape)\n",
    "        #     print('T.shape', T.shape)\n",
    "        #     print('T: ', T[0])\n",
    "        loss = loss_fn(Y, T.long())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "#         if batch % 50 == 0:\n",
    "#             loss, current = loss.item(), batch * len(X)\n",
    "# print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17432e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, T in dataloader:\n",
    "            X, T = X.to(device), T.to(device)\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, T.long()).item()\n",
    "            correct += (pred.argmax(1) == T).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(\n",
    "        f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ade0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_epochs(epochs, train_dataloader, model, loss_fn, optimizer):\n",
    "    start = time.time()\n",
    "    for t in range(epochs):\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        if t % 100 == 0:\n",
    "            print(f\"Epoch {t}\\n-------------------------------\")\n",
    "            test(validate_dataloader, model, loss_fn)\n",
    "    end = time.time()\n",
    "    print(\"Done: \", end - start, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ba2342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fashion_training_data = datasets.FashionMNIST(\n",
    "#     root=\"data\",\n",
    "#     train=True,\n",
    "#     download=True,\n",
    "#     transform=ToTensor(),\n",
    "# )\n",
    "# fashion_train_dataloader = DataLoader(fashion_training_data, batch_size=64)\n",
    "# n_samples, n_inputs = train_dataset.__len__(), 28*28\n",
    "# n_outputs = 1\n",
    "# n_hiddens = [10, 10]\n",
    "# model = fpnn.NeuralNetwork(28*28, n_hiddens, 10).to(device)\n",
    "# print(model)\n",
    "\n",
    "# learning_rate = 0.5 / (n_samples * n_outputs)  ## Larger learning rate\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# train(fashion_train_dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3190dd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in train_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bb6285-a276-4dc3-8b56-e9a0551747b7",
   "metadata": {},
   "source": [
    "I again started out with a dummy run using dummy data that had a trivial pattern which the neural network should easily be able to learn. I did this to simply test that the network was correctly setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "544cb811-d26b-476f-9f35-30d5871fe03b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AF3             float64\n",
      "F7              float64\n",
      "F3              float64\n",
      "FC5             float64\n",
      "T7              float64\n",
      "P7              float64\n",
      "O1              float64\n",
      "O2              float64\n",
      "P8              float64\n",
      "T8              float64\n",
      "FC6             float64\n",
      "F4              float64\n",
      "F8              float64\n",
      "AF4             float64\n",
      "eyeDetection      int64\n",
      "dtype: object\n",
      "AF3             float64\n",
      "F7              float64\n",
      "F3              float64\n",
      "FC5             float64\n",
      "T7              float64\n",
      "P7              float64\n",
      "O1              float64\n",
      "O2              float64\n",
      "P8              float64\n",
      "T8              float64\n",
      "FC6             float64\n",
      "F4              float64\n",
      "F8              float64\n",
      "AF4             float64\n",
      "eyeDetection      int64\n",
      "dtype: object\n",
      "AF3             float64\n",
      "F7              float64\n",
      "F3              float64\n",
      "FC5             float64\n",
      "T7              float64\n",
      "P7              float64\n",
      "O1              float64\n",
      "O2              float64\n",
      "P8              float64\n",
      "T8              float64\n",
      "FC6             float64\n",
      "F4              float64\n",
      "F8              float64\n",
      "AF4             float64\n",
      "eyeDetection      int64\n",
      "dtype: object\n",
      "11984\n"
     ]
    }
   ],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "\n",
    "# train_file = \"EEG_Eye_State_Train.csv\"\n",
    "# validate_file = \"EEG_Eye_State_Validate.csv\"\n",
    "# test_file = \"EEG_Eye_State_Test.csv\"\n",
    "\n",
    "train_file = \"Dummy_Train.csv\"\n",
    "validate_file = \"Dummy_Validate.csv\"\n",
    "test_file = \"Dummy_Test.csv\"\n",
    "\n",
    "train_dataset = fpnn.FinalProjectEEGDataset(train_file, root_directory)\n",
    "validate_dataset = fpnn.FinalProjectEEGDataset(validate_file, root_directory)\n",
    "test_dataset = fpnn.FinalProjectEEGDataset(test_file, root_directory)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796d2033-3bac-4f43-be51-a1e49e188983",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 14\n",
    "n_outputs = 2\n",
    "n_hiddens = [5, 5]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs).to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "train_for_epochs(epochs, train_dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcde14a-4ea1-441f-bb63-0da1c1f0d0b6",
   "metadata": {},
   "source": [
    "domain model result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cdf9b4a-56dd-4686-85fd-499a718c7cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AF3             float64\n",
      "F7              float64\n",
      "F3              float64\n",
      "FC5             float64\n",
      "T7              float64\n",
      "P7              float64\n",
      "O1              float64\n",
      "O2              float64\n",
      "P8              float64\n",
      "T8              float64\n",
      "FC6             float64\n",
      "F4              float64\n",
      "F8              float64\n",
      "AF4             float64\n",
      "eyeDetection      int64\n",
      "dtype: object\n",
      "AF3             float64\n",
      "F7              float64\n",
      "F3              float64\n",
      "FC5             float64\n",
      "T7              float64\n",
      "P7              float64\n",
      "O1              float64\n",
      "O2              float64\n",
      "P8              float64\n",
      "T8              float64\n",
      "FC6             float64\n",
      "F4              float64\n",
      "F8              float64\n",
      "AF4             float64\n",
      "eyeDetection      int64\n",
      "dtype: object\n",
      "AF3             float64\n",
      "F7              float64\n",
      "F3              float64\n",
      "FC5             float64\n",
      "T7              float64\n",
      "P7              float64\n",
      "O1              float64\n",
      "O2              float64\n",
      "P8              float64\n",
      "T8              float64\n",
      "FC6             float64\n",
      "F4              float64\n",
      "F8              float64\n",
      "AF4             float64\n",
      "eyeDetection      int64\n",
      "dtype: object\n",
      "11984\n"
     ]
    }
   ],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "\n",
    "train_file = \"EEG_Eye_State_Train.csv\"\n",
    "validate_file = \"EEG_Eye_State_Validate.csv\"\n",
    "test_file = \"EEG_Eye_State_Test.csv\"\n",
    "\n",
    "# train_file = \"Dummy_Train.csv\"\n",
    "# validate_file = \"Dummy_Validate.csv\"\n",
    "# test_file = \"Dummy_Test.csv\"\n",
    "\n",
    "train_dataset = fpnn.FinalProjectEEGDataset(train_file, root_directory)\n",
    "validate_dataset = fpnn.FinalProjectEEGDataset(validate_file, root_directory)\n",
    "test_dataset = fpnn.FinalProjectEEGDataset(test_file, root_directory)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da15e092-edf9-4da9-b2b3-34b5e7d8cd16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_samples, n_inputs = train_dataset.__len__(), 14\n",
    "n_outputs = 2\n",
    "n_hiddens = [5, 5]\n",
    "model = fpnn.NeuralNetwork(n_inputs, n_hiddens, n_outputs).to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "train_for_epochs(epochs, train_dataloader, model, loss_fn, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp",
   "language": "python",
   "name": "fp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
