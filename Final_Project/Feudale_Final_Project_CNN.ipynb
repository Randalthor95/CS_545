{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334a04dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c49f5edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import finalprojectneuralnetworks as fpnn\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9ceec06-d47d-4466-a3ae-49cde53a351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 2\n",
    "\n",
    "\n",
    "def print_confusion_matric(nb_classes, dataloader):\n",
    "    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, classes) in enumerate(dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            classes = classes.to(device)\n",
    "            outputs = model(inputs.float())\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            for t, p in zip(classes.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "    print(confusion_matrix)\n",
    "    new_confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "    for i, row in enumerate(confusion_matrix):\n",
    "        sum = row.sum()\n",
    "        new_confusion_matrix[i, 0] = 100 * row[0] / sum\n",
    "        new_confusion_matrix[i, 1] = 100 * row[1] / sum\n",
    "    conf_matrix = pd.DataFrame(\n",
    "        new_confusion_matrix.numpy(),\n",
    "        index=[\"open\", \"closed\"],\n",
    "        columns=[\"open\", \"closed\"],\n",
    "    )\n",
    "    # cf.style.background_gradient(cmap='Blues').format(\"{:.1f} %\")\n",
    "    print(\"Percent Correct\")\n",
    "    return conf_matrix.style.background_gradient(cmap=\"Blues\").format(\"{:.1f}\")\n",
    "    # print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb987a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11984, 15)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "fp_csv_file = \"EEG_Eye_State_Train.csv\"\n",
    "# fp_csv_file = \"Dummy_Train.csv\"\n",
    "eeg_data = pd.read_csv(\n",
    "    os.path.join(root_directory, fp_csv_file), delimiter=\",\", usecols=range(15)\n",
    ").dropna(axis=0)\n",
    "\n",
    "# print(eeg_data.iloc[0, :])\n",
    "open_or_closed = eeg_data.iloc[0, 14]\n",
    "channel_data = eeg_data.iloc[0, 0:14]\n",
    "channel_data = np.array([channel_data])\n",
    "channel_data = channel_data.astype(\"float\").reshape(-1, 14)\n",
    "# print(open_or_closed)\n",
    "# print(channel_data)\n",
    "eeg_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d7acdc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "\n",
    "train_file = \"EEG_Eye_State_Train.csv\"\n",
    "validate_file = \"EEG_Eye_State_Validate.csv\"\n",
    "test_file = \"EEG_Eye_State_Test.csv\"\n",
    "\n",
    "# train_file = \"Dummy_Train.csv\"\n",
    "# validate_file = \"Dummy_Validate.csv\"\n",
    "# test_file = \"Dummy_Test.csv\"\n",
    "\n",
    "train_dataset = fpnn.FinalProjectEEGDataset(train_file, root_directory)\n",
    "validate_dataset = fpnn.FinalProjectEEGDataset(validate_file, root_directory)\n",
    "test_dataset = fpnn.FinalProjectEEGDataset(test_file, root_directory)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a360238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c852c421-239e-4363-abbd-045ad9dc7de8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class ConvNet(torch.nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         n_inputs,\n",
    "#         n_hiddens_per_conv_layer,\n",
    "#         n_hiddens_per_fc_layer,\n",
    "#         n_outputs,\n",
    "#         patch_size_per_conv_layer,\n",
    "#         stride_per_conv_layer,\n",
    "#         activation_function=\"tanh\",\n",
    "#         device=\"cpu\",\n",
    "#     ):\n",
    "\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.device = device\n",
    "\n",
    "#         n_conv_layers = len(n_hiddens_per_conv_layer)\n",
    "#         if (\n",
    "#             len(patch_size_per_conv_layer) != n_conv_layers\n",
    "#             or len(stride_per_conv_layer) != n_conv_layers\n",
    "#         ):\n",
    "#             raise Exception(\n",
    "#                 \"The lengths of n_hiddens_per_conv_layer, patch_size_per_conv_layer, and stride_per_conv_layer must be equal.\"\n",
    "#             )\n",
    "\n",
    "#         self.activation_function = (\n",
    "#             torch.tanh if activation_function == \"tanh\" else torch.relu\n",
    "#         )\n",
    "\n",
    "#         # Create all convolutional layers\n",
    "#         # First argument to first Conv1d is number of channels for a sample\n",
    "#         n_in = 14\n",
    "#         input_hw = int(\n",
    "#             np.sqrt(n_inputs)\n",
    "#         )  # original input image height (=width because image assumed square)\n",
    "#         self.conv_layers = torch.nn.ModuleList()\n",
    "#         for nh, patch_size, stride in zip(\n",
    "#             n_hiddens_per_conv_layer, patch_size_per_conv_layer, stride_per_conv_layer\n",
    "#         ):\n",
    "#             self.conv_layers.append(\n",
    "#                 torch.nn.Conv1d(n_in, nh, kernel_size=patch_size, stride=stride)\n",
    "#             )\n",
    "#             conv_layer_output_hw = (input_hw - patch_size) // stride + 1\n",
    "#             input_hw = conv_layer_output_hw  # for next trip through this loop\n",
    "#             n_in = nh\n",
    "\n",
    "#         # Create all fully connected layers.  First must determine number of inputs to first\n",
    "#         # fully-connected layer that results from flattening the images coming out of the last\n",
    "#         # convolutional layer.\n",
    "#         n_in = input_hw ** 2 * n_in  # n_hiddens_per_fc_layer[0]\n",
    "#         self.fc_layers = torch.nn.ModuleList()\n",
    "#         for nh in n_hiddens_per_fc_layer:\n",
    "#             self.fc_layers.append(torch.nn.Linear(n_in, nh))\n",
    "#             n_in = nh\n",
    "#         self.fc_layers.append(torch.nn.Linear(n_in, n_outputs))\n",
    "\n",
    "#         self.to(self.device)\n",
    "\n",
    "#     def forward_all_outputs(self, X):\n",
    "#         n_samples = X.shape[0]\n",
    "#         Ys = [X]\n",
    "#         for conv_layer in self.conv_layers:\n",
    "#             Ys.append(self.activation_function(conv_layer(Ys[-1])))\n",
    "\n",
    "#         for layeri, fc_layer in enumerate(self.fc_layers[:-1]):\n",
    "#             if layeri == 0:\n",
    "#                 Ys.append(\n",
    "#                     self.activation_function(fc_layer(Ys[-1].reshape(n_samples, -1)))\n",
    "#                 )\n",
    "#             else:\n",
    "#                 Ys.append(self.activation_function(fc_layer(Ys[-1])))\n",
    "\n",
    "#         Ys.append(self.fc_layers[-1](Ys[-1]))\n",
    "#         return Ys\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         Ys = self.forward_all_outputs(X)\n",
    "#         return Ys[-1]\n",
    "\n",
    "#     def train(\n",
    "#         self, X, T, batch_size, n_epochs, learning_rate, method=\"sgd\", verbose=True\n",
    "#     ):\n",
    "\n",
    "#         # Set data matrices to torch.tensors if not already.\n",
    "#         if not isinstance(X, torch.Tensor):\n",
    "#             X = torch.from_numpy(X).float().to(self.device)\n",
    "#         if not isinstance(T, torch.Tensor):\n",
    "#             T = (\n",
    "#                 torch.from_numpy(T).long().to(self.device)\n",
    "#             )  # required for classification in pytorch\n",
    "\n",
    "#         X.requires_grad_(True)\n",
    "\n",
    "#         self.classes = torch.unique(T)\n",
    "\n",
    "#         if method == \"sgd\":\n",
    "#             optimizer = torch.optim.SGD(\n",
    "#                 self.parameters(), lr=learning_rate, momentum=0.9\n",
    "#             )\n",
    "#         else:\n",
    "#             optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "#         CELoss = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "#         self.error_trace = []\n",
    "\n",
    "#         for epoch in range(n_epochs):\n",
    "\n",
    "#             num_batches = X.shape[0] // batch_size\n",
    "#             loss_sum = 0\n",
    "\n",
    "#             for k in range(num_batches):\n",
    "#                 start = k * batch_size\n",
    "#                 end = (k + 1) * batch_size\n",
    "#                 X_batch = X[start:end, ...]\n",
    "#                 T_batch = T[start:end, ...]\n",
    "\n",
    "#                 Y = self.forward(X_batch)\n",
    "\n",
    "#                 loss = CELoss(Y, T_batch)\n",
    "#                 loss.backward()\n",
    "\n",
    "#                 # Update parameters\n",
    "#                 optimizer.step()\n",
    "#                 optimizer.zero_grad()\n",
    "\n",
    "#                 loss_sum += loss\n",
    "\n",
    "#             self.error_trace.append(loss_sum / num_batches)\n",
    "\n",
    "#             if n_epochs >= 10:\n",
    "#                 if verbose and (epoch + 1) % (n_epochs // 10) == 0:\n",
    "#                     print(\n",
    "#                         f\"{method}: Epoch {epoch + 1} Loss {self.error_trace[-1]:.3f}\"\n",
    "#                     )\n",
    "#             else:\n",
    "#                 print(f\"{method}: Epoch {epoch + 1} Loss {self.error_trace[-1]:.3f}\")\n",
    "\n",
    "#         return self\n",
    "\n",
    "#     def softmax(self, Y):\n",
    "#         \"\"\"Apply to final layer weighted sum outputs\"\"\"\n",
    "#         # Trick to avoid overflow\n",
    "#         maxY = torch.max(Y, axis=1)[0].reshape((-1, 1))\n",
    "#         expY = torch.exp(Y - maxY)\n",
    "#         denom = torch.sum(expY, axis=1).reshape((-1, 1))\n",
    "#         Y = expY / denom\n",
    "#         return Y\n",
    "\n",
    "#     def use(self, X):\n",
    "#         # Set input matrix to torch.tensors if not already.\n",
    "#         if not isinstance(X, torch.Tensor):\n",
    "#             X = torch.from_numpy(X).float().to(self.device)\n",
    "#         Y = self.forward(X)\n",
    "#         probs = self.softmax(Y)\n",
    "#         classes = self.classes[torch.argmax(probs, axis=1).cpu().numpy()]\n",
    "#         return classes.cpu().numpy(), probs.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4e966c0-9c1d-424c-97ea-c9350103eb14",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "\n",
    "# with open(\"Dummy_Lumped_Test.csv\", \"w\", newline=\"\") as csvfile:\n",
    "#     spamwriter = csv.writer(\n",
    "#         csvfile, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL\n",
    "#     )\n",
    "#     col_labels = [\n",
    "#         \"AF3_0\",\n",
    "#         \"F7_0\",\n",
    "#         \"F3_0\",\n",
    "#         \"FC5_0\",\n",
    "#         \"T7_0\",\n",
    "#         \"P7_0\",\n",
    "#         \"O1_0\",\n",
    "#         \"O2_0\",\n",
    "#         \"P8_0\",\n",
    "#         \"T8_0\",\n",
    "#         \"FC6_0\",\n",
    "#         \"F4_0\",\n",
    "#         \"F8_0\",\n",
    "#         \"AF4_0\",\n",
    "#         \"AF3_1\",\n",
    "#         \"F7_1\",\n",
    "#         \"F3_1\",\n",
    "#         \"FC5_1\",\n",
    "#         \"T7_1\",\n",
    "#         \"P7_1\",\n",
    "#         \"O1_1\",\n",
    "#         \"O2_1\",\n",
    "#         \"P8_1\",\n",
    "#         \"T8_1\",\n",
    "#         \"FC6_1\",\n",
    "#         \"F4_1\",\n",
    "#         \"F8_1\",\n",
    "#         \"AF4_1\",\n",
    "#         \"AF3_2\",\n",
    "#         \"F7_2\",\n",
    "#         \"F3_2\",\n",
    "#         \"FC5_2\",\n",
    "#         \"T7_2\",\n",
    "#         \"P7_2\",\n",
    "#         \"O1_2\",\n",
    "#         \"O2_2\",\n",
    "#         \"P8_2\",\n",
    "#         \"T8_2\",\n",
    "#         \"FC6_2\",\n",
    "#         \"F4_2\",\n",
    "#         \"F8_2\",\n",
    "#         \"AF4_2\",\n",
    "#         \"AF3_3\",\n",
    "#         \"F7_3\",\n",
    "#         \"F3_3\",\n",
    "#         \"FC5_3\",\n",
    "#         \"T7_3\",\n",
    "#         \"P7_3\",\n",
    "#         \"O1_3\",\n",
    "#         \"O2_3\",\n",
    "#         \"P8_3\",\n",
    "#         \"T8_3\",\n",
    "#         \"FC6_3\",\n",
    "#         \"F4_3\",\n",
    "#         \"F8_3\",\n",
    "#         \"AF4_3\",\n",
    "#         \"AF3_4\",\n",
    "#         \"F7_4\",\n",
    "#         \"F3_4\",\n",
    "#         \"FC5_4\",\n",
    "#         \"T7_4\",\n",
    "#         \"P7_4\",\n",
    "#         \"O1_4\",\n",
    "#         \"O2_4\",\n",
    "#         \"P8_4\",\n",
    "#         \"T8_4\",\n",
    "#         \"FC6_4\",\n",
    "#         \"F4_4\",\n",
    "#         \"F8_4\",\n",
    "#         \"AF4_4\",\n",
    "#         \"AF3_5\",\n",
    "#         \"F7_5\",\n",
    "#         \"F3_5\",\n",
    "#         \"FC5_5\",\n",
    "#         \"T7_5\",\n",
    "#         \"P7_5\",\n",
    "#         \"O1_5\",\n",
    "#         \"O2_5\",\n",
    "#         \"P8_5\",\n",
    "#         \"T8_5\",\n",
    "#         \"FC6_5\",\n",
    "#         \"F4_5\",\n",
    "#         \"F8_5\",\n",
    "#         \"AF4_5\",\n",
    "#         \"AF3_6\",\n",
    "#         \"F7_6\",\n",
    "#         \"F3_6\",\n",
    "#         \"FC5_6\",\n",
    "#         \"T7_6\",\n",
    "#         \"P7_6\",\n",
    "#         \"O1_6\",\n",
    "#         \"O2_6\",\n",
    "#         \"P8_6\",\n",
    "#         \"T8_6\",\n",
    "#         \"FC6_6\",\n",
    "#         \"F4_6\",\n",
    "#         \"F8_6\",\n",
    "#         \"AF4_6\",\n",
    "#         \"AF3_7\",\n",
    "#         \"F7_7\",\n",
    "#         \"F3_7\",\n",
    "#         \"FC5_7\",\n",
    "#         \"T7_7\",\n",
    "#         \"P7_7\",\n",
    "#         \"O1_7\",\n",
    "#         \"O2_7\",\n",
    "#         \"P8_7\",\n",
    "#         \"T8_7\",\n",
    "#         \"FC6_7\",\n",
    "#         \"F4_7\",\n",
    "#         \"F8_7\",\n",
    "#         \"AF4_7\",\n",
    "#         \"AF3_8\",\n",
    "#         \"F7_8\",\n",
    "#         \"F3_8\",\n",
    "#         \"FC5_8\",\n",
    "#         \"T7_8\",\n",
    "#         \"P7_8\",\n",
    "#         \"O1_8\",\n",
    "#         \"O2_8\",\n",
    "#         \"P8_8\",\n",
    "#         \"T8_8\",\n",
    "#         \"FC6_8\",\n",
    "#         \"F4_8\",\n",
    "#         \"F8_8\",\n",
    "#         \"AF4_8\",\n",
    "#         \"AF3_9\",\n",
    "#         \"F7_9\",\n",
    "#         \"F3_9\",\n",
    "#         \"FC5_9\",\n",
    "#         \"T7_9\",\n",
    "#         \"P7_9\",\n",
    "#         \"O1_9\",\n",
    "#         \"O2_9\",\n",
    "#         \"P8_9\",\n",
    "#         \"T8_9\",\n",
    "#         \"FC6_9\",\n",
    "#         \"F4_9\",\n",
    "#         \"F8_9\",\n",
    "#         \"AF4_9\",\n",
    "#         \"eyeDetection\",\n",
    "#     ]\n",
    "#     spamwriter.writerow(col_labels)\n",
    "\n",
    "#     list_a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "#     list_b = [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
    "#     list_big_a = []\n",
    "#     list_big_b = []\n",
    "#     for i in range(10):\n",
    "#         for j in range(14):\n",
    "#             list_big_a.append(list_a[j])\n",
    "#             list_big_b.append(list_b[j])\n",
    "\n",
    "#     list_big_a.append(0)\n",
    "#     list_big_b.append(1)\n",
    "#     for i in range(500):\n",
    "#         random_number = random.randint(1, 10)\n",
    "#         if random_number > 5:\n",
    "#             spamwriter.writerow(list_big_a)\n",
    "#         else:\n",
    "#             spamwriter.writerow(list_big_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94e3b846-55b1-4ef7-8e2d-8fdab8750ab6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): Conv1d(1, 10, kernel_size=(3,), stride=(2,))\n",
      "    (1): Conv1d(10, 20, kernel_size=(3,), stride=(2,))\n",
      "  )\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=40, out_features=5, bias=True)\n",
      "    (1): Linear(in_features=5, out_features=5, bias=True)\n",
      "    (2): Linear(in_features=5, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = fpnn.CNN(\n",
    "    1,\n",
    "    14,\n",
    "    [10, 20],\n",
    "    [5, 5],\n",
    "    2,\n",
    "    kernel_size_per_conv_layer=[3, 3],\n",
    "    stride_per_conv_layer=[2, 2],\n",
    ").to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a301e97d-8669-4c34-9020-614e3f3246f9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from functools import reduce\n",
    "# from operator import mul\n",
    "\n",
    "# print(torch.rand(5, 14))\n",
    "# X = torch.rand(100, 14, device=device)\n",
    "# print(X.shape)\n",
    "# T = torch.randint(0, 2, (100, 1), device=device)\n",
    "# logits = model(X)\n",
    "# print(logits)\n",
    "# pred_probab = nn.Softmax(dim=1)(logits)\n",
    "# y_pred = pred_probab.argmax(1)\n",
    "# print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55e5e4ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    # first = True\n",
    "    for batch, (X, T) in enumerate(dataloader):\n",
    "        X, T = X.to(device), T.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        Y = model(X.float())\n",
    "        # if first:\n",
    "        #     first = False\n",
    "        #     print('Y.shape', Y.shape)\n",
    "        #     print('T.shape', T.shape)\n",
    "        #     print('T: ', T[0])\n",
    "        loss = loss_fn(Y, T.long())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "#         if batch % 50 == 0:\n",
    "#             loss, current = loss.item(), batch * len(X)\n",
    "# print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17432e51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, T in dataloader:\n",
    "            X, T = X.to(device), T.to(device)\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, T.long()).item()\n",
    "            correct += (pred.argmax(1) == T).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(\n",
    "        f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32ade0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_epochs(\n",
    "    epochs, train_dataloader, _validate_dataloader, model, loss_fn, optimizer\n",
    "):\n",
    "    start = time.time()\n",
    "    for t in range(epochs):\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        if t % 50 == 0:\n",
    "            print(f\"Epoch {t}\\n-------------------------------\")\n",
    "            test(_validate_dataloader, model, loss_fn)\n",
    "    end = time.time()\n",
    "    print(\"Done: \", end - start, \" seconds\")\n",
    "    test(_validate_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41ba2342",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fashion_training_data = datasets.FashionMNIST(\n",
    "#     root=\"data\",\n",
    "#     train=True,\n",
    "#     download=True,\n",
    "#     transform=ToTensor(),\n",
    "# )\n",
    "# fashion_train_dataloader = DataLoader(fashion_training_data, batch_size=64)\n",
    "# n_samples, n_inputs = train_dataset.__len__(), 28*28\n",
    "# n_outputs = 1\n",
    "# n_hiddens = [10, 10]\n",
    "# model = fpnn.NeuralNetwork(28*28, n_hiddens, 10).to(device)\n",
    "# print(model)\n",
    "\n",
    "# learning_rate = 0.5 / (n_samples * n_outputs)  ## Larger learning rate\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# train(fashion_train_dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3190dd90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([64, 1, 14])\n",
      "Shape of y:  torch.Size([64]) torch.float64\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bb6285-a276-4dc3-8b56-e9a0551747b7",
   "metadata": {},
   "source": [
    "I again started out with a dummy run using dummy data that had a trivial pattern which the neural network should easily be able to learn. I did this to simply test that the network was correctly setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "544cb811-d26b-476f-9f35-30d5871fe03b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "\n",
    "# train_file = \"EEG_Eye_State_Train.csv\"\n",
    "# validate_file = \"EEG_Eye_State_Validate.csv\"\n",
    "# test_file = \"EEG_Eye_State_Test.csv\"\n",
    "\n",
    "train_file = \"Dummy_Lumped_Train.csv\"\n",
    "validate_file = \"Dummy_Lumped_Validate.csv\"\n",
    "test_file = \"Dummy_Lumped_Test.csv\"\n",
    "\n",
    "train_dataset = fpnn.FinalProjectEEGDataset(train_file, root_directory)\n",
    "validate_dataset = fpnn.FinalProjectEEGDataset(validate_file, root_directory)\n",
    "test_dataset = fpnn.FinalProjectEEGDataset(test_file, root_directory)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e431da3e-10e3-4a48-bd32-0bbfd6be3bd3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): Conv1d(1, 20, kernel_size=(10,), stride=(1,))\n",
      "    (1): Conv1d(20, 20, kernel_size=(10,), stride=(1,))\n",
      "  )\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=2440, out_features=20, bias=True)\n",
      "    (1): Linear(in_features=20, out_features=15, bias=True)\n",
      "    (2): Linear(in_features=15, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.069871 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000094 \n",
      "\n",
      "Done:  11.338661193847656  seconds\n",
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000063 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_channels = 1\n",
    "num_inputs = 140\n",
    "num_hiddens_per_conv_layer = [20, 20]\n",
    "num_hiddens_per_fc_layer = [20, 15]\n",
    "num_outputs = 2\n",
    "kernel_size_per_conv_layer = [10, 10]\n",
    "stride_per_conv_layer = [1, 1]\n",
    "\n",
    "\n",
    "model = fpnn.CNN(\n",
    "    num_channels,\n",
    "    num_inputs,\n",
    "    num_hiddens_per_conv_layer,\n",
    "    num_hiddens_per_fc_layer,\n",
    "    num_outputs,\n",
    "    kernel_size_per_conv_layer=kernel_size_per_conv_layer,\n",
    "    stride_per_conv_layer=stride_per_conv_layer,\n",
    ").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.05\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 100\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "086789af-6c0a-442e-b27e-941424bbacce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 100.0%, Avg loss: 0.000063 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcde14a-4ea1-441f-bb63-0da1c1f0d0b6",
   "metadata": {},
   "source": [
    "domain model result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cdf9b4a-56dd-4686-85fd-499a718c7cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_directory = \"D:\\College\\CS_545\\Final_Project\"\n",
    "# root_directory = \"/s/chopin/l/grad/acf003/CS_545\"\n",
    "\n",
    "train_file = \"EEG_Eye_State_Lumped_10_Normalized_Train.csv\"\n",
    "validate_file = \"EEG_Eye_State_Lumped_10_Normalized_Validate.csv\"\n",
    "test_file = \"EEG_Eye_State_Lumped_10_Normalized_Test.csv\"\n",
    "\n",
    "train_dataset = fpnn.FinalProjectEEGDataset(train_file, root_directory)\n",
    "validate_dataset = fpnn.FinalProjectEEGDataset(validate_file, root_directory)\n",
    "test_dataset = fpnn.FinalProjectEEGDataset(test_file, root_directory)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "validate_dataloader = DataLoader(validate_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da15e092-edf9-4da9-b2b3-34b5e7d8cd16",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): Conv1d(1, 20, kernel_size=(10,), stride=(1,))\n",
      "    (1): Conv1d(20, 20, kernel_size=(10,), stride=(1,))\n",
      "  )\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=2440, out_features=20, bias=True)\n",
      "    (1): Linear(in_features=20, out_features=15, bias=True)\n",
      "    (2): Linear(in_features=15, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Avg loss: 0.603634 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.299650 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.278495 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.282074 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.288265 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.308168 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.318619 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.327215 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.336606 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.344593 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 93.6%, Avg loss: 0.350636 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 93.6%, Avg loss: 0.355814 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 93.6%, Avg loss: 0.360859 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 93.6%, Avg loss: 0.369343 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 93.6%, Avg loss: 0.376887 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 94.3%, Avg loss: 0.380439 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 94.3%, Avg loss: 0.382873 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 94.3%, Avg loss: 0.384798 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 94.3%, Avg loss: 0.386415 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 94.3%, Avg loss: 0.387785 \n",
      "\n",
      "Done:  332.98439741134644  seconds\n",
      "Test Error: \n",
      " Accuracy: 94.3%, Avg loss: 0.388887 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_channels = 1\n",
    "num_inputs = 140\n",
    "num_hiddens_per_conv_layer = [20, 20]\n",
    "num_hiddens_per_fc_layer = [20, 15]\n",
    "num_outputs = 2\n",
    "kernel_size_per_conv_layer = [10, 10]\n",
    "stride_per_conv_layer = [1, 1]\n",
    "\n",
    "\n",
    "model = fpnn.CNN(\n",
    "    num_channels,\n",
    "    num_inputs,\n",
    "    num_hiddens_per_conv_layer,\n",
    "    num_hiddens_per_fc_layer,\n",
    "    num_outputs,\n",
    "    kernel_size_per_conv_layer=kernel_size_per_conv_layer,\n",
    "    stride_per_conv_layer=stride_per_conv_layer,\n",
    ").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.05\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 1000\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "746637f2-2995-4d85-a7a7-aff78c4d5a61",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): Conv1d(1, 20, kernel_size=(10,), stride=(1,))\n",
      "    (1): Conv1d(20, 20, kernel_size=(10,), stride=(1,))\n",
      "  )\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=2440, out_features=20, bias=True)\n",
      "    (1): Linear(in_features=20, out_features=15, bias=True)\n",
      "    (2): Linear(in_features=15, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 56.7%, Avg loss: 0.631273 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.207903 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.228532 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.238833 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.245316 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.250075 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.253914 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.257138 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.259915 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.262351 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.264518 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.266469 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.268244 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.269870 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.271371 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.272763 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.274062 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.275278 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.276423 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.277504 \n",
      "\n",
      "Done:  322.1543629169464  seconds\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.278507 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_channels = 1\n",
    "num_inputs = 140\n",
    "num_hiddens_per_conv_layer = [20, 20]\n",
    "num_hiddens_per_fc_layer = [20, 15]\n",
    "num_outputs = 2\n",
    "kernel_size_per_conv_layer = [10, 10]\n",
    "stride_per_conv_layer = [1, 1]\n",
    "\n",
    "\n",
    "model = fpnn.CNN(\n",
    "    num_channels,\n",
    "    num_inputs,\n",
    "    num_hiddens_per_conv_layer,\n",
    "    num_hiddens_per_fc_layer,\n",
    "    num_outputs,\n",
    "    kernel_size_per_conv_layer=kernel_size_per_conv_layer,\n",
    "    stride_per_conv_layer=stride_per_conv_layer,\n",
    ").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 1000\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd8011c8-6b4b-4e9f-8259-9fe17c1a7392",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): Conv1d(1, 1000, kernel_size=(10,), stride=(1,))\n",
      "    (1): Conv1d(1000, 20, kernel_size=(10,), stride=(1,))\n",
      "    (2): Conv1d(20, 10, kernel_size=(10,), stride=(1,))\n",
      "  )\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=1130, out_features=500, bias=True)\n",
      "    (1): Linear(in_features=500, out_features=15, bias=True)\n",
      "    (2): Linear(in_features=15, out_features=10, bias=True)\n",
      "    (3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 0.608456 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.269878 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.382582 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.410375 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.427347 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.439617 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.449244 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.457177 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.463928 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.469808 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.475018 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.479697 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.483944 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.487836 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.491425 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.494759 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.497871 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.500790 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.503539 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.506136 \n",
      "\n",
      "Done:  2614.1272342205048  seconds\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.508549 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_channels = 1\n",
    "num_inputs = 140\n",
    "num_hiddens_per_conv_layer = [1000, 20, 10]\n",
    "num_hiddens_per_fc_layer = [500, 15, 10]\n",
    "num_outputs = 2\n",
    "kernel_size_per_conv_layer = [10, 10, 10]\n",
    "stride_per_conv_layer = [1, 1, 1]\n",
    "\n",
    "\n",
    "model = fpnn.CNN(\n",
    "    num_channels,\n",
    "    num_inputs,\n",
    "    num_hiddens_per_conv_layer,\n",
    "    num_hiddens_per_fc_layer,\n",
    "    num_outputs,\n",
    "    kernel_size_per_conv_layer=kernel_size_per_conv_layer,\n",
    "    stride_per_conv_layer=stride_per_conv_layer,\n",
    ").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.05\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 1000\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3dd92362-a5fe-483e-995c-b8ab27a6a4fe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): Conv1d(1, 10, kernel_size=(10,), stride=(1,))\n",
      "  )\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=1310, out_features=10, bias=True)\n",
      "    (1): Linear(in_features=10, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.554793 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.407030 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.443656 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.463671 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.470202 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.474084 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.476955 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.479734 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.482403 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.484543 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.486178 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.487460 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.488483 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.489289 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.489912 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.490374 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.490698 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.490897 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.490966 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.490889 \n",
      "\n",
      "Done:  282.5732274055481  seconds\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.490649 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_channels = 1\n",
    "num_inputs = 140\n",
    "num_hiddens_per_conv_layer = [10]\n",
    "num_hiddens_per_fc_layer = [10]\n",
    "num_outputs = 2\n",
    "kernel_size_per_conv_layer = [10]\n",
    "stride_per_conv_layer = [1]\n",
    "\n",
    "\n",
    "model = fpnn.CNN(\n",
    "    num_channels,\n",
    "    num_inputs,\n",
    "    num_hiddens_per_conv_layer,\n",
    "    num_hiddens_per_fc_layer,\n",
    "    num_outputs,\n",
    "    kernel_size_per_conv_layer=kernel_size_per_conv_layer,\n",
    "    stride_per_conv_layer=stride_per_conv_layer,\n",
    ").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.05\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 1000\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8a65969-f168-4a01-b29a-4683658064cd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): Conv1d(1, 20, kernel_size=(10,), stride=(1,))\n",
      "    (1): Conv1d(20, 10, kernel_size=(10,), stride=(1,))\n",
      "    (2): Conv1d(10, 10, kernel_size=(10,), stride=(1,))\n",
      "  )\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=1130, out_features=20, bias=True)\n",
      "    (1): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 49.6%, Avg loss: 0.707113 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.275624 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.357023 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.402117 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.421686 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.434964 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.445185 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.453590 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.460781 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.467103 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.472766 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.477904 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.482627 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.487027 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.491147 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.495031 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.498725 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.502250 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.505627 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.508866 \n",
      "\n",
      "Done:  336.99566626548767  seconds\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.511930 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_channels = 1\n",
    "num_inputs = 140\n",
    "num_hiddens_per_conv_layer = [\n",
    "    int(np.sqrt(np.prod(117))) * 2,\n",
    "    int(np.sqrt(np.prod(117))),\n",
    "    10,\n",
    "]\n",
    "num_hiddens_per_fc_layer = [\n",
    "    int(np.sqrt(np.prod(117))) * 2,\n",
    "    int(np.sqrt(np.prod(117))),\n",
    "    10,\n",
    "]\n",
    "num_outputs = 2\n",
    "kernel_size_per_conv_layer = [10, 10, 10]\n",
    "stride_per_conv_layer = [1, 1, 1]\n",
    "\n",
    "\n",
    "model = fpnn.CNN(\n",
    "    num_channels,\n",
    "    num_inputs,\n",
    "    num_hiddens_per_conv_layer,\n",
    "    num_hiddens_per_fc_layer,\n",
    "    num_outputs,\n",
    "    kernel_size_per_conv_layer=kernel_size_per_conv_layer,\n",
    "    stride_per_conv_layer=stride_per_conv_layer,\n",
    ").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.05\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 1000\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5903c50-2f89-42ce-ae39-9dd42adde637",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): Conv1d(1, 20, kernel_size=(10,), stride=(1,))\n",
      "    (1): Conv1d(20, 20, kernel_size=(10,), stride=(1,))\n",
      "    (2): Conv1d(20, 10, kernel_size=(10,), stride=(1,))\n",
      "    (3): Conv1d(10, 10, kernel_size=(10,), stride=(1,))\n",
      "    (4): Conv1d(10, 5, kernel_size=(10,), stride=(1,))\n",
      "  )\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=475, out_features=20, bias=True)\n",
      "    (1): Linear(in_features=20, out_features=15, bias=True)\n",
      "    (2): Linear(in_features=15, out_features=15, bias=True)\n",
      "    (3): Linear(in_features=15, out_features=10, bias=True)\n",
      "    (4): Linear(in_features=10, out_features=5, bias=True)\n",
      "    (5): Linear(in_features=5, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 49.6%, Avg loss: 0.699657 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.269158 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.507028 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.311398 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.525840 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.363310 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.289021 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.401791 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.377731 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.342948 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.358809 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.470437 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.404211 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 77.3%, Avg loss: 0.392960 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.476361 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.406996 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.316269 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.383859 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.349261 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.372250 \n",
      "\n",
      "Done:  391.0361313819885  seconds\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.313519 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_channels = 1\n",
    "num_inputs = 140\n",
    "num_hiddens_per_conv_layer = [20, 20, 10, 10, 5]\n",
    "num_hiddens_per_fc_layer = [20, 15, 15, 10, 5]\n",
    "num_outputs = 2\n",
    "kernel_size_per_conv_layer = [10, 10, 10, 10, 10]\n",
    "stride_per_conv_layer = [1, 1, 1, 1, 1]\n",
    "\n",
    "\n",
    "model = fpnn.CNN(\n",
    "    num_channels,\n",
    "    num_inputs,\n",
    "    num_hiddens_per_conv_layer,\n",
    "    num_hiddens_per_fc_layer,\n",
    "    num_outputs,\n",
    "    kernel_size_per_conv_layer=kernel_size_per_conv_layer,\n",
    "    stride_per_conv_layer=stride_per_conv_layer,\n",
    ").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.05\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 1000\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de294a4e-5f6f-4c8d-8278-8a475e38aa14",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): Conv1d(1, 100, kernel_size=(10,), stride=(1,))\n",
      "    (1): Conv1d(100, 20, kernel_size=(10,), stride=(1,))\n",
      "  )\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=2440, out_features=20, bias=True)\n",
      "    (1): Linear(in_features=20, out_features=15, bias=True)\n",
      "    (2): Linear(in_features=15, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.596524 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.268456 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.331914 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.362615 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.382574 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.395071 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.401332 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.406762 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.412929 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.418184 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.422693 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.426687 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.430263 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.433463 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.436309 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.438824 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.441022 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.442898 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.444440 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.445605 \n",
      "\n",
      "Done:  542.980455160141  seconds\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.446280 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_channels = 1\n",
    "num_inputs = 140\n",
    "num_hiddens_per_conv_layer = [100, 20]\n",
    "num_hiddens_per_fc_layer = [20, 15]\n",
    "num_outputs = 2\n",
    "kernel_size_per_conv_layer = [10, 10]\n",
    "stride_per_conv_layer = [1, 1]\n",
    "\n",
    "\n",
    "model = fpnn.CNN(\n",
    "    num_channels,\n",
    "    num_inputs,\n",
    "    num_hiddens_per_conv_layer,\n",
    "    num_hiddens_per_fc_layer,\n",
    "    num_outputs,\n",
    "    kernel_size_per_conv_layer=kernel_size_per_conv_layer,\n",
    "    stride_per_conv_layer=stride_per_conv_layer,\n",
    ").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.05\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 1000\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef37ca02-61a3-47f4-ac1b-6c481ff45cb5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): Conv1d(1, 100, kernel_size=(10,), stride=(1,))\n",
      "    (1): Conv1d(100, 20, kernel_size=(10,), stride=(1,))\n",
      "  )\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=2440, out_features=20, bias=True)\n",
      "    (1): Linear(in_features=20, out_features=15, bias=True)\n",
      "    (2): Linear(in_features=15, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.596524 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.268456 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.331914 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.362615 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.382574 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.395071 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.401332 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.406762 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.412929 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.418184 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.422693 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.426687 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.430263 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.433463 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_channels = 1\n",
    "num_inputs = 140\n",
    "num_hiddens_per_conv_layer = [100, 20]\n",
    "num_hiddens_per_fc_layer = [20, 15]\n",
    "num_outputs = 2\n",
    "kernel_size_per_conv_layer = [10, 10]\n",
    "stride_per_conv_layer = [1, 1]\n",
    "\n",
    "\n",
    "model = fpnn.CNN(\n",
    "    num_channels,\n",
    "    num_inputs,\n",
    "    num_hiddens_per_conv_layer,\n",
    "    num_hiddens_per_fc_layer,\n",
    "    num_outputs,\n",
    "    kernel_size_per_conv_layer=kernel_size_per_conv_layer,\n",
    "    stride_per_conv_layer=stride_per_conv_layer,\n",
    ").to(device)\n",
    "print(model)\n",
    "\n",
    "# learning_rate = 0.05 / (n_samples * n_outputs)\n",
    "learning_rate = 0.05\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs = 1000\n",
    "train_for_epochs(\n",
    "    epochs, train_dataloader, validate_dataloader, model, loss_fn, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c11080a-bd26-415f-a45f-88637fdc9cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(train_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e79e286e-1fa9-4341-8121-87ed6aa0b05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2147182-979a-4829-a680-ea2b6b5908cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp",
   "language": "python",
   "name": "fp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
